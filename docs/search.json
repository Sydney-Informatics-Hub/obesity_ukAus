[
  {
    "objectID": "400_analysis/ConditionAndPersonFirst.html",
    "href": "400_analysis/ConditionAndPersonFirst.html",
    "title": "Identity and person-first language and use of nominal adjectival “the obese”",
    "section": "",
    "text": "Both corpora had data from 2008-2016, which was used for the analyses below.\n\nWe use the following tests to compare the sub-corpora:\n\nChi-square goodness-of-fit tests, considerin:\n\nThe total sub-corpus word count as the probability of observing a specific feature, when considering the sum of instances.\nThe total number of articles in the subcorpus, when considering number of articles that feature at least one instance.\n\n\nParametric t-tests comparing the mean frequencies, with texts without the feature considered to have a value of 0.\nThe non-parametric Fisher-Pitman Permutation Test test with 10000 re-samples, comparing the mean frequencies, with texts without the feature considered to have a value of 0.\n\n\n\nArticles in Australian broadsheets are longer than in Australian tabloids (mean 830.78 vs 558.72, respectively, p(FP) < 1e-05).\nArticles in UK broadsheets are longer than in UK tabloids (mean 718.72 vs 557.11, respectively, p(FP) < 1e-04).\nArticles in Australian broadsheets are longer than in UK broadsheets (mean 830.78 vs 718.72, respectively, p(FP) < 1e-04).\nThere is no difference in length between articles in Australian and UK tabloids (mean 558.72 vs 557.11, respectively).\n\n\nAustralian broadsheets have somewhat higher total number of instances (p(Chi-sq) < 0.05) and number of articles (p < 0.001) using person-first language than tabloids, however, this difference is not supported when considering the mean frequency of language use across the corpus with parametric or non-parametric tests.\nIn the UK corpus, no differences in the number of instances or articles are observed between tabloids and broadsheets when considering contingency table and frequency based analyses similar to the above.\nAustralian broadsheets have somewhat higher total number of instances (p(Chi-sq) < 0.001) and number of articles (p < 0.001) using person-first language than UK broadsheets. This difference is supported when considering the mean frequency of language use (mean Australian broadsheets = 22.61 words per million (wpm), mean UK broadsheets 10.68 wpm) across the corpus with parametric and non-parametric tests (p(FP) < 0.05)).\nThere is no difference between the number of instances, articles or frequency of between Australian and UK tabloids.\n\n\nAustralian broadsheets have a lower than expected total number of instances (p(Chi-sq) < 0.001) using identity-first language vs Australian tabloids; however, this is not supported by comparing the number of articles with at least one instance, where no difference is detected. If we instead look at the frequency of use, the opposite result is observed, with a higher frequency of identity-first language in tabloids than in broadsheets (mean broadsheets = 1076.39 wpm vs 1642.61 wpm in tabloids, p(FP) < 1e-04). This discrepancy may be attributable to the fact that articles in broadsheets are longer than those in tabloids (mean 830.78 words for broadsheets vs 558.72 for tabloids), as described above, so comparing mean frequencies of the same number of instances results in a difference frequency because the word count being divided by for broadsheets is 1.5x than that of tabloids.\nIn the UK corpus, there is a higher number than expected of instances and articles containing at least one instance of identity-first language used in tabloids, and a lower number in broadsheets (p < 0.01). A very small difference in frequency is also detected, with a higher mean frequency in tabloids than broadsheets (mean broadsheets 788.81 wpm, mean of tabloids 1009.57 wpm, p(FP) < 1e-04).\nThere is a higher number of instances and articles with at least one instance (p(Chi-sq) < 0.001) of identity-first language in Australian vs UK broadsheets. If we consider the frequency, a difference is also detected, with Australian broadsheets having a higher frequency than UK ones (mean Australian 1076.39 wpm, mean UK 788.81 wpm, p(FP) < 1e-04).\nThere is a higher number of instances and articles that use identity-first language in Australian tabloids, and a lower number in UK tabloids (p(Chi-sq) < 0.001). The frequency is also higher in Australian tabloids (mean Australian 1642.61 wpm, mean UK 1009.57 wpm, p(FP) < 1e-04). This is especially striking given that there is no difference in overall article length between Australian and UK tabloids.\n\n\nIn the Australian corpus, no differences in the number of instances or articles are observed between tabloids and broadsheets when considering contingency table based analysis. Frequency-based analysis does not suggest a strong difference either.\nSimilarly, in the UK corpus, no differences in the number of instances or articles are observed between tabloids and broadsheets when considering contingency table based analysis. There is a very small difference detected (p(t-test) < 0.01, p(FP) < 0.01) when considering frequencies, with tabloids having a higher frequency of use of nominal adjectival “the obese” vs broadsheets (mean broadsheet frequency 78.9 wpm, mean tabloid 116.11 wpm).\nWhen comparing use of nominal adjectival “the obese” in the Australian vs UK broadsheets, no differences in the number of instances or articles are observed when considering contingency table based analysis. There is a very small difference detected (p(t-test) < 0.05) when considering frequencies, with UK broadsheets having a higher frequency of use of nominal adjectival “the obese” vs Australian ones (mean frequency Australia - 54.14 wpm, UK - 78.90 wpm), but this is not supported with the non-parametric test.\nThere is a somewhat lower number of total instances of use (p(Chi-sq) < 0.05) and articles (p(Chi-sq) < 0.1) with at least one instance of use of nominal adjectival “the obese” in Australian tabloids, and a higher number in UK tabloids . However, this difference is not supported by frequency analysis (mean Australian 87.95 wpm, mean UK 116.11 wpm), when using both parametric and non-parametric tests.\n\n\nCodelibrary(here)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(corrplot)\nlibrary(coin)\nlibrary(ggvenn)\ntheme_set(theme_minimal())\n\n\n\nCodemetadata <-read_csv(here::here(\"200_data_clean/\", \"full_metadata.csv\")) |> \n  # get data for correct time window\n  filter(year >= 2008 & year <=2016)\nboth_datasets <-  read_csv(here::here(\"200_data_clean/\", \"both_datasets.csv\")) |> \n  # get data for correct time window\n  filter(year >= 2008 & year <=2016) |>\n  # change cond to iden to match what's in final article text\n  mutate(dataset = case_when(dataset == \"cond\" ~ \"iden\", TRUE ~ dataset))\n\nsource(here::here(\"400_analysis\", \"functions.R\"))\n\n# get some wordcounts \nau_tabloid_wordcounts <- get_wc(\n  corpuslabel = \"AUS\",\n  sourcetypelabel = \"tabloid\")\n\nau_broadsheet_wordcounts <- get_wc(\n  corpuslabel = \"AUS\",\n  sourcetypelabel = \"broadsheet\")\n\nuk_tabloid_wordcounts <- get_wc(\n  corpuslabel = \"UK\",\n  sourcetypelabel = \"tabloid\")\n\nuk_broadsheet_wordcounts <- get_wc(\n  corpuslabel = \"UK\",\n  sourcetypelabel = \"broadsheet\")\n\n\nfrequency_table <-\n  metadata |>\n  select(article_id, corpus, source_type) |> \n  left_join(\n  {\n  # this gets a frequency table per article where\n  # a language type is used\n  both_datasets |>\n  select(dataset, article_id, freq_per_million_words) |>\n  pivot_wider(names_from = dataset, \n      names_glue = \"{dataset}_freq\",\n      values_from = freq_per_million_words,\n      values_fill = 0)}\n  ) |>\n  replace_na(list(iden_freq = 0, \n      pers_freq = 0,\n      obes_freq = 0))\n\n\nWe group articles into tabloids and broadsheets in the following manner:\n\nCodemetadata |>\n  select(source, source_type, corpus) |>\n  distinct() |>\n  kable()\n\n\n\nTable 1: Classification of sources by country (corpus of origin) and into types.\n\nsource\nsource_type\ncorpus\n\n\n\nDaily Express\ntabloid\nUK\n\n\nDaily Mail\ntabloid\nUK\n\n\nDaily Star\ntabloid\nUK\n\n\nDaily Telegraph\nbroadsheet\nUK\n\n\nGuardian\nbroadsheet\nUK\n\n\nIndependent\nbroadsheet\nUK\n\n\nMirror\ntabloid\nUK\n\n\nSun\ntabloid\nUK\n\n\nTimes\nbroadsheet\nUK\n\n\nAdvertiser\ntabloid\nAUS\n\n\nAustralian\nbroadsheet\nAUS\n\n\nNorthernT\ntabloid\nAUS\n\n\nCourierMail\ntabloid\nAUS\n\n\nAge\nbroadsheet\nAUS\n\n\nSydHerald\nbroadsheet\nAUS\n\n\nTelegraph\ntabloid\nAUS\n\n\nWestAus\ntabloid\nAUS\n\n\nCanTimes\nbroadsheet\nAUS\n\n\nHeraldSun\ntabloid\nAUS\n\n\nHobMercury\ntabloid\nAUS\n\n\nBrisTimes\nbroadsheet\nAUS\n\n\n\n\n\n\nWe have the following number of articles and total word count from each of the sources:\n\nCodecorpus_stats_grouped <-\n  metadata |> \n  group_by(source, corpus,source_type) |> \n  summarise(total_articles = n(), total_words = sum(no_words_in_text)) |>\n  arrange(corpus, source)\ncorpus_stats_grouped |> kable()\n\n\n\nTable 2: Total number of articles and words from each source.\n\nsource\ncorpus\nsource_type\ntotal_articles\ntotal_words\n\n\n\nAdvertiser\nAUS\ntabloid\n537\n314243\n\n\nAge\nAUS\nbroadsheet\n449\n410596\n\n\nAustralian\nAUS\nbroadsheet\n276\n237544\n\n\nBrisTimes\nAUS\nbroadsheet\n28\n27785\n\n\nCanTimes\nAUS\nbroadsheet\n402\n281738\n\n\nCourierMail\nAUS\ntabloid\n536\n329035\n\n\nHeraldSun\nAUS\ntabloid\n611\n351203\n\n\nHobMercury\nAUS\ntabloid\n262\n118547\n\n\nNorthernT\nAUS\ntabloid\n125\n42908\n\n\nSydHerald\nAUS\nbroadsheet\n586\n488728\n\n\nTelegraph\nAUS\ntabloid\n106\n65198\n\n\nWestAus\nAUS\ntabloid\n361\n196907\n\n\nDaily Express\nUK\ntabloid\n1217\n603896\n\n\nDaily Mail\nUK\ntabloid\n1713\n1317359\n\n\nDaily Star\nUK\ntabloid\n310\n92031\n\n\nDaily Telegraph\nUK\nbroadsheet\n1457\n771630\n\n\nGuardian\nUK\nbroadsheet\n1620\n1439909\n\n\nIndependent\nUK\nbroadsheet\n1057\n714998\n\n\nMirror\nUK\ntabloid\n1610\n850706\n\n\nSun\nUK\ntabloid\n1605\n732172\n\n\nTimes\nUK\nbroadsheet\n1895\n1406620\n\n\n\n\n\n\nAnd the following statistics for the entire datasets:\n\nCodecorpus_stats_grouped |>\n  ungroup()|>\n  group_by(corpus,source_type) |>\n  summarise(all_words_corpus = sum(total_words),\n         all_articles = sum(total_articles)) |>\n  rename(`Total words in subcorpus` = all_words_corpus, \n         `Total articles in subcorpus`= all_articles) |>\n  kable()\n\n\n\nTable 3: Total number of articles and words from each source type, grouped by country of origin.\n\n\n\n\n\n\n\ncorpus\nsource_type\nTotal words in subcorpus\nTotal articles in subcorpus\n\n\n\nAUS\nbroadsheet\n1446391\n1741\n\n\nAUS\ntabloid\n1418041\n2538\n\n\nUK\nbroadsheet\n4333157\n6029\n\n\nUK\ntabloid\n3596164\n6455"
  },
  {
    "objectID": "400_analysis/ConditionAndPersonFirst.html#proportion-and-number-of-articles-using-any-of-the-language-types-in-the-two-corpora",
    "href": "400_analysis/ConditionAndPersonFirst.html#proportion-and-number-of-articles-using-any-of-the-language-types-in-the-two-corpora",
    "title": "Identity and person-first language and use of nominal adjectival “the obese”",
    "section": "Proportion and number of articles using any of the language types in the two corpora",
    "text": "Proportion and number of articles using any of the language types in the two corpora\nLet’s find the total number of articles that use one or more language types in the two corpora.\n\nCodearticle_id_iden <- get_article_ids(both_datasets, \"iden\")\narticle_id_pers <- get_article_ids(both_datasets, \"pers\")\narticle_id_obes <- get_article_ids(both_datasets, \"obes\")\n\nggvenn(list(\n  `Identity first` = article_id_iden,\n  `Person first` = article_id_pers,\n  `Obese` = article_id_obes),\n  fill_color = c(\"white\", \"white\", \"white\")) +\n  labs(title = \"Number of articles that use the language features\")\n\nfor_stacked_chart <-\n  metadata |>\n  select(article_id, corpus, source_type) |>\n  mutate(grp = case_when(\n  article_id %in% base::intersect(base::intersect(article_id_iden, article_id_pers), article_id_obes) ~ \"all3\",\n  # two\n  article_id %in% base::intersect(article_id_iden, article_id_pers) ~ \"iden_pers\",\n  article_id %in% base::intersect(article_id_obes, article_id_iden) ~ \"iden_obes\",\n  article_id %in% base::intersect(article_id_obes, article_id_pers) ~ \"pers_obes\",\n  # one\n  article_id %in% article_id_iden ~ \"iden\",\n  article_id %in% article_id_pers ~ \"pers\",\n  article_id %in% article_id_obes ~ \"obes\",\n  # none\n  TRUE ~ \"none\"\n  )) \n\nggsave(device = \"png\",\n   here::here(\"400_analysis\",\"venn.png\"),\n   bg = \"white\",\n   width = 10,\n   height = 4)\n\n\nFigure 1: Venn diagram of overlap between articles that use identity-first, person-first and nominal adjectival ‘the obese’ in the two corpora.\n\n\n\n\n\n\nCodefor_stacked_chart_data <- for_stacked_chart |> \n  mutate(group = case_when(\n  grp == \"iden_obes\" ~ \"two\",\n  grp == \"pers_obes\" ~ \"two\",\n  grp == \"iden_pers\" ~ \"two\",\n  TRUE ~ grp\n  ))  |>\n  mutate(group = factor(group, \n levels = c(\"none\", \"pers\", \"all3\", \"two\", \"iden_obes\", \"pers_obes\",\"iden_pers\", \"obes\",  \"iden\"),\n labels = c(\n \"no occurrences\",\n \"person-first language\",\n \"all three practices\",\n \"two language practices\",\n \"iden_obes\", \"pers_obes\",\"iden_pers\",\n \"nominal adjectival 'the obese'\",\n \"identity-first language\"\n ))) \n\nfor_stacked_chart_data |>\n #    mutate(group = factor(group, \n # labels = c(\n # \"NoOccurence\",\n # \"Person-first\",\n # \"All3\",\n # \"2of3\",\n # \"Obese\",\n # \"identity-first\"\n # ))) |>\n  group_by(group, source_type, corpus) |>\n  summarise(n_articles = n()) |>\n  ungroup() |>\n  group_by(source_type, corpus) |>\n  mutate(total = sum(n_articles), proportion_articles = round(100 * n_articles/total, 2)) |>\n  select(-total, -n_articles) |>\n  pivot_wider(values_from = proportion_articles, names_from = group, values_fill = 0) |>\n  write_csv(here::here(\"300_data_processed\", \"percentage_table.csv\"))\n\n\n\n\nmfig <-\n  for_stacked_chart_data |>\n  ggplot() + \n  geom_bar(\n  aes(fill=group, \n    x=source_type),\n  position=\"fill\") + \n  facet_grid(~corpus) + theme_bw() + \n  scale_fill_brewer(palette = \"Accent\") +\n scale_y_continuous(name = \"Proportion articles in corpus\",\n        labels = scales::percent) +\n # set axis limits in coord_cartesian\n scale_fill_grey(start = 0.9, end = 0.1) + \n  theme(axis.text.x = element_text(angle = 35, vjust = 1, hjust=1)) +\n  labs(x = \"\",\n   fill = \"\")\n\nmfig\nggsave(device = \"png\",\n   here::here(\"400_analysis\",\"b_w_proportion_to100.png\"),\n   bg = \"white\",\n   width = 10,\n   height = 4)\n\nmfig + coord_cartesian(ylim = c(0, 0.4)) \nggsave(device = \"png\",\n   here::here(\"400_analysis\",\"b_w_proportion.png\"),\n   bg = \"white\",\n   width = 10,\n   height = 4)\n\nmfig + theme(legend.position = \"bottom\")\nggsave(device = \"png\",\n   here::here(\"400_analysis\",\"b_w_proportion_legendbottom_to100.png\"),\n   bg = \"white\",\n   width = 12,\n   height = 4)\n\nmfig + theme(legend.position = \"bottom\") + coord_cartesian(ylim = c(0, 0.4)) \nggsave(device = \"png\",\n   here::here(\"400_analysis\",\"b_w_proportion_legendbottom.png\"),\n   bg = \"white\",\n   width = 12,\n   height = 4)\n\n\nFigure 2: Proportion of articles in each corpus that have instances of all three language types, two language types, a single (specified) language type or no language types. We can see that most articles in both corpora have no use of any of the 3 language types described.\n\n\n\n\n\n\nFigure 3: Proportion of articles in each corpus that have instances of all three language types, two language types, a single (specified) language type or no language types. We can see that most articles in both corpora have no use of any of the 3 language types described.\n\n\n\n\n\n\nFigure 4: Proportion of articles in each corpus that have instances of all three language types, two language types, a single (specified) language type or no language types. We can see that most articles in both corpora have no use of any of the 3 language types described.\n\n\n\n\n\n\nFigure 5: Proportion of articles in each corpus that have instances of all three language types, two language types, a single (specified) language type or no language types. We can see that most articles in both corpora have no use of any of the 3 language types described.\n\n\n\n\n\n\nCodecorpus_articles_grouped <- for_stacked_chart |>\n  group_by(corpus, source_type) |>\n  count() |> \n  rename(total_articles = n)\n\nfor_stacked_chart |> \n  group_by(corpus, source_type) |>\n  count(grp) |>\n  arrange(-n) |>\n  left_join(corpus_articles_grouped) |>\n  mutate(percent = round(100 * n/total_articles, 2)) |>\n  select(-total_articles) |>\n  pivot_wider(names_from = corpus, values_from = c(n, percent), values_fill = 0) |>\n  rename(observed_language = grp) |>\n  kable()\n\n\n\nTable 4: Number and percent of articles in each corpus, grouped by source type, that feature each of the language types.\n\nsource_type\nobserved_language\nn_UK\nn_AUS\npercent_UK\npercent_AUS\n\n\n\ntabloid\nnone\n4810\n1641\n74.52\n64.66\n\n\nbroadsheet\nnone\n4643\n1162\n77.01\n66.74\n\n\ntabloid\niden\n1395\n815\n21.61\n32.11\n\n\nbroadsheet\niden\n1145\n490\n18.99\n28.14\n\n\nbroadsheet\niden_obes\n112\n33\n1.86\n1.90\n\n\ntabloid\niden_obes\n112\n37\n1.74\n1.46\n\n\ntabloid\nobes\n109\n32\n1.69\n1.26\n\n\nbroadsheet\nobes\n91\n28\n1.51\n1.61\n\n\nbroadsheet\niden_pers\n18\n17\n0.30\n0.98\n\n\ntabloid\npers\n16\n7\n0.25\n0.28\n\n\nbroadsheet\npers\n15\n10\n0.25\n0.57\n\n\ntabloid\niden_pers\n11\n6\n0.17\n0.24\n\n\nbroadsheet\nall3\n4\n1\n0.07\n0.06\n\n\nbroadsheet\npers_obes\n1\n0\n0.02\n0.00\n\n\ntabloid\nall3\n1\n0\n0.02\n0.00\n\n\ntabloid\npers_obes\n1\n0\n0.02\n0.00\n\n\n\n\n\n\nWe can also provide a less detailed version of the above summary, where each article could be counted once to each category it belonged to (so, for example, one with all 3 could be counted 3 times: towards identity-first, person-first and “the obese” (nominal adjectival)). Let’s generate this summary:\n\nCodefortable11 <-\n  both_datasets |>\n  select(article_id, corpus, source_type, dataset) |>\n  group_by(corpus, source_type, dataset) |>\n  count() |>\n  rename(article_count = n) |>\n  left_join({\n  metadata |>\n  select(article_id, corpus, source_type) |>\n  group_by(corpus, source_type) |>\n  count() |>\n  rename(corpus_count = n)}) |>\n  mutate(subset = paste(corpus, source_type, sep = \"_\")) |>\n  ungroup() |>\n  select(-corpus, -source_type) |>\n  mutate(dataset = factor(dataset, levels = c(\"pers\", \"iden\", \"obes\")),\n   subset = factor(subset, levels = c(\"AUS_tabloid\",\n                \"UK_tabloid\",\n                \"AUS_broadsheet\",\n                \"UK_broadsheet\"))) |> arrange(dataset, subset)\n\nfortable11 |>\n  select(-corpus_count) |>\n  pivot_wider(values_from = article_count, names_from = subset) |>\n  kable()\n\n\n\nTable 5: Number of articles that have each of the language types. Each article may be counted multiple times, once for each instance of language use.\n\ndataset\nAUS_tabloid\nUK_tabloid\nAUS_broadsheet\nUK_broadsheet\n\n\n\npers\n13\n29\n28\n38\n\n\niden\n858\n1519\n541\n1279\n\n\nobes\n69\n223\n62\n208\n\n\n\n\n\n\nAnd get the percentages table:\n\nCodefortable11 |>\nmutate(prop = round(100*article_count/corpus_count, 2)) |>\n  select(-article_count, -corpus_count) |>\n  pivot_wider(values_from = prop, names_from = subset) |>\n  kable()\n\n\n\nTable 6: Percentage of articles (relative to total number of articles in each subcorpus) that feature the language types. Note that each article may be counted multiple times, for example, twice, if it includes use of both identity-first language and ‘the obese’ (nominal adjectival).\n\ndataset\nAUS_tabloid\nUK_tabloid\nAUS_broadsheet\nUK_broadsheet\n\n\n\npers\n0.51\n0.45\n1.61\n0.63\n\n\niden\n33.81\n23.53\n31.07\n21.21\n\n\nobes\n2.72\n3.45\n3.56\n3.45\n\n\n\n\n\n\nFinally, we can summarise the total number of instances of language use in the entire corpus (if an article has 5 instances of identity-first language, 2 of person-first and 3 of the nominal adjectival “the obese” it will contribute 5, 2 and 3 to the sum of the values of the “iden”, “pers” and “obes” in the table below, vs 1 each in the table above).\n\nCodeboth_datasets |> \n  select(corpus, source_type, dataset, no_hits_in_text) |> \n  group_by(dataset, corpus, source_type) |> \n  summarise(total_instances = sum(no_hits_in_text)) |>\n  mutate(title_row = paste(corpus, source_type, sep = \"_\")) |>\n  ungroup() |>\n  select(-corpus, -source_type) |>\n  pivot_wider(names_from = title_row, values_from = total_instances) |>\n  mutate(dataset = factor(dataset, levels = c(\"pers\", \"iden\", \"obes\")))|>\n  arrange(dataset)|>\n  kable()\n\n\n\nTable 7: Sum of instances of language use.\n\ndataset\nAUS_broadsheet\nAUS_tabloid\nUK_broadsheet\nUK_tabloid\n\n\n\npers\n30\n15\n41\n30\n\n\niden\n1143\n1428\n2320\n2399\n\n\nobes\n72\n78\n263\n264"
  },
  {
    "objectID": "400_analysis/ConditionAndPersonFirst.html#article-length-differences-in-australian-and-uk-broadsheets-and-tabloids",
    "href": "400_analysis/ConditionAndPersonFirst.html#article-length-differences-in-australian-and-uk-broadsheets-and-tabloids",
    "title": "Identity and person-first language and use of nominal adjectival “the obese”",
    "section": "Article length differences in Australian and UK broadsheets and tabloids",
    "text": "Article length differences in Australian and UK broadsheets and tabloids\nAustralian broadsheets vs tabloids\nAre texts in the Australian sub-corpus used for this study also longer than in broadsheets than in tabloids, like was reported for the larger study?\n\nCodehistogram_pairwise(au_broadsheet_wordcounts,\n       au_tabloid_wordcounts,\n       \"broadsheet\",\n       \"tabloid\")\n\n\nFigure 6: Histogram of word count (x) and number (y) of articles from tabloids and broadsheets in the Australian corpus. Broadsheets appear to have higher word counts than tabloids in the dataset.\n\n\n\n\n\nLet’s use a t-test to consider?\nCodereport::report(t.test(au_broadsheet_wordcounts,\n        au_tabloid_wordcounts))\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between au_broadsheet_wordcounts and au_tabloid_wordcounts (mean of x = 830.78, mean of y = 558.72) suggests that the effect is positive, statistically significant, and small (difference = 272.06, 95% CI [237.87, 306.25], t(3367.20) = 15.60, p < .001; Cohen’s d = 0.49, 95% CI [0.43, 0.56])\nThe non-parametric FP test supports this as well:\n\nCodefp_test_wc(wc1 = au_broadsheet_wordcounts,\n     wc2 = au_tabloid_wordcounts,\n     label1 = \"broadsheet\",\n     label2 = \"tabloid\")\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  wc by label (broadsheet, tabloid)\nZ = 15.575, p-value < 1e-04\nalternative hypothesis: true mu is not equal to 0\n\n\nConclusion: Articles in Australian broadsheets are longer than articles in Australian tabloids.\nUK broadsheets vs tabloids\nAre texts in tabloids also shorter than in broadsheets in the UK, like they are in Australia?\n\nCodehistogram_pairwise(wc1 = uk_broadsheet_wordcounts,\n       wc2 = uk_tabloid_wordcounts,\n       label1 = \"broadsheets\",\n       label2 = \"tabloids\")\n\n\nFigure 7: Histogram of word count (x) and number (y) of articles from tabloids and broadsheets in the UK corpus. Similar to the Australian corpus, broadsheets appear to have higher word counts than tabloids in the dataset.\n\n\n\n\n\nBased on the visualisation in Figure Figure 7, this appears to be the case. Let’s use a parametric test to assess.\nCodereport::report(t.test(\n  uk_broadsheet_wordcounts, \n  uk_tabloid_wordcounts))\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between uk_broadsheet_wordcounts and uk_tabloid_wordcounts (mean of x = 718.72, mean of y = 557.11) suggests that the effect is positive, statistically significant, and small (difference = 161.61, 95% CI [142.54, 180.68], t(11987.64) = 16.61, p < .001; Cohen’s d = 0.30, 95% CI [0.26, 0.33])\nYes, articles in broadsheets are indeed longer, similar to the Australian corpus. Does the non-parametric FP test support this as well?\n\nCodefp_test_wc(wc1 = uk_broadsheet_wordcounts,\n     wc2 = uk_tabloid_wordcounts,\n     label1 = \"broadsheet\",\n     label2 = \"tabloid\")\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  wc by label (broadsheet, tabloid)\nZ = 16.507, p-value < 1e-04\nalternative hypothesis: true mu is not equal to 0\n\n\nYes, it does.\nConclusion: Articles in UK broadsheets are longer than articles in UK tabloids.\nAustralian vs UK broadsheets\nAre texts in broadsheets from Australia and the UK different in length?\nThe visualisation in Figure Figure 8 suggests this is the case.\n\nCodehistogram_pairwise(au_broadsheet_wordcounts,\n       uk_broadsheet_wordcounts,\n       \"Aus\",\n       \"UK\")\n\n\nFigure 8: Histogram of word count (x) and number (y) of articles from broadsheets in the Australian and UK corpus. Articles in UK broadsheets are somewhat shorter than in Australian ones.\n\n\n\n\n\nLet’s use a t-test to consider?\nCodereport::report(t.test(au_broadsheet_wordcounts,\n        uk_broadsheet_wordcounts))\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between au_broadsheet_wordcounts and uk_broadsheet_wordcounts (mean of x = 830.78, mean of y = 718.72) suggests that the effect is positive, statistically significant, and very small (difference = 112.06, 95% CI [80.65, 143.48], t(2766.93) = 6.99, p < .001; Cohen’s d = 0.19, 95% CI [0.14, 0.25])\nYes, there appears to be a very small difference in text length between Australian and UK broadsheets, with articles in UK broadsheets being somewhat shorter.\nDoes the non-parametric FP test support this as well?\n\nCodefp_test_wc(wc1 = au_broadsheet_wordcounts,\n     wc2 = uk_broadsheet_wordcounts,\n     label1 = \"AUS\",\n     label2 = \"UK\")\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  wc by label (AUS, UK)\nZ = 7.0697, p-value < 1e-04\nalternative hypothesis: true mu is not equal to 0\n\n\nYes, it does.\nConclusion: Articles in Australian broadsheets are longer than articles in UK broadsheets.\nAustralian vs UK tabloids\nAre texts in tabloids from Australia and the UK different in length?\nThe visualisation in Figure Figure 9 suggests that there is no difference between the two.\n\nCodehistogram_pairwise(au_tabloid_wordcounts,\n       uk_tabloid_wordcounts,\n       \"Aus\",\n       \"UK\")\n\n\nFigure 9: Histogram of word count (x) and number (y) of articles from tabloids in the Australian and UK corpus. There doesn’t seem to be a difference in length between articles in Australian and UK tabloids.\n\n\n\n\n\nLet’s use a t-test to consider?\nCodereport::report(t.test(au_tabloid_wordcounts,\n        uk_tabloid_wordcounts))\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between au_tabloid_wordcounts and uk_tabloid_wordcounts (mean of x = 558.72, mean of y = 557.11) suggests that the effect is positive, statistically not significant, and very small (difference = 1.61, 95% CI [-21.76, 24.98], t(4587.29) = 0.14, p = 0.892; Cohen’s d = 3.18e-03, 95% CI [-0.04, 0.05])\nNo, there appears to be no difference in length between articles in Australian and UK tabloids.\nDoes the non-parametric FP test support this as well?\n\nCodefp_test_wc(wc1 = au_tabloid_wordcounts,\n     wc2 = uk_tabloid_wordcounts,\n     label1 = \"AUS\",\n     label2 = \"UK\")\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  wc by label (AUS, UK)\nZ = 0.13595, p-value = 0.8932\nalternative hypothesis: true mu is not equal to 0\n\n\nConclusion: Articles in Australian and UK tabloids do not differ in length."
  },
  {
    "objectID": "400_analysis/ConditionAndPersonFirst.html#person-first-language",
    "href": "400_analysis/ConditionAndPersonFirst.html#person-first-language",
    "title": "Identity and person-first language and use of nominal adjectival “the obese”",
    "section": "Person-first language",
    "text": "Person-first language\nAustralian corpus\nLet’s generate a frequency table (sum of no hits in text) for person-first language:\n\nCodepers_4chisq_matrix_sumhits <-\n  both_datasets |> \n  filter(dataset == \"pers\") |>\n  generate_conting_tab_sumhits() |>\n  as.matrix()\n\npers_4chisq_matrix_noarticles <-\n  both_datasets |> \n  filter(dataset == \"pers\") |>\n  generate_conting_tab_noarticles() |>\n  as.matrix()\n\npers_4chisq_matrix_sumhits |>\n  kable()\n\n\n\nTable 8: Number of articles that feature person-first language in UK and Australian tabloids and broadsheets.\n\n\nAUS\nUK\n\n\n\nbroadsheet\n30\n41\n\n\ntabloid\n15\n30\n\n\n\n\n\n\nWe also need to find the probability of observing these values given the total word count in each of the cells (for example, the number of total words in the Australian broadsheet corpus). Let’s calculate these values:\n\nCodegenerate_wc(metadata) |>\n  kable()\n\n\n\nTable 9: Total word count of articles in UK and Australian tabloids and broadsheets.\n\n\nAUS\nUK\n\n\n\nbroadsheet\n1446391\n4333157\n\n\ntabloid\n1418041\n3596164\n\n\n\n\n\n\nIs there a difference in the frequency of use of person-first language in tabloids and broadsheets in the Australian corpus?\n\nCodeget_chisq_tabloid_broadsheet(pers_4chisq_matrix_sumhits, \n           col_no = 1, \n           type = \"wc\") |>\n  kable()\n\n\n\nTable 10: Results of Chi-square test between the number of instances of person-first language in the Australian corpus vs the number that would be expected based on total word count of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n30\n\n\nbroadsheet_expected\n22.72269\n\n\ntabloid_observed\n15\n\n\ntabloid_expected\n22.27731\n\n\nstatistic\n4.707952\n\n\np.value\n0.0300234\n\n\nparameter\n1\n\n\neffect_size\n0.3234519\n\n\n\n\n\n\nBased on a Chi-square test, there is a slight difference between the expected and observed frequency of instances of person-first language in the Australian corpus (p < 0.05), with broadsheets having somewhat more instances of person-first language than tabloids in the Australian corpus.\nThis also holds true when we consider the number of articles that have at least one instance of person-first language:\n\nCodeget_chisq_tabloid_broadsheet(pers_4chisq_matrix_noarticles, \n           col_no = 1, \n           type = \"art\") |>\n  kable()\n\n\n\nTable 11: Results of Chi-square test between the number of articles that use person-first language in the Australian corpus vs the number that would be expected based on the total number of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n28\n\n\nbroadsheet_expected\n16.6817\n\n\ntabloid_observed\n13\n\n\ntabloid_expected\n24.3183\n\n\nstatistic\n12.9471\n\n\np.value\n0.0003204168\n\n\nparameter\n1\n\n\neffect_size\n0.5619457\n\n\n\n\n\n\nIf we consider a t-test on the frequency (per million words) of the Australian corpus (including all texts in each sub-corpus, including those without instances), a difference is not detected.\nCodettest_broad_vs_tabl(frequency_table, \"AUS\", \"pers_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between broadsheet and tabloid (mean of x = 22.61, mean of y = 14.58) suggests that the effect is positive, statistically not significant, and very small (difference = 8.03, 95% CI [-5.02, 21.09], t(4083.60) = 1.21, p = 0.228; Cohen’s d = 0.04, 95% CI [-0.02, 0.10])\nIf we instead use a non-parametric test, there is also no difference detected.\n\nCodefp_test_broad_vs_tabl(freq_table = frequency_table,\n        corpus_label = \"AUS\",\n        dataset = \"pers\",\n        myformula = formula(pers_freq ~ source_type),\n        dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  pers_freq by source_type (broadsheet, tabloid)\nZ = 1.1711, p-value = 0.2538\nalternative hypothesis: true mu is not equal to 0\n\n\nUK corpus\nWhat about the total number of instances of person-first language in the UK corpus?\n\nCodeget_chisq_tabloid_broadsheet(pers_4chisq_matrix_sumhits, \n           col_no = 2,\n           type = \"wc\") |>\n  kable()\n\n\n\nTable 12: Results of Chi-square test between the number of instances of person-first language in the UK corpus vs the number that would be expected based on total word count of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n41\n\n\nbroadsheet_expected\n38.79956\n\n\ntabloid_observed\n30\n\n\ntabloid_expected\n32.20044\n\n\nstatistic\n0.2751628\n\n\np.value\n0.5998894\n\n\nparameter\n1\n\n\neffect_size\n0.06225378\n\n\n\n\n\n\nNo difference is observed between the number of observed and expected instances of use of person-first language in the UK corpus between tabloids and broadsheets.\n\nCodeget_chisq_tabloid_broadsheet(pers_4chisq_matrix_noarticles, \n           col_no = 2, \n           type = \"art\") |>\n  kable()\n\n\n\nTable 13: Results of Chi-square test between the number of articles that use person-first language in the UK corpus vs the number that would be expected based on the total number of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n38\n\n\nbroadsheet_expected\n32.35686\n\n\ntabloid_observed\n29\n\n\ntabloid_expected\n34.64314\n\n\nstatistic\n1.903414\n\n\np.value\n0.1676966\n\n\nparameter\n1\n\n\neffect_size\n0.1685502\n\n\n\n\n\n\nThis lack of a difference also holds true when we consider the number of articles that have at least one instance of person-first language.\nIf we consider a t-test on the frequency (per million words) of the UK corpus (including all texts in each sub-corpus, including those without instances), a difference is not detected.\nCodettest_broad_vs_tabl(frequency_table, \"UK\", \"pers_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between broadsheet and tabloid (mean of x = 10.68, mean of y = 14.65) suggests that the effect is negative, statistically not significant, and very small (difference = -3.98, 95% CI [-11.56, 3.61], t(11408.91) = -1.03, p = 0.304; Cohen’s d = -0.02, 95% CI [-0.06, 0.02])\nIf we use a non-parametric test, there is also no difference detected.\n\nCodefp_test_broad_vs_tabl(freq_table = frequency_table,\n        corpus_label = \"UK\",\n        dataset = \"pers\",\n        myformula = formula(pers_freq ~ source_type),\n        dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  pers_freq by source_type (broadsheet, tabloid)\nZ = -1.0151, p-value = 0.3082\nalternative hypothesis: true mu is not equal to 0\n\n\nAustralian vs UK broadsheets\nLet’s compare Australian and UK broadsheets.\nIs there a difference in the frequency of use of person-first language in broadsheets in the Australian vs UK corpus?\n\nCode# broadsheets\nget_chisq_aus_uk(pers_4chisq_matrix_sumhits, \n       row_no = 1,\n       type = \"wc\")|>\n  kable()\n\n\n\nTable 14: Results of Chi-square test between the number of instances of person-first language in broadsheets in UK and Australia vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\nbroadsheet\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n30\n\n\nAUS_expected\n17.76848\n\n\nUK_observed\n41\n\n\nUK_expected\n53.23152\n\n\nstatistic\n11.23053\n\n\np.value\n0.000804625\n\n\nparameter\n1\n\n\neffect_size\n0.3977141\n\n\n\n\n\n\nThere is a difference between the expected and observed frequency of instances of person-first language in Australian vs UK broadsheets(p < 0.005).\nWe can see that person-first language is used more frequently than we would expect in Australian broadsheets, and less frequently in UK ones.\n\nCode# broadsheets\nget_chisq_aus_uk(pers_4chisq_matrix_noarticles, \n       row_no = 1,\n       type = \"art\")|>\n  kable()\n\n\n\nTable 15: Results of Chi-square test between the number of articles that use person-first language in broadsheets in UK and Australia vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\nbroadsheet\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n28\n\n\nAUS_expected\n14.78842\n\n\nUK_observed\n38\n\n\nUK_expected\n51.21158\n\n\nstatistic\n15.21121\n\n\np.value\n9.613107e-05\n\n\nparameter\n1\n\n\neffect_size\n0.4800759\n\n\n\n\n\n\nThis also holds true when we consider the number of articles that have at least one instance of person-first language.\nIf we consider the frequency (per million words) of the Australian vs UK broadsheets (including all texts in each sub-corpus, including those without instances), a very small difference is detected, with Australian broadsheets having a higher frequency than UK ones:\nCodettest_aus_vs_uk(frequency_table, \"broadsheet\", \"pers_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between aus and uk (mean of x = 22.61, mean of y = 10.68) suggests that the effect is positive, statistically significant, and very small (difference = 11.94, 95% CI [1.61, 22.26], t(2541.00) = 2.27, p = 0.023; Cohen’s d = 0.09, 95% CI [0.01, 0.17])\nIs this supported by non-parametric analysis\n\nCodefp_test_aus_vs_uk(freq_table = frequency_table,\n      source_label = \"broadsheet\",\n      dataset = \"pers\",\n      myformula = formula(pers_freq ~ corpus),\n      dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  pers_freq by corpus (AUS, UK)\nZ = 2.453, p-value = 0.0147\nalternative hypothesis: true mu is not equal to 0\n\n\nYes, this difference is supported by the non-parametric FP test, with p(FP) < 0.05.\nAustralian vs UK tabloids\nLet’s compare Australian and UK tabloids.\nIs there a difference in the frequency of use of person-first language in tabloids in the Australian vs UK corpus?\n\nCode# tabloids\nget_chisq_aus_uk(pers_4chisq_matrix_sumhits, \n       row_no = 2,\n       type = \"wc\")|>\n  kable()\n\n\n\nTable 16: Results of Chi-square test between the number of instances of person-first language in tabloids in UK and Australia vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\ntabloid\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n15\n\n\nAUS_expected\n12.72621\n\n\nUK_observed\n30\n\n\nUK_expected\n32.27379\n\n\nstatistic\n0.5664514\n\n\np.value\n0.4516725\n\n\nparameter\n1\n\n\neffect_size\n0.1121954\n\n\n\n\n\n\nThe observed and expected number of instances of person-first language is approximately the same in Australian and British tabloids.\nThis also holds true when we consider the number of articles that have at least one instance of person-first language:\n\nCode# tabloids\nget_chisq_aus_uk(pers_4chisq_matrix_noarticles, \n       row_no = 2,\n       type = \"art\")|>\n  kable()\n\n\n\nTable 17: Results of Chi-square test between the number of articles that use person-first language in tabloids in UK and Australia vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\ntabloid\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n13\n\n\nAUS_expected\n11.85322\n\n\nUK_observed\n29\n\n\nUK_expected\n30.14678\n\n\nstatistic\n0.1545727\n\n\np.value\n0.6942032\n\n\nparameter\n1\n\n\neffect_size\n0.0606655\n\n\n\n\n\n\nIf we consider the frequency (per million words) in the Australian vs UK tabloids (including all texts in each sub-corpus, including those without instances), no significant differences are observed as well.\nCodettest_aus_vs_uk(frequency_table, \"tabloid\", \"pers_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between aus and uk (mean of x = 14.58, mean of y = 14.65) suggests that the effect is negative, statistically not significant, and very small (difference = -0.07, 95% CI [-11.09, 10.95], t(5019.87) = -0.01, p = 0.989; Cohen’s d = -3.72e-04, 95% CI [-0.06, 0.05])\nThe non-parametric test leads to a similar conclusion:\n\nCodefp_test_aus_vs_uk(frequency_table,\n      \"tabloid\",\n      \"pers\",\n      formula(pers_freq ~ corpus),\n      dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  pers_freq by corpus (AUS, UK)\nZ = -0.012715, p-value = 0.9886\nalternative hypothesis: true mu is not equal to 0\n\n\nIn summary, we do not observe a difference in the use of person-first language in Australian vs UK tabloids."
  },
  {
    "objectID": "400_analysis/ConditionAndPersonFirst.html#identity-first-language",
    "href": "400_analysis/ConditionAndPersonFirst.html#identity-first-language",
    "title": "Identity and person-first language and use of nominal adjectival “the obese”",
    "section": "Identity-first language",
    "text": "Identity-first language\nLet’s generate a frequency table (sum of no hits in text) for identity-first language:\n\nCodeiden_4chisq_matrix_sumhits <-\n  both_datasets |> \n  filter(dataset == \"iden\") |>\n  generate_conting_tab_sumhits() |>\n  as.matrix()\n\niden_4chisq_matrix_noarticles <-\n  both_datasets |> \n  filter(dataset == \"iden\") |>\n  generate_conting_tab_noarticles() |>\n  as.matrix()\n\niden_4chisq_matrix_sumhits |>\n  kable()\n\n\n\nTable 18: Number of articles that feature identity-first language in UK and Australian tabloids and broadsheets.\n\n\nAUS\nUK\n\n\n\nbroadsheet\n1143\n2320\n\n\ntabloid\n1428\n2399\n\n\n\n\n\n\nAustralian corpus\nWe can assess the difference between these total number of uses of identity-first language in the Australian corpus, using a Chi-square test:\n\nCodeget_chisq_tabloid_broadsheet(iden_4chisq_matrix_sumhits, \n           col_no = 1,\n           type = \"wc\") |>\n  kable()\n\n\n\nTable 19: Results of Chi-square test between the number of instances of identity-first language in the Australian corpus vs the number that would be expected based on total word count of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n1143\n\n\nbroadsheet_expected\n1298.223\n\n\ntabloid_observed\n1428\n\n\ntabloid_expected\n1272.777\n\n\nstatistic\n37.48971\n\n\np.value\n9.189633e-10\n\n\nparameter\n1\n\n\neffect_size\n0.120755\n\n\n\n\n\n\nThere is a higher number of instances of identity-first language used in tabloids, and a lower number of instances in broadsheets in the Australian corpus.\n\nCodeget_chisq_tabloid_broadsheet(iden_4chisq_matrix_noarticles, \n           col_no = 1,\n           type = \"art\")|>\n  kable()\n\n\n\nTable 20: Results of Chi-square test between the number of articles that use identity-first language in the Australian corpus vs the number that would be expected based on the total number of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n541\n\n\nbroadsheet_expected\n569.2122\n\n\ntabloid_observed\n858\n\n\ntabloid_expected\n829.7878\n\n\nstatistic\n2.357493\n\n\np.value\n0.1246824\n\n\nparameter\n1\n\n\neffect_size\n0.0410503\n\n\n\n\n\n\nNo difference is observed, however, when we contrast the number of articles that have at least one instance of identity-first language.\nIf we consider a t-test on the frequency (per million words) of the Australian corpus (including all texts in each sub-corpus, including those without instances), a very small difference is detected, with tabloids having a higher frequency of use of identity-first language vs broadsheets.\nCodettest_broad_vs_tabl(frequency_table, \"AUS\", \"iden_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between broadsheet and tabloid (mean of x = 1076.39, mean of y = 1642.61) suggests that the effect is negative, statistically significant, and very small (difference = -566.22, 95% CI [-747.02, -385.42], t(4274.85) = -6.14, p < .001; Cohen’s d = -0.19, 95% CI [-0.25, -0.13])\n\nCodefp_test_broad_vs_tabl(frequency_table,\n        corpus_label = \"AUS\",\n        dataset = \"iden\", \n        formula(iden_freq ~ source_type), \n        dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  iden_freq by source_type (broadsheet, tabloid)\nZ = -5.6985, p-value < 1e-04\nalternative hypothesis: true mu is not equal to 0\n\n\nUK corpus\nWe can assess the difference between these total number of uses of identity-first language in the UK corpus, using a Chi-square test. There is a higher number of instances of identity-first language used in tabloids, and a lower number of instances in broadsheets, in the UK corpus:\n\nCodeget_chisq_tabloid_broadsheet(iden_4chisq_matrix_sumhits, \n           col_no = 2,\n           type = \"wc\") |>\n  kable()\n\n\n\nTable 21: Results of Chi-square test between the number of instances of identity-first language in the UK corpus vs the number that would be expected based on total word count of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n2320\n\n\nbroadsheet_expected\n2578.804\n\n\ntabloid_observed\n2399\n\n\ntabloid_expected\n2140.196\n\n\nstatistic\n57.26924\n\n\np.value\n3.80053e-14\n\n\nparameter\n1\n\n\neffect_size\n0.110163\n\n\n\n\n\n\nThis also holds true when we consider the number of articles that have at least one instance of identity-first language.\n\nCodeget_chisq_tabloid_broadsheet(iden_4chisq_matrix_noarticles, \n           col_no = 2,\n           type = \"art\") |>\n  kable()\n\n\n\nTable 22: Results of Chi-square test between the number of articles that use identity-first language in the UK corpus vs the number that would be expected based on the total number of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n1279\n\n\nbroadsheet_expected\n1351.261\n\n\ntabloid_observed\n1519\n\n\ntabloid_expected\n1446.739\n\n\nstatistic\n7.473532\n\n\np.value\n0.006261259\n\n\nparameter\n1\n\n\neffect_size\n0.05168197\n\n\n\n\n\n\nIf we consider a t-test on the frequency (per million words) of the UK corpus (including all texts in each sub-corpus, including those without instances), a very small difference is detected, with a higher mean frequency in tabloids than broadsheets.\nCodettest_broad_vs_tabl(frequency_table, \"UK\", \"iden_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between broadsheet and tabloid (mean of x = 788.81, mean of y = 1009.57) suggests that the effect is negative, statistically significant, and very small (difference = -220.75, 95% CI [-307.84, -133.67], t(12394.87) = -4.97, p < .001; Cohen’s d = -0.09, 95% CI [-0.12, -0.05])\nThis is also supported by the FP test:\n\nCodefp_test_broad_vs_tabl(frequency_table,\n        corpus_label = \"UK\",\n        dataset = \"iden\")\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  iden_freq by source_type (broadsheet, tabloid)\nZ = -4.9387, p-value < 1e-04\nalternative hypothesis: true mu is not equal to 0\n\n\nAustralian vs UK broadsheets\nThere is a higher number of uses of identity-first language in Australian broadsheets, and a lower number in UK broadsheets:\n\nCodeget_chisq_aus_uk(iden_4chisq_matrix_sumhits, \n       row_no = 1,\n       type = \"wc\") |>\n  kable()\n\n\n\nTable 23: Results of Chi-square test between the number of instances of identity-first language in broadsheets in UK and Australia vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\nbroadsheet\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n1143\n\n\nAUS_expected\n866.6512\n\n\nUK_observed\n2320\n\n\nUK_expected\n2596.349\n\n\nstatistic\n117.5332\n\n\np.value\n2.19402e-27\n\n\nparameter\n1\n\n\neffect_size\n0.1842273\n\n\n\n\n\n\nThis also holds true when we consider the number of articles that have at least one instance of identity-first language:\n\nCodeget_chisq_aus_uk(iden_4chisq_matrix_noarticles, \n           row_no = 1,\n           type = \"art\") |>\n  kable()\n\n\n\nTable 24: Results of Chi-square test between the number of articles that use identity-first language in broadsheets in UK and Australia vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\nbroadsheet\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n541\n\n\nAUS_expected\n407.8018\n\n\nUK_observed\n1279\n\n\nUK_expected\n1412.198\n\n\nstatistic\n56.06906\n\n\np.value\n6.99695e-14\n\n\nparameter\n1\n\n\neffect_size\n0.1755197\n\n\n\n\n\n\nIf we consider the frequency (per million words) of the Australian vs UK broadsheets (including all texts in each sub-corpus, including those without instances), a very small difference is detected, with Australian broadsheets having a higher frequency than UK ones:\nCodettest_aus_vs_uk(frequency_table, \"broadsheet\", \"iden_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between aus and uk (mean of x = 1076.39, mean of y = 788.81) suggests that the effect is positive, statistically significant, and very small (difference = 287.58, 95% CI [159.94, 415.21], t(2704.03) = 4.42, p < .001; Cohen’s d = 0.17, 95% CI [0.09, 0.25])\nThis is also supported by the FP test:\n\nCodefp_test_aus_vs_uk(freq_table = frequency_table, \n  source_label = \"broadsheet\",\n  dataset = \"iden\",\n  myformula = formula(iden_freq ~ corpus),\n  dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  iden_freq by corpus (AUS, UK)\nZ = 4.5489, p-value < 1e-04\nalternative hypothesis: true mu is not equal to 0\n\n\nAustralian vs UK tabloids\nThere is a higher number of uses of identity-first language in Australian tabloids, and a lower number in UK tabloids:\n\nCodeget_chisq_aus_uk(iden_4chisq_matrix_sumhits, \n       row_no = 2,\n       type = \"wc\") |>\n  kable()\n\n\n\nTable 25: Results of Chi-square test between the number of instances of identity-first language in tabloids in UK and Australia vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\ntabloid\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n1428\n\n\nAUS_expected\n1082.294\n\n\nUK_observed\n2399\n\n\nUK_expected\n2744.706\n\n\nstatistic\n153.9685\n\n\np.value\n2.352981e-35\n\n\nparameter\n1\n\n\neffect_size\n0.2005796\n\n\n\n\n\n\nThis also holds true when we consider the number of articles that have at least one instance of identity-first language:\n\nCodeget_chisq_aus_uk(iden_4chisq_matrix_noarticles, \n           row_no  = 2,\n           type = \"art\") |>\n  kable()\n\n\n\nTable 26: Results of Chi-square test between the number of articles that use identity-first language in tabloids in UK and Australia vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\ntabloid\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n858\n\n\nAUS_expected\n670.8358\n\n\nUK_observed\n1519\n\n\nUK_expected\n1706.164\n\n\nstatistic\n72.75082\n\n\np.value\n1.470972e-17\n\n\nparameter\n1\n\n\neffect_size\n0.1749461\n\n\n\n\n\n\nIf we consider the frequency (per million words) in the Australian vs UK tabloids (including all texts in each sub-corpus, including those without instances), the frequency of use of identity-first language is higher in Australian tabloids, and lower in UK ones.\nCodettest_aus_vs_uk(frequency_table, \"tabloid\", \"iden_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between aus and uk (mean of x = 1642.61, mean of y = 1009.57) suggests that the effect is positive, statistically significant, and small (difference = 633.04, 95% CI [478.14, 787.94], t(3676.67) = 8.01, p < .001; Cohen’s d = 0.26, 95% CI [0.20, 0.33])\nThis is also supported by the FP test:\n\nCodefp_test_aus_vs_uk(freq_table = frequency_table, \n  source_label = \"tabloid\",\n  dataset = \"iden\",\n  myformula = formula(iden_freq ~ corpus),\n  dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  iden_freq by corpus (AUS, UK)\nZ = 9.0711, p-value < 1e-04\nalternative hypothesis: true mu is not equal to 0\n\n\nAs shown above, there is no significant difference in articles lengths between Australian and UK tabloids."
  },
  {
    "objectID": "400_analysis/ConditionAndPersonFirst.html#use-of-nominal-adjectival-the-obese",
    "href": "400_analysis/ConditionAndPersonFirst.html#use-of-nominal-adjectival-the-obese",
    "title": "Identity and person-first language and use of nominal adjectival “the obese”",
    "section": "Use of nominal adjectival “the obese”",
    "text": "Use of nominal adjectival “the obese”\nLet’s generate a frequency table (sum of no hits in text) for use of nominal adjectival “the obese”:\n\nCodeobes_4chisq_matrix_sumhits <-\n  both_datasets |> \n  filter(dataset == \"obes\") |>\n  generate_conting_tab_sumhits() |>\n  as.matrix()\n\nobes_4chisq_matrix_noarticles <-\n  both_datasets |> \n  filter(dataset == \"obes\") |>\n  generate_conting_tab_noarticles() |>\n  as.matrix()\n\nobes_4chisq_matrix_sumhits |>\n  kable()\n\n\n\nTable 27: Number of articles that feature use of “the obese” (nominal adjectival) in UK and Australian tabloids and broadsheets.\n\n\nAUS\nUK\n\n\n\nbroadsheet\n72\n263\n\n\ntabloid\n78\n264\n\n\n\n\n\n\nAustralian corpus\nWe can assess the difference between these total number of uses of nominal adjectival “the obese” language in the Australian corpus, using a Chi-square test:\n\nCodeget_chisq_tabloid_broadsheet(obes_4chisq_matrix_sumhits, \n           col_no = 1,\n           type = \"wc\") |>\n  kable()\n\n\n\nTable 28: Results of Chi-square test between the number of instances of the nominal adjectival “the obese” in Australian broadsheets vs tabloids vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n72\n\n\nbroadsheet_expected\n75.74229\n\n\ntabloid_observed\n78\n\n\ntabloid_expected\n74.25771\n\n\nstatistic\n0.3734969\n\n\np.value\n0.5411043\n\n\nparameter\n1\n\n\neffect_size\n0.04989969\n\n\n\n\n\n\nThere is no difference in the total count of use of nominal adjectival “the obese” in Australian broadsheets vs tabloids.\n\nCodeget_chisq_tabloid_broadsheet(obes_4chisq_matrix_noarticles, \n           col_no = 1,\n           type = \"art\")|>\n  kable()\n\n\n\nTable 29: Results of Chi-square test between the number of articles that use nominal adjectival “the obese” in Australian broadsheets vs tabloids vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n62\n\n\nbroadsheet_expected\n53.30007\n\n\ntabloid_observed\n69\n\n\ntabloid_expected\n77.69993\n\n\nstatistic\n2.394167\n\n\np.value\n0.1217886\n\n\nparameter\n1\n\n\neffect_size\n0.1351891\n\n\n\n\n\n\nThere is no difference in the number of articles that use nominal adjectival “the obese” in Australian broadsheets vs tabloids.\nIf we consider a t-test on the frequency (per million words) of the Australian corpus (including all texts in each sub-corpus, including those without instances), a very small difference is detected (p < 0.05), with tabloids having a higher frequency of use of “the obese” (nominal adjectival) vs broadsheets.\nCodettest_broad_vs_tabl(frequency_table, \"AUS\", \"obes_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between broadsheet and tabloid (mean of x = 54.14, mean of y = 87.95) suggests that the effect is negative, statistically significant, and very small (difference = -33.81, 95% CI [-65.76, -1.87], t(3897.95) = -2.08, p = 0.038; Cohen’s d = -0.07, 95% CI [-0.13, -3.68e-03])\nThis could be attributed in part to the longer article length described for tabloids than for broadsheets above.\nThis is not strongly supported (p < 0.1) via a non-parametric FP test:\n\nCodefp_test_broad_vs_tabl(frequency_table,\n        corpus_label = \"AUS\",\n        dataset = \"obes\", \n        formula(obes_freq ~ source_type), \n        dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  obes_freq by source_type (broadsheet, tabloid)\nZ = -1.8469, p-value = 0.0651\nalternative hypothesis: true mu is not equal to 0\n\n\nUK corpus\nWe can assess the difference between these total number of uses of nominal adjectival “the obese” in the UK corpus between tabloids and broadsheets. Similar to the Australian corpus, no difference is detected.\n\nCodeget_chisq_tabloid_broadsheet(pers_4chisq_matrix_sumhits, \n           col_no = 2,\n           type = \"wc\") |>\n  kable()\n\n\n\nTable 30: Results of Chi-square test between the number of instances of the nominal adjectival “the obese” in UK broadsheets vs tabloids vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n41\n\n\nbroadsheet_expected\n38.79956\n\n\ntabloid_observed\n30\n\n\ntabloid_expected\n32.20044\n\n\nstatistic\n0.2751628\n\n\np.value\n0.5998894\n\n\nparameter\n1\n\n\neffect_size\n0.06225378\n\n\n\n\n\n\nThere is also no difference when we consider the number of articles that have at least one instance of use of “the obese” (nominal adjectival).\n\nCodeget_chisq_tabloid_broadsheet(pers_4chisq_matrix_noarticles, \n           col_no = 2,\n           type = \"art\") |>\n  kable()\n\n\n\nTable 31: Results of Chi-square test between the number of articles that use nominal adjectival “the obese” in UK broadsheets vs tabloids vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n38\n\n\nbroadsheet_expected\n32.35686\n\n\ntabloid_observed\n29\n\n\ntabloid_expected\n34.64314\n\n\nstatistic\n1.903414\n\n\np.value\n0.1676966\n\n\nparameter\n1\n\n\neffect_size\n0.1685502\n\n\n\n\n\n\nIf we consider a t-test on the frequency (per million words) of the UK corpus (including all texts in each sub-corpus, including those without instances), a very small difference is detected, with a higher mean frequency in tabloids than broadsheets.\nCodettest_broad_vs_tabl(frequency_table, \"UK\", \"obes_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between broadsheet and tabloid (mean of x = 78.90, mean of y = 116.11) suggests that the effect is negative, statistically significant, and very small (difference = -37.21, 95% CI [-63.73, -10.69], t(11499.67) = -2.75, p = 0.006; Cohen’s d = -0.05, 95% CI [-0.09, -0.01])\nAgain, like the case of the Australian corpus, this could be explained by articles in tabloids being overall shorter than in broadsheets in the UK corpus.\nThere is some support for this difference when using a non-parametric FP test (p < 0.01):\n\nCodefp_test_broad_vs_tabl(frequency_table,\n        corpus_label = \"UK\",\n        dataset = \"obes\", \n        formula(obes_freq ~ source_type), \n        dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  obes_freq by source_type (broadsheet, tabloid)\nZ = -2.716, p-value = 0.0062\nalternative hypothesis: true mu is not equal to 0\n\n\nAustralian vs UK broadsheets\nNo difference is observed in the use of nominal adjectival “the obese” between Australian and UK broadsheets.\n\nCodeget_chisq_aus_uk(obes_4chisq_matrix_sumhits, \n       row_no = 1,\n       type = \"wc\") |>\n  kable()\n\n\n\nTable 32: Results of Chi-square test between the instances of the nominal adjectival “the obese” in broadsheets in the UK and Australia vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\nbroadsheet\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n72\n\n\nAUS_expected\n83.83718\n\n\nUK_observed\n263\n\n\nUK_expected\n251.1628\n\n\nstatistic\n2.2292\n\n\np.value\n0.1354239\n\n\nparameter\n1\n\n\neffect_size\n0.08157407\n\n\n\n\n\n\nThis lack of a difference also holds true when we consider the number of articles that have at least one instance of “the obese” (nominal adjectival):\n\nCodeget_chisq_aus_uk(obes_4chisq_matrix_sumhits, \n           row_no = 1,\n           type = \"art\") |>\n  kable()\n\n\n\nTable 33: Results of Chi-square test between the number of articles that use nominal adjectival “the obese” in broadsheets in the UK and Australia vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\nbroadsheet\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n72\n\n\nAUS_expected\n75.06242\n\n\nUK_observed\n263\n\n\nUK_expected\n259.9376\n\n\nstatistic\n0.161021\n\n\np.value\n0.6882182\n\n\nparameter\n1\n\n\neffect_size\n0.02192395\n\n\n\n\n\n\nIf we consider the frequency (per million words) of the Australian vs UK broadsheets (including all texts in each sub-corpus, including those without instances), a very small difference is detected, with UK broadsheets having a higher frequency than Australian ones:\nCodettest_aus_vs_uk(frequency_table, \"broadsheet\", \"obes_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between aus and uk (mean of x = 54.14, mean of y = 78.90) suggests that the effect is negative, statistically significant, and very small (difference = -24.76, 95% CI [-47.03, -2.50], t(5142.60) = -2.18, p = 0.029; Cohen’s d = -0.06, 95% CI [-0.12, -6.13e-03])\nThe FP test does not detect this difference:\n\nCodefp_test_aus_vs_uk(freq_table = frequency_table, \n  source_label = \"broadsheet\",\n  dataset = \"obes\",\n  myformula = formula(obes_freq ~ corpus),\n  dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  obes_freq by corpus (AUS, UK)\nZ = -1.622, p-value = 0.1045\nalternative hypothesis: true mu is not equal to 0\n\n\nThis discrepancy may be explained by the fact that, as discussed above, articles in UK broadsheets are somewhat shorter than in Australian ones.\nAustralian vs UK tabloids\nThere is a somewhat lower number of total instances of use of “the obese” (nominal adjectival) in Australian tabloids, and a higher number in UK tabloids (p < 0.05):\n\nCodeget_chisq_aus_uk(obes_4chisq_matrix_sumhits, \n       row_no = 2,\n       type = \"wc\") |>\n  kable()\n\n\n\nTable 34: Results of Chi-square test between the number of instances of the nominal adjectival “the obese” in tabloids in the UK and Australia vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\ntabloid\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n78\n\n\nAUS_expected\n96.71923\n\n\nUK_observed\n264\n\n\nUK_expected\n245.2808\n\n\nstatistic\n5.05156\n\n\np.value\n0.02460378\n\n\nparameter\n1\n\n\neffect_size\n0.1215345\n\n\n\n\n\n\nThis is somewhat supported (p < 0.1) when we consider the number of articles that have at least one instance of “the obese” (nominal adjectival):\n\nCodeget_chisq_aus_uk(obes_4chisq_matrix_noarticles, \n           row_no  = 2,\n           type = \"art\") |>\n  kable()\n\n\n\nTable 35: Results of Chi-square test between the number of articles that use nominal adjectival “the obese” in tabloids in the UK and Australia vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\ntabloid\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n69\n\n\nAUS_expected\n82.4081\n\n\nUK_observed\n223\n\n\nUK_expected\n209.5919\n\n\nstatistic\n3.039293\n\n\np.value\n0.08127128\n\n\nparameter\n1\n\n\neffect_size\n0.1020222\n\n\n\n\n\n\nIf we consider the frequency (per million words) in the Australian vs UK tabloids (including all texts in each sub-corpus, including those without instances), no significant differences in frequency are observed.\nCodettest_aus_vs_uk(frequency_table, \"tabloid\", \"obes_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between aus and uk (mean of x = 87.95, mean of y = 116.11) suggests that the effect is negative, statistically not significant, and very small (difference = -28.16, 95% CI [-63.20, 6.88], t(5750.29) = -1.58, p = 0.115; Cohen’s d = -0.04, 95% CI [-0.09, 0.01])\nThis is also supported by the FP test:\n\nCodefp_test_aus_vs_uk(freq_table = frequency_table, \n  source_label = \"tabloid\",\n  dataset = \"obes\",\n  myformula = formula(obes_freq ~ corpus),\n  dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  obes_freq by corpus (AUS, UK)\nZ = -1.4319, p-value = 0.1538\nalternative hypothesis: true mu is not equal to 0\n\n\nAs shown above, there is no significant difference in articles lengths between Australian and UK tabloids."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Obesity comparison of UK and Australian corpora",
    "section": "",
    "text": "Please see the rendered analysis results and code here."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "The Sydney Informatics Hub is a Core Research Facility of the University of Sydney.\nThe use of the SIH services including the Artemis HPC and associated support and training warrants acknowledgement in any publications, conference proceedings or posters describing work facilitated by these services.\nThe continued acknowledgement of the use of SIH facilities ensures the sustainability of our services."
  },
  {
    "objectID": "about.html#suggested-wording",
    "href": "about.html#suggested-wording",
    "title": "Sydney Informatics Hub",
    "section": "Suggested wording",
    "text": "Suggested wording\n\nGeneral acknowledgement:\nThe authors acknowledge the technical assistance provided by the Sydney Informatics Hub, a Core Research Facility of the University of Sydney.\n\n\nAcknowledging specific staff:\nThe authors acknowledge the technical assistance of (name of staff) of the Sydney Informatics Hub, a Core Research Facility of the University of Sydney.\nFor further information about acknowledging the Sydney Informatics Hub, please contact us at sih.info@sydney.edu.au."
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/dataEDA.html",
    "href": "100_data_cleaning_scripts_EDA/dataEDA.html",
    "title": "Loading and exploring the data",
    "section": "",
    "text": "In this file, we start by loading and exploring the data for the UK and Australian obesity corpus.\nThe Australian corpus was first explored (for texts that had >= 3 mentions of the word “obes*“, to maintain consistency with the UK corpus) using the same syntax as in our previous manuscript. This resulted in the”May” dataset (note spaces have been added after the pipe to enable line breaks below; spaces were not part of the original query).\n\nMay Condition-first: “obese (adolescent| adolescents| adult| adults| American| Americans| amputee| amputees| asthmatic| asthmatics| aussie| aussies| australian| Australians| banker| bankers| boss| bosses| boy| boys| Brit| Brits| Canberran| Canberrans| child| children| citizen| citizens| client| clients| contestant| contestants| customer| customers| dad| dads| daughter| daughters| diabetic| diabetics| dieter| dieters| driver| drivers| employee| employees| fan| fans| father| fathers| Frenchman| Frenchmen| Frenchwomen| friend| friends| girl| girls| guy| guys| individual| individuals| kid| kids| ladies| lady| man| men| model| models| mother| mothers| motorist| motorists| mum| mums| pal| pals| parent| parents| participant| participants| passenger| passengers| patient| patients| people| person| persons| preschooler| preschoolers| Queenslander| Queenslanders| resident| residents| smoker| smokers| socialite| socialites| soldier| soldiers| son| sons| student| students| subject| subjects| Tasmanian| Tasmanians| teen| teenager| teenagers| teens| traveller| travellers| Victorian| Victorians| volunteer| volunteers| woman| women| worker| workers| youngster| youngsters)”\nMay Person-first: “(adolescent| adolescents| adult| adults| American| Americans| amputee| amputees| asthmatic| asthmatics| aussie| aussies| australian| Australians| banker| bankers| boss| bosses| boy| boys| Brit| Brits| Canberran| Canberrans| child| children| citizen| citizens| client| clients| contestant| contestants| customer| customers| dad| dads| daughter| daughters| diabetic| diabetics| dieter| dieters| driver| drivers| employee| employees| fan| fans| father| fathers| Frenchman| Frenchmen| Frenchwomen| friend| friends| girl| girls| guy| guys| individual| individuals| kid| kids| ladies| lady| man| men| model| models| mother| mothers| motorist| motorists| mum| mums| pal| pals| parent| parents| participant| participants| passenger| passengers| patient| patients| people| person| persons| preschooler| preschoolers| Queenslander| Queenslanders| resident| residents| smoker| smokers| socialite| socialites| soldier| soldiers| son| sons| student| students| subject| subjects| Tasmanian| Tasmanians| teen| teenager| teenagers| teens| traveller| travellers| Victorian| Victorians| volunteer| volunteers| woman| women| worker| workers| youngster| youngsters| those| many) with * obesity”, in subcorpus “Tara Parallel Corpus”\n\nIt was then explored using the same syntax as the UK corpus, resulting in the early July dataset.\nFinally, a different syntax was used to generate the final UK and Australian corpora. This “late July” dataset was used for subsequent comparison and analysis in the paper: ‘[word=“obese”%c] [fullsemtag=“.*S2.*”]’\n\nPerson-first language: UK vs Australia: i.e. uk_with_obesity_new.txt vs aus_subset_with_obesity_new.txt.\n“The nominal adjectival ‘the obese’”: UK vs Australia i.e uk_the obese text-frequency-data.txt vs aoc_subset_the_obese_cat_nominal.txt\n\n\nCodelibrary(here)\nlibrary(janitor)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(patchwork)\ntheme_set(theme_minimal())\n\n\n\n\nread_cqpweb <- function(filename){\n  read.csv(\n    here(\"100_data_raw\", filename), \n    skip = 3, sep = \"\\t\") |> \n    janitor::clean_names() |> \n  rename(article_id = text)\n}\n\n\n\nCodemay_aus_cond <- read_cqpweb(\"may/aoc_subset_condition_first.txt\")\njul_aus_cond <- read_cqpweb(\"july/aoc_subset_condition_first.txt\")\nuk_cond <- read_cqpweb(\"british_data/uk_condition first lang text-frequency-data.txt\") \n  \nmay_aus_pers <- read_cqpweb(\"may/aoc_subset_person_first.txt\")\njul_aus_pers <- read_cqpweb(\"july/aoc_subset_person_first.txt\")\nuk_pers <- read_cqpweb(\"british_data/uk_person first language text-frequency-data.txt\") \n\nmay_aus_obese <- read_cqpweb(\"may/aoc_subset_the_obese_cat_nominal.txt\")\njul_aus_obese <- read_cqpweb(\"july/aoc_subset_the_obese_cat_nominal.txt\")\nuk_obes <- read_cqpweb(\"british_data/uk_the obese nominal text-frequency-data.txt\") \n\n# grab the late july datasets\nlatejul_uk_cond <- read_cqpweb(\"new_syntax_july22/uk_obese_first_new.txt\")\nlatejul_au_cond <- read_cqpweb(\"new_syntax_july22/aus_subset_obese_first_new.txt\")\nlatejul_uk_pers <- read_cqpweb(\"new_syntax_july22/uk_with_obesity_new.txt\")\nlatejul_au_pers <- read_cqpweb(\"new_syntax_july22/aus_subset_with_obesity_new.txt\")\nlatejul_uk_theobese <- read_cqpweb(\"new_syntax_july22/uk_the obese nominal text-frequency-data.txt\")\nlatejul_au_theobese <- read_cqpweb(\"new_syntax_july22/aoc_subset_the_obese_cat_nominal.txt\")\nlatejul_uk_fullwc <- read_cqpweb(\"new_syntax_july22/UK corpus text-frequency-data.txt\") |> select(article_id, no_words_in_text)\nlatejul_au_fullwc <- read_cqpweb(\"new_syntax_july22/aus_subset_text_frequency_data.txt\") |> select(article_id, no_words_in_text)\n\n# this is the early JULY dataset!\nall_aus <- rbind({jul_aus_cond |> mutate(dataset = \"cond\")}, \n                {jul_aus_pers |> mutate(dataset = \"pers\")}, \n                {jul_aus_obese |> mutate(dataset = \"obes\")})\n\n# late july datasets\nlatejul_all_au <- rbind(\n  {latejul_au_cond |> mutate(dataset = \"cond\")}, \n  {latejul_au_pers |> mutate(dataset = \"pers\")}, \n  {latejul_au_theobese |> mutate(dataset = \"obes\")})\n\nlatejul_all_uk <- rbind(\n  {latejul_uk_cond |> mutate(dataset = \"cond\")}, \n  {latejul_uk_pers |> mutate(dataset = \"pers\")}, \n  {latejul_uk_theobese |> mutate(dataset = \"obes\")})\n\n# joined datasets\njoined_cond <- dplyr::full_join(\n  dplyr::full_join(may_aus_cond, \n                   jul_aus_cond, by = \"article_id\", suffix = c(\"may\", \"jul\")),\n  # using suffix = c(\"may\", \"jul\") doesn't work\n  latejul_au_cond, by = \"article_id\") |> rename(\n        no_words_in_textlatejul = no_words_in_text,\n        no_hits_in_textlatejul = no_hits_in_text,\n        freq_per_million_wordslatejul = freq_per_million_words\n  )\n\njoined_pers <- dplyr::full_join(full_join(may_aus_pers, jul_aus_pers, by = \"article_id\", suffix = c(\"may\", \"jul\")),\n  # using suffix = c(\"may\", \"jul\") doesn't work\n  latejul_au_pers, by = \"article_id\") |> rename(\n        no_words_in_textlatejul = no_words_in_text,\n        no_hits_in_textlatejul = no_hits_in_text,\n        freq_per_million_wordslatejul = freq_per_million_words\n  )\njoined_obese <- dplyr::full_join(full_join(may_aus_obese, jul_aus_obese, by = \"article_id\", suffix = c(\"may\", \"jul\")),\n  # using suffix = c(\"may\", \"jul\") doesn't work\n  latejul_au_theobese, by = \"article_id\") |> rename(\n        no_words_in_textlatejul = no_words_in_text,\n        no_hits_in_textlatejul = no_hits_in_text,\n        freq_per_million_wordslatejul = freq_per_million_words\n  )\n\n# australian metadata\naus_tabloid_anno <- read_csv(here::here(\"100_data_raw/aus_subset_data\",\n                                    \"addition_source_metadata.csv\")) |>\n  select(source, source_type)\n\naus_metadata <- read_csv(here::here(\"100_data_raw/aus_subset_data\",\n                                    \"corpus_3plus_metadata.csv\")) |> filter(obesity_boolean_ukcorpus == TRUE) |>\n  rename(source_code = shortcode) |>\n  select(article_id, source_code, year, month_metadata, source, date) |>\n  rename(month = month_metadata) |>\n  left_join(aus_tabloid_anno, by = \"source\") |>\n  left_join(latejul_au_fullwc, by = \"article_id\")\n\n# this is a file I created from information in a spreadsheet Monika provided manually\nuk_source_anno <- read_csv(here::here(\"100_data_raw\", \"british_corpus_source_annotation.csv\"))"
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/dataEDA.html#normalised-frequency",
    "href": "100_data_cleaning_scripts_EDA/dataEDA.html#normalised-frequency",
    "title": "Loading and exploring the data",
    "section": "Normalised frequency",
    "text": "Normalised frequency\nWhat is the normalised frequency of each of the language types?\n\nCodeget_norm_freq <- function(df, wc_aus_broad, wc_aus_tab, wc_uk_broad, wc_uk_tab){\n  df |> \n  group_by(dataset, corpus, source_type) |>\n  summarise(total_instances = sum(no_hits_in_text)) |>\n  # create tmp column b/c case_when doesn't work with multiple conditions\n  mutate(tmp = paste0(corpus, source_type)) |>\n  ungroup() |>\n  mutate(total_words_in_corpus = case_when(\n    tmp == \"AUSbroadsheet\" ~ wc_aus_broad,\n    tmp == \"AUStabloid\" ~ wc_aus_tab,\n    tmp == \"UKbroadsheet\" ~ wc_uk_broad,\n    tmp == \"UKtabloid\" ~ wc_uk_tab\n  )) |>\n  select(-tmp) |>\n  mutate(normalised_corpus_per_million = 10^6*total_instances/total_words_in_corpus) \n}\n\nfreq_normalised <- get_norm_freq(both_datasets,\n                                 total_words_aus_broad,\n                                 total_words_aus_tab,\n                                 total_words_uk_broad,\n                                 total_words_uk_tab)\n  \nfreq_normalised_bothyears <- both_datasets |> \n  filter(year >= 2008 & year <=2016) |>\n  get_norm_freq(\n    total_words_aus_broad_bothyears,\n    total_words_aus_tab_bothyears,\n    total_words_uk_broad_bothyears,\n    total_words_uk_tab_bothyears)\n\nfreq_normalised |>\n  kable()\n\n\n\nTable 18: Normalised frequency, instances per million words as counted using CQP-web, across the entire UK and Australian corpora.\n\n\n\n\n\n\n\n\n\ndataset\ncorpus\nsource_type\ntotal_instances\ntotal_words_in_corpus\nnormalised_corpus_per_million\n\n\n\ncond\nAUS\nbroadsheet\n1360\n1860166\n731.117545\n\n\ncond\nAUS\ntabloid\n1718\n1758364\n977.044571\n\n\ncond\nUK\nbroadsheet\n2859\n5427551\n526.756911\n\n\ncond\nUK\ntabloid\n2892\n4377933\n660.585715\n\n\nobes\nAUS\nbroadsheet\n77\n1860166\n41.394155\n\n\nobes\nAUS\ntabloid\n89\n1758364\n50.615231\n\n\nobes\nUK\nbroadsheet\n315\n5427551\n58.037225\n\n\nobes\nUK\ntabloid\n317\n4377933\n72.408600\n\n\npers\nAUS\nbroadsheet\n68\n1860166\n36.555877\n\n\npers\nAUS\ntabloid\n29\n1758364\n16.492603\n\n\npers\nUK\nbroadsheet\n45\n5427551\n8.291032\n\n\npers\nUK\ntabloid\n30\n4377933\n6.852549\n\n\n\n\n\n\nWhat about the normalised frequency across the time period where we have data for both corpora?\n\nCodefreq_normalised_bothyears |>\n  kable()\n\n\n\nTable 19: Normalised frequency, instances per million words as counted using CQP-web, across the UK and Australian corpora between 2008 and 2016, when we have data from both countries (i.e. the final corpus used for comparison).\n\n\n\n\n\n\n\n\n\ndataset\ncorpus\nsource_type\ntotal_instances\ntotal_words_in_corpus\nnormalised_corpus_per_million\n\n\n\ncond\nAUS\nbroadsheet\n1143\n1446391\n790.242749\n\n\ncond\nAUS\ntabloid\n1428\n1418041\n1007.023069\n\n\ncond\nUK\nbroadsheet\n2320\n4333157\n535.406402\n\n\ncond\nUK\ntabloid\n2399\n3596164\n667.099721\n\n\nobes\nAUS\nbroadsheet\n72\n1446391\n49.779071\n\n\nobes\nAUS\ntabloid\n78\n1418041\n55.005462\n\n\nobes\nUK\nbroadsheet\n263\n4333157\n60.694778\n\n\nobes\nUK\ntabloid\n264\n3596164\n73.411557\n\n\npers\nAUS\nbroadsheet\n30\n1446391\n20.741280\n\n\npers\nAUS\ntabloid\n15\n1418041\n10.577973\n\n\npers\nUK\nbroadsheet\n41\n4333157\n9.461923\n\n\npers\nUK\ntabloid\n30\n3596164\n8.342222\n\n\n\n\n\n\n\nCodeexport_summary_stats <- function(freq_calc1, freq_norm, outfile){\n  rbind(\n  {freq_calc1 %>%\n  pivot_longer(cols = c(\"total_instances\", \"total_articles_with_one_or_more_instance\"))},\n  {freq_norm %>% \n  select(-total_instances, -total_words_in_corpus) %>%\n  pivot_longer(cols = \"normalised_corpus_per_million\")}) %>%\n  pivot_wider(names_from = corpus, values_from = value) %>%\n  # reorder in the order Monika has in her xlsx file\n  mutate(dataset = factor(dataset, levels = c(\"pers\", \"cond\", \"obes\")),\n         source_type = factor(source_type, levels = c(\"tabloid\", \"broadsheet\")),\n         name = factor(name,\n                       levels = c(\"total_instances\",\n                                  \"normalised_corpus_per_million\",\n                                  \"total_articles_with_one_or_more_instance\"))\n         ) %>%\n  arrange(dataset, source_type, name) %>%\n  write_csv(file = here::here(\"300_data_processed\", paste0(outfile, \".csv\")))\n}\n\n# export so Monika can copy-paste into csv file\nexport_summary_stats(freq_calc1 = freq_calc,\n                     freq_norm = freq_normalised,\n                     outfile = \"all_data\")\n\nexport_summary_stats(freq_calc1 = freq_calc_bothyears,\n                     freq_norm = freq_normalised_bothyears,\n                     outfile = \"overlapping_years\")\n\n# how many words in corpus?\nrbind(\n  {freq_normalised_bothyears |> \n  select(corpus, source_type, total_words_in_corpus) |>\n  distinct() |>\n  mutate(period = \"2008-2016\")},\n  {freq_normalised |>\n  select(corpus, source_type, total_words_in_corpus) |>\n  distinct() |>\n  mutate(period = \"alltime\")}\n) %>%\n  pivot_wider(names_from = period, values_from = total_words_in_corpus) |>\n  write_csv(file = here::here(\"300_data_processed\", \"wordcounts_corpus.csv\"))\n\n\nA small sanity check about the differences:\nAs discussed above:\n\nThere is no data from Australia for 2006-2007\nBoth countries have data from 2008-2016\nThere is no data from the UK for 2017-2019\n\nHow many articles with how many words do we have in Australia in 2017 - 2019?\n\nCodeaus_missing_summary <- aus_metadata |>\n  filter(year >= 2017) |>\n  group_by(year, source_type) |> \n  summarise(count = n(),\n            wc = sum(no_words_in_text)) |>\n  pivot_wider(names_from = source_type, values_from = c(\"count\", \"wc\"))\n\naus_full_summary <- aus_metadata |>\n  group_by(year, source_type) |> \n  summarise(count = n(),\n            wc = sum(no_words_in_text)) |>\n  pivot_wider(names_from = source_type, values_from = c(\"count\", \"wc\"))\n\naus_missing_summary |> kable()\n\n\n\nTable 20: Number of articles and their word count, from Australian tabloids and broadsheets, filtered out from the dataset due to no matching data existing from the UK.\n\nyear\ncount_broadsheet\ncount_tabloid\nwc_broadsheet\nwc_tabloid\n\n\n\n2017\n179\n218\n147728\n122405\n\n\n2018\n182\n246\n170085\n140285\n\n\n2019\n108\n166\n95962\n77633\n\n\n\n\n\n\nTotal filtered out from Australian corpus:\n\nCodesummary_clean <- function(df) {\n  x <- df |>\n    ungroup() |>\n    summarise(across(everything(), sum)) |>\n    select(-year) |>\n    data.table::transpose() \n  x$variable <- colnames(df)[colnames(df) != \"year\"]\n  x |> select(variable, V1)\n}\n\nsummary_clean(aus_full_summary) |>\n  rename(full_dataset = V1) |>\n  left_join(\n  {summary_clean(aus_missing_summary) |>\n  rename(filtered_out = V1)}\n  ) |> mutate(\n    remaining = full_dataset - filtered_out,\n    perc_filt_out = round(100* filtered_out/full_dataset, digits = 2)\n  ) |> kable()\n\n\n\nTable 21: Proportion of data filtered out from the Australian corpus.\n\n\n\n\n\n\n\n\nvariable\nfull_dataset\nfiltered_out\nremaining\nperc_filt_out\n\n\n\ncount_broadsheet\n2210\n469\n1741\n21.22\n\n\ncount_tabloid\n3168\n630\n2538\n19.89\n\n\nwc_broadsheet\n1860166\n413775\n1446391\n22.24\n\n\nwc_tabloid\n1758364\n340323\n1418041\n19.35\n\n\n\n\n\n\nHow many articles with how many words do we have in the UK in 2006 - 2007?\n\nCodeuk_missing_summary <- full_metadata |>\n  filter(corpus == \"UK\") |>\n  filter(year <= 2007) |>\n  group_by(year, source_type) |> \n  summarise(count = n(),\n            wc = sum(no_words_in_text)) |>\n  pivot_wider(names_from = source_type, values_from = c(\"count\", \"wc\"))\n\nuk_missing_summary |> kable()\n\n\n\nTable 22: Number of articles and their word count, from UK tabloids and broadsheets, filtered out from the dataset due to no matching data existing from Australia.\n\nyear\ncount_broadsheet\ncount_tabloid\nwc_broadsheet\nwc_tabloid\n\n\n\n2006\n773\n571\n554728\n338643\n\n\n2007\n754\n775\n539666\n443126\n\n\n\n\n\n\nTotal filtered out from UK corpus:\n\nCodeuk_full_summary <-  full_metadata |>\n  filter(corpus == \"UK\") |>\n  group_by(year, source_type) |> \n  summarise(count = n(),\n            wc = sum(no_words_in_text)) |>\n  pivot_wider(names_from = source_type, values_from = c(\"count\", \"wc\"))\n\nsummary_clean(uk_full_summary) |>\n  rename(full_dataset = V1) |>\n  left_join(\n  {summary_clean(uk_missing_summary) |>\n  rename(filtered_out = V1)}\n  ) |> mutate(\n    remaining = full_dataset - filtered_out,\n    perc_filt_out = round(100* filtered_out/full_dataset, digits = 2)\n  ) |> kable()\n\n\n\nTable 23: Proportion of data filtered out from the UK corpus.\n\n\n\n\n\n\n\n\nvariable\nfull_dataset\nfiltered_out\nremaining\nperc_filt_out\n\n\n\ncount_broadsheet\n7556\n1527\n6029\n20.21\n\n\ncount_tabloid\n7801\n1346\n6455\n17.25\n\n\nwc_broadsheet\n5427551\n1094394\n4333157\n20.16\n\n\nwc_tabloid\n4377933\n781769\n3596164\n17.86"
  },
  {
    "objectID": "000_scoping/scoping.html",
    "href": "000_scoping/scoping.html",
    "title": "PIPE-3034: Obesity stigma analysis with pattern frequency over time and publication",
    "section": "",
    "text": "Scope link, Sharepoint Usyd"
  },
  {
    "objectID": "000_scoping/scoping.html#project-details",
    "href": "000_scoping/scoping.html#project-details",
    "title": "PIPE-3034: Obesity stigma analysis with pattern frequency over time and publication",
    "section": "1. Project details",
    "text": "1. Project details\n\nClients: Monika Bednarek\nFaculty: FASS\nCollaborators and their affiliations: Carly Bray (USyd PhD Student), Tara (Lancaster),"
  },
  {
    "objectID": "000_scoping/scoping.html#project-scoping-details",
    "href": "000_scoping/scoping.html#project-scoping-details",
    "title": "PIPE-3034: Obesity stigma analysis with pattern frequency over time and publication",
    "section": "Project scoping details",
    "text": "Project scoping details\n\nScope prepared by Joel Nothman, Feb 2022\nProject manager - Marius Mather"
  }
]