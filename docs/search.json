[
  {
    "objectID": "400_analysis/ConditionAndPersonFirst.html",
    "href": "400_analysis/ConditionAndPersonFirst.html",
    "title": "Condition and person-first language & use of the adjective lemma OBESE",
    "section": "",
    "text": "Both corpora had data from 2008-2016, which was used for the analyses below.\n\nWe use the following tests to compare the sub-corpora:\n\nChi-square goodness-of-fit tests, considering the total sub-corpus word count as the probability of observing a specific feature.\nParametric t-tests comparing the mean frequencies, with texts without the feature considered to have a value of 0.\nThe non-parametric Fisher-Pitman Permutation Test test with 10000 re-samples, comparing the mean frequencies, with texts without the feature considered to have a value of 0.\n\n\n\nArticles in Australian broadsheets are longer than in Australian tabloids (mean 841.70 vs 555.04, respectively, p(FP) < 1e-05).\nArticles in UK broadsheets are longer than in UK tabloids (mean 718.31 vs 561.20, respectively, p(FP) < 1e-04).\nArticles in Australian broadsheets are longer than in UK broadsheets (mean 841.70 vs 718.31, respectively, p(FP) < 1e-04).\nThere is no difference in length between articles in Australian and UK tabloids (mean 555.04 vs 561.20, respectively).\n\n\nAustralian broadsheets have somewhat higher total number of instances (p(Chi-sq) < 0.05) and number of articles (p < 0.001) using person-first language than Australian tabloids, however, this difference is not supported when considering the mean frequency of language use across the corpus with parametric or non-parametric tests.\nIn the UK corpus, no differences in the number of instances or articles are observed between tabloids and broadsheets, when considering contingency table and frequency based analyses similar to the above.\nAustralian broadsheets have somewhat higher total number of instances (p(Chi-sq) < 0.005) and number of articles (p < 0.0005) using person-first language than UK broadsheets. This difference is supported when considering the mean frequency of language use (mean Australian broadsheets = 17.81 words per million (wpm), mean UK broadsheets 8.52 wpm) across the corpus with parametric and non-parametric tests (p(FP) < 0.05)).\nThere is no difference between the number of instances, articles or frequency of between Australian and UK tabloids.\n\n\nAustralian broadsheets have a lower than expected total number of instances (p(Chi-sq) < 0.001) and number of articles (p < 0.1) using condition-first language vs Australian tabloids. If we instead look at the frequency of use, the opposite result is observed, with a higher frequency of condition-first language in tabloids than in broadsheets (mean broadsheets = 847.96 wpm vs 1315.95 wpm in tabloids, p(FP) < 1e-04). This discrepancy may be attributable to the fact that articles in broadsheets are longer than those in tabloids (mean 841.70 words for broadsheets vs 555.04 for tabloids), as described above, so comparing mean frequencies of the same number of instances results in a difference frequency because the word count being divided by for broadsheets is 1.5x than that of tabloids.\nIn the UK corpus, there is a higher number than expected of instances and articles containing at least one instance of condition-first language used in tabloids, and a lower number in broadsheets (p < 0.0001). A very small difference in frequency is also detected, with a higher mean frequency in tabloids than broadsheets (mean broadsheets 629.40 wpm, mean of tabloids 835.37 wpm, p(FP) < 1e-04).\nThere is a higher number of instances and articles with at least one instance (p(Chi-sq) < 0.001) of condition-first language in Australian vs UK broadsheets. If we consider the frequency, a difference is also detected, with Australian broadsheets having a higher frequency than UK ones (mean Australian 847.96 wpm, mean UK 629.40 wpm, p(FP) < 1e-04).\nThere is a higher number of instances and articles that use condition-first language in Australian tabloids, and a lower number in UK tabloids (p(Chi-sq) < 0.001). The frequency is also higher in Australian tabloids (mean Australian 1315.95 wpm, mean UK 835.37 wpm, p(FP) < 1e-04). This is especially striking given that there is no difference in overall article length between Australian and UK tabloids.\n\n\nIn the Australian corpus, no differences in the number of instances or articles are observed between tabloids and broadsheets when considering contingency table based analysis. There is a very small difference detected (p(t-test) < 0.05, p(FP) < 0.1) when considering frequencies, with tabloids having a higher frequency of use of condition-first language vs broadsheets (mean broadsheet frequency 42.65 wpm, mean tabloid 70.46 wpm). This discrepancy is most likely at least partially explained by the much longer article lengths of broadsheets vs tabloids in this corpus.\nSimilarly, in the UK corpus, no differences in the number of instances or articles are observed between tabloids and broadsheets when considering contingency table based analysis. There is a very small difference detected (p(t-test) < 0.01, p(FP) < 0.01) when considering frequencies, with tabloids having a higher frequency of use of the adjective lemma OBESE vs broadsheets (mean broadsheet frequency 62.96 wpm, mean tabloid 96.08 wpm). This discrepancy is most likely at least partially explained by the much longer article lengths of broadsheets vs tabloids in this corpus, similar to the Australian corpus.\nWhen comparing use of the adjective lemma OBESE in the Australian vs UK broadsheets, no differences in the number of instances or articles are observed between tabloids and broadsheets when considering contingency table based analysis. There is a very small difference detected (p(t-test) < 0.05, p(FP) < 0.1) when considering frequencies, with UK broadsheets having a higher frequency of use of the adjective lemma OBESE vs Australian ones (mean frequency Australia - 42.65 wpm, UK - 62.96 wpm). This discrepancy may be explained by the fact that, as discussed above, articles in UK broadsheets are somewhat shorter than in Australian ones.\nThere is a somewhat lower number of total instances of use and articles with at least one instance of use of the adjective lemma OBESE in Australian tabloids, and a higher number in UK tabloids (p(Chi-sq) < 0.05). However, this difference is not supported by frequency analysis (mean Australian 70.46 wpm, mean UK 96.08 wpm), when using both parametric or non-parametric tests.\n\n\nCodelibrary(here)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(corrplot)\nlibrary(coin)\nlibrary(ggvenn)\ntheme_set(theme_minimal())\n\n\n\nCodemetadata <-read_csv(here::here(\"200_data_clean/\", \"full_metadata.csv\"))\nboth_datasets <-  read_csv(here::here(\"200_data_clean/\", \"both_datasets.csv\")) |> \n  # get data for correct time window\n  filter(year >= 2008 & year <=2016)\n\nsource(here::here(\"400_analysis\", \"functions.R\"))\n\n# get some wordcounts \nau_tabloid_wordcounts <- get_wc(\n  corpuslabel = \"AUS\",\n  sourcetypelabel = \"tabloid\")\n\nau_broadsheet_wordcounts <- get_wc(\n  corpuslabel = \"AUS\",\n  sourcetypelabel = \"broadsheet\")\n\nuk_tabloid_wordcounts <- get_wc(\n  corpuslabel = \"UK\",\n  sourcetypelabel = \"tabloid\")\n\nuk_broadsheet_wordcounts <- get_wc(\n  corpuslabel = \"UK\",\n  sourcetypelabel = \"broadsheet\")\n\n\nfrequency_table <-\n  metadata |>\n  select(article_id, corpus, source_type) |> \n  left_join(\n  {\n  # this gets a frequency table per article where\n  # a language type is used\n  both_datasets |>\n  select(dataset, article_id, freq_per_million_words) |>\n  pivot_wider(names_from = dataset, \n      names_glue = \"{dataset}_freq\",\n      values_from = freq_per_million_words,\n      values_fill = 0)}\n  ) |>\n  replace_na(list(cond_freq = 0, \n      pers_freq = 0,\n      obes_freq = 0))\n\n\nWe group articles into tabloids and broadsheets in the following manner:\n\nCodemetadata |>\n  select(source, source_type, corpus) |>\n  distinct() |>\n  kable()\n\n\n\nTable 1: Classification of sources by country (corpus of origin) and into types.\n\nsource\nsource_type\ncorpus\n\n\n\nDaily Express\ntabloid\nUK\n\n\nDaily Mail\ntabloid\nUK\n\n\nDaily Star\ntabloid\nUK\n\n\nDaily Telegraph\nbroadsheet\nUK\n\n\nGuardian\nbroadsheet\nUK\n\n\nIndependent\nbroadsheet\nUK\n\n\nMirror\ntabloid\nUK\n\n\nSun\ntabloid\nUK\n\n\nTimes\nbroadsheet\nUK\n\n\nAdvertiser\ntabloid\nAUS\n\n\nAustralian\nbroadsheet\nAUS\n\n\nNorthernT\ntabloid\nAUS\n\n\nCourierMail\ntabloid\nAUS\n\n\nAge\nbroadsheet\nAUS\n\n\nSydHerald\nbroadsheet\nAUS\n\n\nTelegraph\ntabloid\nAUS\n\n\nWestAus\ntabloid\nAUS\n\n\nCanTimes\nbroadsheet\nAUS\n\n\nHeraldSun\ntabloid\nAUS\n\n\nHobMercury\ntabloid\nAUS\n\n\nBrisTimes\nbroadsheet\nAUS"
  },
  {
    "objectID": "400_analysis/ConditionAndPersonFirst.html#proportion-and-number-of-articles-using-any-of-the-language-types-in-the-two-corpora",
    "href": "400_analysis/ConditionAndPersonFirst.html#proportion-and-number-of-articles-using-any-of-the-language-types-in-the-two-corpora",
    "title": "Condition and person-first language & use of the adjective lemma OBESE",
    "section": "Proportion and number of articles using any of the language types in the two corpora",
    "text": "Proportion and number of articles using any of the language types in the two corpora\nLet’s find the total number of articles that use one or more language types in the two corpora.\n\nCodearticle_id_cond <- get_article_ids(both_datasets, \"cond\")\narticle_id_pers <- get_article_ids(both_datasets, \"pers\")\narticle_id_obes <- get_article_ids(both_datasets, \"obes\")\n\nggvenn(list(\n  `Condition first` = article_id_cond,\n  `Person first` = article_id_pers,\n  `Obese` = article_id_obes),\n  fill_color = c(\"white\", \"white\", \"white\")) +\n  labs(title = \"Number of articles that use the language features\")\n\nfor_stacked_chart <-\n  metadata |>\n  select(article_id, corpus, source_type) |>\n  mutate(grp = case_when(\n  article_id %in% base::intersect(base::intersect(article_id_cond, article_id_pers), article_id_obes) ~ \"all3\",\n  # two\n  article_id %in% base::intersect(article_id_cond, article_id_pers) ~ \"cond_pers\",\n  article_id %in% base::intersect(article_id_obes, article_id_cond) ~ \"cond_obes\",\n  article_id %in% base::intersect(article_id_obes, article_id_pers) ~ \"pers_obes\",\n  # one\n  article_id %in% article_id_cond ~ \"cond\",\n  article_id %in% article_id_pers ~ \"pers\",\n  article_id %in% article_id_obes ~ \"obes\",\n  # none\n  TRUE ~ \"none\"\n  )) \n\nggsave(device = \"png\",\n   here::here(\"400_analysis\",\"venn.png\"),\n   bg = \"white\",\n   width = 10,\n   height = 4)\n\n\n\nFigure 1: Venn diagram of overlap between articles that use condition-first, person-first and adjective lemma OBESE in the two corpora.\n\n\n\n\n\nCodefor_stacked_chart_data <- for_stacked_chart |> \n  mutate(group = case_when(\n  grp == \"cond_obes\" ~ \"two\",\n  grp == \"pers_obes\" ~ \"two\",\n  grp == \"cond_pers\" ~ \"two\",\n  TRUE ~ grp\n  ))  |>\n  mutate(group = factor(group, \n levels = c(\"none\", \"pers\", \"all3\", \"two\", \"cond_obes\", \"pers_obes\",\"cond_pers\", \"obes\",  \"cond\"),\n labels = c(\n \"no occurrences of any of the three targeted language practices\",\n \"occurrences of person-first language\",\n \"occurrences of all three targeted language practices\",\n \"occurrences of two of the three targeted language practices\",\n \"cond_obes\", \"pers_obes\",\"cond_pers\",\n \"occurrences of adjective lemma 'the obese'\",\n \"occurrences of condition-first language\"\n ))) \n\nfor_stacked_chart_data |>\n #    mutate(group = factor(group, \n # labels = c(\n # \"NoOccurence\",\n # \"Person-first\",\n # \"All3\",\n # \"2of3\",\n # \"Obese\",\n # \"Condition-first\"\n # ))) |>\n  group_by(group, source_type, corpus) |>\n  summarise(n_articles = n()) |>\n  ungroup() |>\n  group_by(source_type, corpus) |>\n  mutate(total = sum(n_articles), proportion_articles = round(100 * n_articles/total, 2)) |>\n  select(-total, -n_articles) |>\n  pivot_wider(values_from = proportion_articles, names_from = group, values_fill = 0) |>\n  write_csv(here::here(\"300_data_processed\", \"percentage_table.csv\"))\n\n\n\n\nmfig <-\n  for_stacked_chart_data |>\n  ggplot() + \n  geom_bar(\n  aes(fill=group, \n    x=source_type),\n  position=\"fill\") + \n  facet_grid(~corpus) + theme_bw() + \n  scale_fill_brewer(palette = \"Accent\") +\n scale_y_continuous(name = \"Proportion articles in corpus\",\n        labels = scales::percent) +\n # set axis limits in coord_cartesian\n scale_fill_grey(start = 0.9, end = 0.1) + \n  theme(axis.text.x = element_text(angle = 35, vjust = 1, hjust=1)) +\n  labs(x = \"\",\n   fill = \"\")\n\nmfig\nggsave(device = \"png\",\n   here::here(\"400_analysis\",\"b_w_proportion_to100.png\"),\n   bg = \"white\",\n   width = 10,\n   height = 4)\n\nmfig + coord_cartesian(ylim = c(0, 0.3)) \nggsave(device = \"png\",\n   here::here(\"400_analysis\",\"b_w_proportion.png\"),\n   bg = \"white\",\n   width = 10,\n   height = 4)\n\nmfig + theme(legend.position = \"bottom\")\nggsave(device = \"png\",\n   here::here(\"400_analysis\",\"b_w_proportion_legendbottom_to100.png\"),\n   bg = \"white\",\n   width = 12,\n   height = 4)\n\nmfig + theme(legend.position = \"bottom\") + coord_cartesian(ylim = c(0, 0.3)) \nggsave(device = \"png\",\n   here::here(\"400_analysis\",\"b_w_proportion_legendbottom.png\"),\n   bg = \"white\",\n   width = 12,\n   height = 4)\n\n\n\nFigure 2: Proportion of articles in each corpus that have instances of all three language types, two language types, a single (specified) language type or no language types. We can see that most articles in both corpora have no use of any of the 3 language types described.\n\n\n\n\n\n\nFigure 3: Proportion of articles in each corpus that have instances of all three language types, two language types, a single (specified) language type or no language types. We can see that most articles in both corpora have no use of any of the 3 language types described.\n\n\n\n\n\n\nFigure 4: Proportion of articles in each corpus that have instances of all three language types, two language types, a single (specified) language type or no language types. We can see that most articles in both corpora have no use of any of the 3 language types described.\n\n\n\n\n\n\nFigure 5: Proportion of articles in each corpus that have instances of all three language types, two language types, a single (specified) language type or no language types. We can see that most articles in both corpora have no use of any of the 3 language types described.\n\n\n\n\n\nCodecorpus_articles_grouped <- for_stacked_chart |>\n  group_by(corpus, source_type) |>\n  count() |> \n  rename(total_articles = n)\n\nfor_stacked_chart |> \n  group_by(corpus, source_type) |>\n  count(grp) |>\n  arrange(-n) |>\n  left_join(corpus_articles_grouped) |>\n  mutate(percent = round(100 * n/total_articles, 2)) |>\n  select(-total_articles) |>\n  pivot_wider(names_from = corpus, values_from = c(n, percent), values_fill = 0) |>\n  rename(observed_language = grp) |>\n  kable()\n\n\n\nTable 2: Number and percent of articles in each corpus, grouped by source type, that feature each of the language types.\n\nsource_type\nobserved_language\nn_UK\nn_AUS\npercent_UK\npercent_AUS\n\n\n\nbroadsheet\nnone\n6170\n1631\n81.66\n73.80\n\n\ntabloid\nnone\n6156\n2271\n78.91\n71.69\n\n\ntabloid\ncond\n1395\n815\n17.88\n25.73\n\n\nbroadsheet\ncond\n1145\n490\n15.15\n22.17\n\n\nbroadsheet\ncond_obes\n112\n33\n1.48\n1.49\n\n\ntabloid\ncond_obes\n112\n37\n1.44\n1.17\n\n\ntabloid\nobes\n109\n32\n1.40\n1.01\n\n\nbroadsheet\nobes\n91\n28\n1.20\n1.27\n\n\nbroadsheet\ncond_pers\n18\n17\n0.24\n0.77\n\n\ntabloid\npers\n16\n7\n0.21\n0.22\n\n\nbroadsheet\npers\n15\n10\n0.20\n0.45\n\n\ntabloid\ncond_pers\n11\n6\n0.14\n0.19\n\n\nbroadsheet\nall3\n4\n1\n0.05\n0.05\n\n\nbroadsheet\npers_obes\n1\n0\n0.01\n0.00\n\n\ntabloid\nall3\n1\n0\n0.01\n0.00\n\n\ntabloid\npers_obes\n1\n0\n0.01\n0.00\n\n\n\n\n\n\nWe can also provide a less detailed version of the above summary, where each article could be counted once to each category it belonged to (so, for example, one with all 3 could be counted 3 times: towards condition-first, person-first and OBESE). Let’s generate this summary:\n\nCodefortable11 <-\n  both_datasets |>\n  select(article_id, corpus, source_type, dataset) |>\n  group_by(corpus, source_type, dataset) |>\n  count() |>\n  rename(article_count = n) |>\n  left_join({\n  metadata |>\n  select(article_id, corpus, source_type) |>\n  group_by(corpus, source_type) |>\n  count() |>\n  rename(corpus_count = n)}) |>\n  mutate(subset = paste(corpus, source_type, sep = \"_\")) |>\n  ungroup() |>\n  select(-corpus, -source_type) |>\n  mutate(dataset = factor(dataset, levels = c(\"pers\", \"cond\", \"obes\")),\n   subset = factor(subset, levels = c(\"AUS_tabloid\",\n                \"UK_tabloid\",\n                \"AUS_broadsheet\",\n                \"UK_broadsheet\"))) |> arrange(dataset, subset)\n\nfortable11 |>\n  select(-corpus_count) |>\n  pivot_wider(values_from = article_count, names_from = subset) |>\n  kable()\n\n\n\nTable 3: Number of articles that have each of the language types. Each article may be counted multiple times, once for each instance of language use.\n\ndataset\nAUS_tabloid\nUK_tabloid\nAUS_broadsheet\nUK_broadsheet\n\n\n\npers\n13\n29\n28\n38\n\n\ncond\n858\n1519\n541\n1279\n\n\nobes\n69\n223\n62\n208\n\n\n\n\n\n\nAnd get the percentages table:\n\nCodefortable11 |>\nmutate(prop = round(100*article_count/corpus_count, 2)) |>\n  select(-article_count, -corpus_count) |>\n  pivot_wider(values_from = prop, names_from = subset) |>\n  kable()\n\n\n\nTable 4: Percentage of articles (relative to total number of articles in each subcorpus) that feature the language types. Note that each article may be counted multiple times, for example, twice, if it includes use of both condition-first language and the adjective lemma OBESE.\n\ndataset\nAUS_tabloid\nUK_tabloid\nAUS_broadsheet\nUK_broadsheet\n\n\n\npers\n0.41\n0.37\n1.27\n0.50\n\n\ncond\n27.08\n19.47\n24.48\n16.93\n\n\nobes\n2.18\n2.86\n2.81\n2.75"
  },
  {
    "objectID": "400_analysis/ConditionAndPersonFirst.html#article-length-differences-in-australian-and-uk-broadsheets-and-tabloids",
    "href": "400_analysis/ConditionAndPersonFirst.html#article-length-differences-in-australian-and-uk-broadsheets-and-tabloids",
    "title": "Condition and person-first language & use of the adjective lemma OBESE",
    "section": "Article length differences in Australian and UK broadsheets and tabloids",
    "text": "Article length differences in Australian and UK broadsheets and tabloids\nAustralian broadsheets vs tabloids\nAre texts in the Australian sub-corpus used for this study also longer than in broadsheets than in tabloids, like was reported for the larger study?\n\nCodehistogram_pairwise(au_broadsheet_wordcounts,\n       au_tabloid_wordcounts,\n       \"broadsheet\",\n       \"tabloid\")\n\n\n\nFigure 6: Histogram of word count (x) and number (y) of articles from tabloids and broadsheets in the Australian corpus. Broadsheets appear to have higher word counts than tabloids in the dataset.\n\n\n\n\nLet’s use a t-test to consider?\n\nCodereport::report(t.test(au_broadsheet_wordcounts,\n        au_tabloid_wordcounts))\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between au_broadsheet_wordcounts and au_tabloid_wordcounts (mean of x = 841.70, mean of y = 555.04) suggests that the effect is positive, statistically significant, and medium (difference = 286.66, 95% CI [257.15, 316.17], t(4261.47) = 19.04, p < .001; Cohen’s d = 0.54, 95% CI [0.48, 0.59])\n\nThe non-parametric FP test supports this as well:\n\nCodefp_test_wc(wc1 = au_broadsheet_wordcounts,\n     wc2 = au_tabloid_wordcounts,\n     label1 = \"broadsheet\",\n     label2 = \"tabloid\")\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  wc by label (broadsheet, tabloid)\nZ = 18.92, p-value < 1e-04\nalternative hypothesis: true mu is not equal to 0\n\n\nConclusion: Articles in Australian broadsheets are longer than articles in Australian tabloids.\nUK broadsheets vs tabloids\nAre texts in tabloids also shorter than in broadsheets in the UK, like they are in Australia?\n\nCodehistogram_pairwise(wc1 = uk_broadsheet_wordcounts,\n       wc2 = uk_tabloid_wordcounts,\n       label1 = \"broadsheets\",\n       label2 = \"tabloids\")\n\n\n\nFigure 7: Histogram of word count (x) and number (y) of articles from tabloids and broadsheets in the UK corpus. Similar to the Australian corpus, broadsheets appear to have higher word counts than tabloids in the dataset.\n\n\n\n\nBased on the visualisation in Figure Figure 7, this appears to be the case. Let’s use a parametric test to assess.\n\nCodereport::report(t.test(\n  uk_broadsheet_wordcounts, \n  uk_tabloid_wordcounts))\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between uk_broadsheet_wordcounts and uk_tabloid_wordcounts (mean of x = 718.31, mean of y = 561.20) suggests that the effect is positive, statistically significant, and small (difference = 157.11, 95% CI [139.76, 174.46], t(14856.49) = 17.75, p < .001; Cohen’s d = 0.29, 95% CI [0.26, 0.32])\n\nYes, articles in broadsheets are indeed longer, although the effect size is not as large (small vs medium for the Australian corpus).\nDoes the non-parametric FP test support this as well?\n\nCodefp_test_wc(wc1 = uk_broadsheet_wordcounts,\n     wc2 = uk_tabloid_wordcounts,\n     label1 = \"broadsheet\",\n     label2 = \"tabloid\")\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  wc by label (broadsheet, tabloid)\nZ = 17.615, p-value < 1e-04\nalternative hypothesis: true mu is not equal to 0\n\n\nYes, it does.\nConclusion: Articles in UK broadsheets are longer than articles in UK tabloids.\nAustralian vs UK broadsheets\nAre texts in broadsheets from Australia and the UK different in length?\nThe visualisation in Figure Figure 8 suggests this is the case.\n\nCodehistogram_pairwise(au_broadsheet_wordcounts,\n       uk_broadsheet_wordcounts,\n       \"Aus\",\n       \"UK\")\n\n\n\nFigure 8: Histogram of word count (x) and number (y) of articles from broadsheets in the Australian and UK corpus. Articles in UK broadsheets are somewhat shorter than in Australian ones.\n\n\n\n\nLet’s use a t-test to consider?\n\nCodereport::report(t.test(au_broadsheet_wordcounts,\n        uk_broadsheet_wordcounts))\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between au_broadsheet_wordcounts and uk_broadsheet_wordcounts (mean of x = 841.70, mean of y = 718.31) suggests that the effect is positive, statistically significant, and small (difference = 123.39, 95% CI [95.96, 150.83], t(3660.02) = 8.82, p < .001; Cohen’s d = 0.21, 95% CI [0.16, 0.26])\n\nYes, there appears to be a small difference in text length between Australian and UK broadsheets, with articles in UK broadsheets being somewhat shorter.\nDoes the non-parametric FP test support this as well?\n\nCodefp_test_wc(wc1 = au_broadsheet_wordcounts,\n     wc2 = uk_broadsheet_wordcounts,\n     label1 = \"AUS\",\n     label2 = \"UK\")\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  wc by label (AUS, UK)\nZ = 8.6879, p-value < 1e-04\nalternative hypothesis: true mu is not equal to 0\n\n\nYes, it does.\nConclusion: Articles in Australian broadsheets are longer than articles in UK broadsheets.\nAustralian vs UK tabloids\nAre texts in tabloids from Australia and the UK different in length?\nThe visualisation in Figure Figure 9 suggests that there is no difference between the two.\n\nCodehistogram_pairwise(au_tabloid_wordcounts,\n       uk_tabloid_wordcounts,\n       \"Aus\",\n       \"UK\")\n\n\n\nFigure 9: Histogram of word count (x) and number (y) of articles from tabloids in the Australian and UK corpus. There doesn’t seem to be a difference in length between articles in Australian and UK tabloids.\n\n\n\n\nLet’s use a t-test to consider?\n\nCodereport::report(t.test(au_tabloid_wordcounts,\n        uk_tabloid_wordcounts))\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between au_tabloid_wordcounts and uk_tabloid_wordcounts (mean of x = 555.04, mean of y = 561.20) suggests that the effect is negative, statistically not significant, and very small (difference = -6.16, 95% CI [-26.64, 14.31], t(6001.53) = -0.59, p = 0.555; Cohen’s d = -0.01, 95% CI [-0.05, 0.03])\n\nNo, there appears to be no difference in length between articles in Australian and UK tabloids.\nDoes the non-parametric FP test support this as well?\n\nCodefp_test_wc(wc1 = au_tabloid_wordcounts,\n     wc2 = uk_tabloid_wordcounts,\n     label1 = \"AUS\",\n     label2 = \"UK\")\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  wc by label (AUS, UK)\nZ = -0.58397, p-value = 0.5529\nalternative hypothesis: true mu is not equal to 0\n\n\nConclusion: Articles in Australian and UK tabloids do not differ in length."
  },
  {
    "objectID": "400_analysis/ConditionAndPersonFirst.html#person-first-language",
    "href": "400_analysis/ConditionAndPersonFirst.html#person-first-language",
    "title": "Condition and person-first language & use of the adjective lemma OBESE",
    "section": "Person-first language",
    "text": "Person-first language\nAustralian corpus\nLet’s generate a frequency table (sum of no hits in text) for person-first language:\n\nCodepers_4chisq_matrix_sumhits <-\n  both_datasets |> \n  filter(dataset == \"pers\") |>\n  generate_conting_tab_sumhits() |>\n  as.matrix()\n\npers_4chisq_matrix_noarticles <-\n  both_datasets |> \n  filter(dataset == \"pers\") |>\n  generate_conting_tab_noarticles() |>\n  as.matrix()\n\npers_4chisq_matrix_sumhits |>\n  kable()\n\n\n\nTable 5: Number of articles that feature person-first language in UK and Australian tabloids and broadsheets.\n\n\nAUS\nUK\n\n\n\nbroadsheet\n30\n41\n\n\ntabloid\n15\n30\n\n\n\n\n\n\nWe also need to find the probability of observing these values given the total word count in each of the cells (for example, the number of total words in the Australian broadsheet corpus). Let’s calculate these values:\n\nCodegenerate_wc(metadata) |>\n  kable()\n\n\n\nTable 6: Total word count of articles in UK and Australian tabloids and broadsheets.\n\n\nAUS\nUK\n\n\n\nbroadsheet\n1860166\n5427551\n\n\ntabloid\n1758364\n4377933\n\n\n\n\n\n\nIs there a difference in the frequency of use of person-first language in tabloids and broadsheets in the Australian corpus?\n\nCodeget_chisq_tabloid_broadsheet(pers_4chisq_matrix_sumhits, \n           col_no = 1, \n           type = \"wc\") |>\n  kable()\n\n\n\nTable 7: Results of Chi-square test between the number of articles that use person-first language in the Australian corpus vs the number that would be expected based on total word count of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n30\n\n\nbroadsheet_expected\n23.133\n\n\ntabloid_observed\n15\n\n\ntabloid_expected\n21.867\n\n\nstatistic\n4.194932\n\n\np.value\n0.04054498\n\n\nparameter\n1\n\n\neffect_size\n0.3053207\n\n\n\n\n\n\nBased on a Chi-square test, there is a slight difference between the expected and observed frequency of instances of person-first language in the Australian corpus (p < 0.05), with broadsheets having somewhat more instances of person-first language than tabloids in the Australian corpus.\nThis also holds true when we consider the number of articles that have at least one instance of person-first language:\n\nCodeget_chisq_tabloid_broadsheet(pers_4chisq_matrix_noarticles, \n           col_no = 1, \n           type = \"art\") |>\n  kable()\n\n\n\nTable 8: Results of Chi-square test between the number of articles that use person-first language in the Australian corpus vs the number that would be expected based on the total number of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n28\n\n\nbroadsheet_expected\n16.84827\n\n\ntabloid_observed\n13\n\n\ntabloid_expected\n24.15173\n\n\nstatistic\n12.53039\n\n\np.value\n0.0004003854\n\n\nparameter\n1\n\n\neffect_size\n0.5528285\n\n\n\n\n\n\nIf we consider a t-test on the frequency (per million words) of the Australian corpus (including all texts in each sub-corpus, including those without instances), a difference is not detected.\n\nCodettest_broad_vs_tabl(frequency_table, \"AUS\", \"pers_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between broadsheet and tabloid (mean of x = 17.81, mean of y = 11.68) suggests that the effect is positive, statistically not significant, and very small (difference = 6.13, 95% CI [-4.24, 16.51], t(5183.11) = 1.16, p = 0.247; Cohen’s d = 0.03, 95% CI [-0.02, 0.09])\n\nIf we instead use a non-parametric test, there is also no difference detected.\n\nCodefp_test_broad_vs_tabl(freq_table = frequency_table,\n        corpus_label = \"AUS\",\n        dataset = \"pers\",\n        myformula = formula(pers_freq ~ source_type),\n        dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  pers_freq by source_type (broadsheet, tabloid)\nZ = 1.1248, p-value = 0.265\nalternative hypothesis: true mu is not equal to 0\n\n\nUK corpus\nWhat about the total number of instances of person-first language in the UK corpus?\n\nCodeget_chisq_tabloid_broadsheet(pers_4chisq_matrix_sumhits, \n           col_no = 2,\n           type = \"wc\") |>\n  kable()\n\n\n\nTable 9: Results of Chi-square test between the number of articles that use person-first language in the UK corpus vs the number that would be expected based on total word count of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n41\n\n\nbroadsheet_expected\n39.30006\n\n\ntabloid_observed\n30\n\n\ntabloid_expected\n31.69994\n\n\nstatistic\n0.1646923\n\n\np.value\n0.6848727\n\n\nparameter\n1\n\n\neffect_size\n0.04816233\n\n\n\n\n\n\nNo difference is observed between the number of observed and expected instances of use of person-first language in the UK corpus between tabloids and broadsheets.\n\nCodeget_chisq_tabloid_broadsheet(pers_4chisq_matrix_noarticles, \n           col_no = 2, \n           type = \"art\") |>\n  kable()\n\n\n\nTable 10: Results of Chi-square test between the number of articles that use person-first language in the UK corpus vs the number that would be expected based on the total number of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n38\n\n\nbroadsheet_expected\n32.96555\n\n\ntabloid_observed\n29\n\n\ntabloid_expected\n34.03445\n\n\nstatistic\n1.513559\n\n\np.value\n0.2185969\n\n\nparameter\n1\n\n\neffect_size\n0.1503011\n\n\n\n\n\n\nThis lack of a difference also holds true when we consider the number of articles that have at least one instance of person-first language.\nIf we consider a t-test on the frequency (per million words) of the UK corpus (including all texts in each sub-corpus, including those without instances), a difference is not detected.\n\nCodettest_broad_vs_tabl(frequency_table, \"UK\", \"pers_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between broadsheet and tabloid (mean of x = 8.52, mean of y = 12.12) suggests that the effect is negative, statistically not significant, and very small (difference = -3.61, 95% CI [-9.81, 2.60], t(13624.58) = -1.14, p = 0.255; Cohen’s d = -0.02, 95% CI [-0.05, 0.01])\n\nIf we use a non-parametric test, there is also no difference detected.\n\nCodefp_test_broad_vs_tabl(freq_table = frequency_table,\n        corpus_label = \"UK\",\n        dataset = \"pers\",\n        myformula = formula(pers_freq ~ source_type),\n        dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  pers_freq by source_type (broadsheet, tabloid)\nZ = -1.1324, p-value = 0.2569\nalternative hypothesis: true mu is not equal to 0\n\n\nAustralian vs UK broadsheets\nLet’s compare Australian and UK broadsheets.\nIs there a difference in the frequency of use of person-first language in broadsheets in the Australian vs UK corpus?\n\nCode# broadsheets\nget_chisq_aus_uk(pers_4chisq_matrix_sumhits, \n       row_no = 1,\n       type = \"wc\")|>\n  kable()\n\n\n\nTable 11: Results of Chi-square test between the number of articles that use person-first language in broadsheets in UK and Australia vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\nbroadsheet\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n30\n\n\nAUS_expected\n18.12252\n\n\nUK_observed\n41\n\n\nUK_expected\n52.87748\n\n\nstatistic\n10.45244\n\n\np.value\n0.001224874\n\n\nparameter\n1\n\n\neffect_size\n0.3836893\n\n\n\n\n\n\nThere is a difference between the expected and observed frequency of instances of person-first language in Australian vs UK broadsheets(p < 0.005).\nWe can see that person-first language is used more frequently than we would expect in Australian broadsheets, and less frequently in UK ones.\n\nCode# broadsheets\nget_chisq_aus_uk(pers_4chisq_matrix_noarticles, \n       row_no = 1,\n       type = \"art\")|>\n  kable()\n\n\n\nTable 12: Results of Chi-square test between the number of articles that use person-first language in broadsheets in UK and Australia vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\nbroadsheet\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n28\n\n\nAUS_expected\n14.93549\n\n\nUK_observed\n38\n\n\nUK_expected\n51.06451\n\n\nstatistic\n14.77037\n\n\np.value\n0.0001214283\n\n\nparameter\n1\n\n\neffect_size\n0.4730682\n\n\n\n\n\n\nThis also holds true when we consider the number of articles that have at least one instance of person-first language.\nIf we consider the frequency (per million words) of the Australian vs UK broadsheets (including all texts in each sub-corpus, including those without instances), a very small difference is detected, with Australian broadsheets having a higher frequency than UK ones:\n\nCodettest_aus_vs_uk(frequency_table, \"broadsheet\", \"pers_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between aus and uk (mean of x = 17.81, mean of y = 8.52) suggests that the effect is positive, statistically significant, and very small (difference = 9.30, 95% CI [1.14, 17.46], t(3250.78) = 2.23, p = 0.026; Cohen’s d = 0.08, 95% CI [9.57e-03, 0.15])\n\nIs this supported by non-parametric analysis\n\nCodefp_test_aus_vs_uk(freq_table = frequency_table,\n      source_label = \"broadsheet\",\n      dataset = \"pers\",\n      myformula = formula(pers_freq ~ corpus),\n      dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  pers_freq by corpus (AUS, UK)\nZ = 2.408, p-value = 0.017\nalternative hypothesis: true mu is not equal to 0\n\n\nYes, this difference is supported by the non-parametric FP test, with p(FP) < 0.05.\nAustralian vs UK tabloids\nLet’s compare Australian and UK tabloids.\nIs there a difference in the frequency of use of person-first language in tabloids in the Australian vs UK corpus?\n\nCode# tabloids\nget_chisq_aus_uk(pers_4chisq_matrix_sumhits, \n       row_no = 2,\n       type = \"wc\")|>\n  kable()\n\n\n\nTable 13: Results of Chi-square test between the number of articles that use person-first language in tabloids in UK and Australia vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\ntabloid\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n15\n\n\nAUS_expected\n12.89481\n\n\nUK_observed\n30\n\n\nUK_expected\n32.10519\n\n\nstatistic\n0.4817317\n\n\np.value\n0.487639\n\n\nparameter\n1\n\n\neffect_size\n0.1034657\n\n\n\n\n\n\nThe observed and expected number of instances of person-first language is approximately the same in Australian and British tabloids.\nThis also holds true when we consider the number of articles that have at least one instance of person-first language:\n\nCode# tabloids\nget_chisq_aus_uk(pers_4chisq_matrix_noarticles, \n       row_no = 2,\n       type = \"art\")|>\n  kable()\n\n\n\nTable 14: Results of Chi-square test between the number of articles that use person-first language in tabloids in UK and Australia vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\ntabloid\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n13\n\n\nAUS_expected\n12.13019\n\n\nUK_observed\n29\n\n\nUK_expected\n29.86981\n\n\nstatistic\n0.0877007\n\n\np.value\n0.7671208\n\n\nparameter\n1\n\n\neffect_size\n0.04569586\n\n\n\n\n\n\nIf we consider the frequency (per million words) in the Australian vs UK tabloids (including all texts in each sub-corpus, including those without instances), no significant differences are observed as well.\n\nCodettest_aus_vs_uk(frequency_table, \"tabloid\", \"pers_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between aus and uk (mean of x = 11.68, mean of y = 12.12) suggests that the effect is negative, statistically not significant, and very small (difference = -0.45, 95% CI [-9.37, 8.48], t(6445.87) = -0.10, p = 0.922; Cohen’s d = -2.44e-03, 95% CI [-0.05, 0.05])\n\nThe non-parametric test leads to a similar conclusion:\n\nCodefp_test_aus_vs_uk(frequency_table,\n      \"tabloid\",\n      \"pers\",\n      formula(pers_freq ~ corpus),\n      dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  pers_freq by corpus (AUS, UK)\nZ = -0.093724, p-value = 0.9238\nalternative hypothesis: true mu is not equal to 0\n\n\nIn summary, we do not observe a difference in the use of person-first language in Australian vs UK tabloids."
  },
  {
    "objectID": "400_analysis/ConditionAndPersonFirst.html#condition-first-language",
    "href": "400_analysis/ConditionAndPersonFirst.html#condition-first-language",
    "title": "Condition and person-first language & use of the adjective lemma OBESE",
    "section": "Condition-first language",
    "text": "Condition-first language\nLet’s generate a frequency table (sum of no hits in text) for condition-first language:\n\nCodecond_4chisq_matrix_sumhits <-\n  both_datasets |> \n  filter(dataset == \"cond\") |>\n  generate_conting_tab_sumhits() |>\n  as.matrix()\n\ncond_4chisq_matrix_noarticles <-\n  both_datasets |> \n  filter(dataset == \"cond\") |>\n  generate_conting_tab_noarticles() |>\n  as.matrix()\n\ncond_4chisq_matrix_sumhits |>\n  kable()\n\n\n\nTable 15: Number of articles that feature condition-first language in UK and Australian tabloids and broadsheets.\n\n\nAUS\nUK\n\n\n\nbroadsheet\n1143\n2320\n\n\ntabloid\n1428\n2399\n\n\n\n\n\n\nAustralian corpus\nWe can assess the difference between these total number of uses of condition-first language in the Australian corpus, using a Chi-square test:\n\nCodeget_chisq_tabloid_broadsheet(cond_4chisq_matrix_sumhits, \n           col_no = 1,\n           type = \"wc\") |>\n  kable()\n\n\n\nTable 16: Results of Chi-square test between the number of articles that use condition-first language in the Australian corpus vs the number that would be expected based on total word count of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n1143\n\n\nbroadsheet_expected\n1321.666\n\n\ntabloid_observed\n1428\n\n\ntabloid_expected\n1249.334\n\n\nstatistic\n49.70315\n\n\np.value\n1.788587e-12\n\n\nparameter\n1\n\n\neffect_size\n0.1390404\n\n\n\n\n\n\nThere is a higher number of instances of condition-first language used in tabloids, and a lower number of instances in broadsheets in the Australian corpus.\n\nCodeget_chisq_tabloid_broadsheet(cond_4chisq_matrix_noarticles, \n           col_no = 1,\n           type = \"art\")|>\n  kable()\n\n\n\nTable 17: Results of Chi-square test between the number of articles that use condition-first language in the Australian corpus vs the number that would be expected based on the total number of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n541\n\n\nbroadsheet_expected\n574.8959\n\n\ntabloid_observed\n858\n\n\ntabloid_expected\n824.1041\n\n\nstatistic\n3.392658\n\n\np.value\n0.06548731\n\n\nparameter\n1\n\n\neffect_size\n0.04924489\n\n\n\n\n\n\nThis also somewhat holds true, although is not highly significant, when we consider the number of articles that have at least one instance of condition-first language.\nIf we consider a t-test on the frequency (per million words) of the Australian corpus (including all texts in each sub-corpus, including those without instances), a very small difference is detected, with tabloids having a higher frequency of use of condition-first language vs broadsheets.\n\nCodettest_broad_vs_tabl(frequency_table, \"AUS\", \"cond_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between broadsheet and tabloid (mean of x = 847.96, mean of y = 1315.95) suggests that the effect is negative, statistically significant, and very small (difference = -467.99, 95% CI [-614.83, -321.16], t(5364.35) = -6.25, p < .001; Cohen’s d = -0.17, 95% CI [-0.22, -0.12])\n\n\nCodefp_test_broad_vs_tabl(frequency_table,\n        corpus_label = \"AUS\",\n        dataset = \"cond\", \n        formula(cond_freq ~ source_type), \n        dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  cond_freq by source_type (broadsheet, tabloid)\nZ = -5.8137, p-value < 1e-04\nalternative hypothesis: true mu is not equal to 0\n\n\nUK corpus\nWe can assess the difference between these total number of uses of condition-first language in the UK corpus, using a Chi-square test. There is a higher number of instances of condition-first language used in tabloids, and a lower number of instances in broadsheets, in the UK corpus:\n\nCodeget_chisq_tabloid_broadsheet(cond_4chisq_matrix_sumhits, \n           col_no = 2,\n           type = \"wc\") |>\n  kable()\n\n\n\nTable 18: Results of Chi-square test between the number of articles that use condition-first language in the UK corpus vs the number that would be expected based on total word count of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n2320\n\n\nbroadsheet_expected\n2612.07\n\n\ntabloid_observed\n2399\n\n\ntabloid_expected\n2106.93\n\n\nstatistic\n73.14586\n\n\np.value\n1.204139e-17\n\n\nparameter\n1\n\n\neffect_size\n0.1245002\n\n\n\n\n\n\nThis also holds true when we consider the number of articles that have at least one instance of condition-first language.\n\nCodeget_chisq_tabloid_broadsheet(cond_4chisq_matrix_noarticles, \n           col_no = 2,\n           type = \"art\") |>\n  kable()\n\n\n\nTable 19: Results of Chi-square test between the number of articles that use condition-first language in the UK corpus vs the number that would be expected based on the total number of articles from tabloids and broadsheets in that subcorpus.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n1279\n\n\nbroadsheet_expected\n1376.681\n\n\ntabloid_observed\n1519\n\n\ntabloid_expected\n1421.319\n\n\nstatistic\n13.644\n\n\np.value\n0.0002209459\n\n\nparameter\n1\n\n\neffect_size\n0.0698308\n\n\n\n\n\n\nIf we consider a t-test on the frequency (per million words) of the UK corpus (including all texts in each sub-corpus, including those without instances), a very small difference is detected, with a higher mean frequency in tabloids than broadsheets.\n\nCodettest_broad_vs_tabl(frequency_table, \"UK\", \"cond_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between broadsheet and tabloid (mean of x = 629.40, mean of y = 835.37) suggests that the effect is negative, statistically significant, and very small (difference = -205.97, 95% CI [-277.76, -134.18], t(15067.94) = -5.62, p < .001; Cohen’s d = -0.09, 95% CI [-0.12, -0.06])\n\nThis is also supported by the FP test:\n\nCodefp_test_broad_vs_tabl(frequency_table,\n        corpus_label = \"UK\",\n        dataset = \"cond\")\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  cond_freq by source_type (broadsheet, tabloid)\nZ = -5.603, p-value < 1e-04\nalternative hypothesis: true mu is not equal to 0\n\n\nAustralian vs UK broadsheets\nThere is a higher number of uses of condition-first language in Australian broadsheets, and a lower number in UK broadsheets:\n\nCodeget_chisq_aus_uk(cond_4chisq_matrix_sumhits, \n       row_no = 1,\n       type = \"wc\") |>\n  kable()\n\n\n\nTable 20: Results of Chi-square test between the number of articles that use condition-first language in broadsheets in UK and Australia vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\nbroadsheet\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n1143\n\n\nAUS_expected\n883.9195\n\n\nUK_observed\n2320\n\n\nUK_expected\n2579.081\n\n\nstatistic\n101.9634\n\n\np.value\n5.655592e-24\n\n\nparameter\n1\n\n\neffect_size\n0.1715916\n\n\n\n\n\n\nThis also holds true when we consider the number of articles that have at least one instance of condition-first language:\n\nCodeget_chisq_aus_uk(cond_4chisq_matrix_noarticles, \n           row_no = 1,\n           type = \"art\") |>\n  kable()\n\n\n\nTable 21: Results of Chi-square test between the number of articles that use condition-first language in broadsheets in UK and Australia vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\nbroadsheet\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n541\n\n\nAUS_expected\n411.8575\n\n\nUK_observed\n1279\n\n\nUK_expected\n1408.143\n\n\nstatistic\n52.33792\n\n\np.value\n4.672644e-13\n\n\nparameter\n1\n\n\neffect_size\n0.1695792\n\n\n\n\n\n\nIf we consider the frequency (per million words) of the Australian vs UK broadsheets (including all texts in each sub-corpus, including those without instances), a very small difference is detected, with Australian broadsheets having a higher frequency than UK ones:\n\nCodettest_aus_vs_uk(frequency_table, \"broadsheet\", \"cond_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between aus and uk (mean of x = 847.96, mean of y = 629.40) suggests that the effect is positive, statistically significant, and very small (difference = 218.56, 95% CI [115.85, 321.27], t(3442.55) = 4.17, p < .001; Cohen’s d = 0.14, 95% CI [0.08, 0.21])\n\nThis is also supported by the FP test:\n\nCodefp_test_aus_vs_uk(freq_table = frequency_table, \n  source_label = \"broadsheet\",\n  dataset = \"cond\",\n  myformula = formula(cond_freq ~ corpus),\n  dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  cond_freq by corpus (AUS, UK)\nZ = 4.3017, p-value = 1e-04\nalternative hypothesis: true mu is not equal to 0\n\n\nAustralian vs UK tabloids\nThere is a higher number of uses of condition-first language in Australian tabloids, and a lower number in UK tabloids:\n\nCodeget_chisq_aus_uk(cond_4chisq_matrix_sumhits, \n       row_no = 2,\n       type = \"wc\") |>\n  kable()\n\n\n\nTable 22: Results of Chi-square test between the number of articles that use condition-first language in tabloids in UK and Australia vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\ntabloid\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n1428\n\n\nAUS_expected\n1096.632\n\n\nUK_observed\n2399\n\n\nUK_expected\n2730.368\n\n\nstatistic\n140.3453\n\n\np.value\n2.237234e-32\n\n\nparameter\n1\n\n\neffect_size\n0.1915004\n\n\n\n\n\n\nThis also holds true when we consider the number of articles that have at least one instance of condition-first language:\n\nCodeget_chisq_aus_uk(cond_4chisq_matrix_noarticles, \n           row_no  = 2,\n           type = \"art\") |>\n  kable()\n\n\n\nTable 23: Results of Chi-square test between the number of articles that use condition-first language in tabloids in UK and Australia vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\ntabloid\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n858\n\n\nAUS_expected\n686.5107\n\n\nUK_observed\n1519\n\n\nUK_expected\n1690.489\n\n\nstatistic\n60.23424\n\n\np.value\n8.421443e-15\n\n\nparameter\n1\n\n\neffect_size\n0.1591868\n\n\n\n\n\n\nIf we consider the frequency (per million words) in the Australian vs UK tabloids (including all texts in each sub-corpus, including those without instances), the frequency of use of condition-first language is higher in Australian tabloids, and lower in UK ones.\n\nCodettest_aus_vs_uk(frequency_table, \"tabloid\", \"cond_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between aus and uk (mean of x = 1315.95, mean of y = 835.37) suggests that the effect is positive, statistically significant, and small (difference = 480.58, 95% CI [353.41, 607.75], t(4659.62) = 7.41, p < .001; Cohen’s d = 0.22, 95% CI [0.16, 0.27])\n\nThis is also supported by the FP test:\n\nCodefp_test_aus_vs_uk(freq_table = frequency_table, \n  source_label = \"tabloid\",\n  dataset = \"cond\",\n  myformula = formula(cond_freq ~ corpus),\n  dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  cond_freq by corpus (AUS, UK)\nZ = 8.3394, p-value < 1e-04\nalternative hypothesis: true mu is not equal to 0\n\n\nAs shown above, there is no significant difference in articles lengths between Australian and UK tabloids."
  },
  {
    "objectID": "400_analysis/ConditionAndPersonFirst.html#use-of-the-adjective-lemma-obese-1",
    "href": "400_analysis/ConditionAndPersonFirst.html#use-of-the-adjective-lemma-obese-1",
    "title": "Condition and person-first language & use of the adjective lemma OBESE",
    "section": "Use of the adjective lemma OBESE",
    "text": "Use of the adjective lemma OBESE\nLet’s generate a frequency table (sum of no hits in text) for use of the adjective lemma OBESE:\n\nCodeobes_4chisq_matrix_sumhits <-\n  both_datasets |> \n  filter(dataset == \"obes\") |>\n  generate_conting_tab_sumhits() |>\n  as.matrix()\n\nobes_4chisq_matrix_noarticles <-\n  both_datasets |> \n  filter(dataset == \"obes\") |>\n  generate_conting_tab_noarticles() |>\n  as.matrix()\n\nobes_4chisq_matrix_sumhits |>\n  kable()\n\n\n\nTable 24: Number of articles that feature use of the adjective lemma OBESE in UK and Australian tabloids and broadsheets.\n\n\nAUS\nUK\n\n\n\nbroadsheet\n72\n263\n\n\ntabloid\n78\n264\n\n\n\n\n\n\nAustralian corpus\nWe can assess the difference between these total number of uses of the adjective lemma OBESE language in the Australian corpus, using a Chi-square test:\n\nCodeget_chisq_tabloid_broadsheet(obes_4chisq_matrix_sumhits, \n           col_no = 1,\n           type = \"wc\") |>\n  kable()\n\n\n\nTable 25: Results of Chi-square test between the number of articles that use the adjective lemma OBESE in Australian broadsheets vs tabloids vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n72\n\n\nbroadsheet_expected\n77.11001\n\n\ntabloid_observed\n78\n\n\ntabloid_expected\n72.88999\n\n\nstatistic\n0.6968781\n\n\np.value\n0.4038347\n\n\nparameter\n1\n\n\neffect_size\n0.0681605\n\n\n\n\n\n\nThere is no difference in the total count of use of the adjective lemma OBESE in Australian broadsheets vs tabloids.\n\nCodeget_chisq_tabloid_broadsheet(obes_4chisq_matrix_noarticles, \n           col_no = 1,\n           type = \"art\")|>\n  kable()\n\n\n\nTable 26: Results of Chi-square test between the number of articles that use the adjective lemma OBESE in Australian broadsheets vs tabloids vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n62\n\n\nbroadsheet_expected\n53.83228\n\n\ntabloid_observed\n69\n\n\ntabloid_expected\n77.16772\n\n\nstatistic\n2.103752\n\n\np.value\n0.1469382\n\n\nparameter\n1\n\n\neffect_size\n0.1267248\n\n\n\n\n\n\nThere is no difference in the number of articles that use the adjective lemma OBESE in Australian broadsheets vs tabloids.\nIf we consider a t-test on the frequency (per million words) of the Australian corpus (including all texts in each sub-corpus, including those without instances), a very small difference is detected (p < 0.05), with tabloids having a higher frequency of use of condition-first language vs broadsheets.\n\nCodettest_broad_vs_tabl(frequency_table, \"AUS\", \"obes_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between broadsheet and tabloid (mean of x = 42.65, mean of y = 70.46) suggests that the effect is negative, statistically significant, and very small (difference = -27.81, 95% CI [-53.34, -2.29], t(4843.32) = -2.14, p = 0.033; Cohen’s d = -0.06, 95% CI [-0.12, -5.04e-03])\n\nThis could be attributed in part to the longer article length described for tabloids than for broadsheets above.\nThis is not strongly supported (p < 0.1) via a non-parametric FP test:\n\nCodefp_test_broad_vs_tabl(frequency_table,\n        corpus_label = \"AUS\",\n        dataset = \"obes\", \n        formula(obes_freq ~ source_type), \n        dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  obes_freq by source_type (broadsheet, tabloid)\nZ = -1.9091, p-value = 0.0521\nalternative hypothesis: true mu is not equal to 0\n\n\nUK corpus\nWe can assess the difference between these total number of uses of the adjective lemma OBESE in the UK corpus between tabloids and broadsheets. Similar to the Australian corpus, no difference is detected.\n\nCodeget_chisq_tabloid_broadsheet(pers_4chisq_matrix_sumhits, \n           col_no = 2,\n           type = \"wc\") |>\n  kable()\n\n\n\nTable 27: Results of Chi-square test between the number of articles that use the adjective lemma OBESE in UK broadsheets vs tabloids vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n41\n\n\nbroadsheet_expected\n39.30006\n\n\ntabloid_observed\n30\n\n\ntabloid_expected\n31.69994\n\n\nstatistic\n0.1646923\n\n\np.value\n0.6848727\n\n\nparameter\n1\n\n\neffect_size\n0.04816233\n\n\n\n\n\n\nThere is also no difference when we consider the number of articles that have at least one instance of condition-first language.\n\nCodeget_chisq_tabloid_broadsheet(pers_4chisq_matrix_noarticles, \n           col_no = 2,\n           type = \"art\") |>\n  kable()\n\n\n\nTable 28: Results of Chi-square test between the number of articles that use the adjective lemma OBESE in UK broadsheets vs tabloids vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\nmethod\nChi-squared test for given probabilities\n\n\nbroadsheet_observed\n38\n\n\nbroadsheet_expected\n32.96555\n\n\ntabloid_observed\n29\n\n\ntabloid_expected\n34.03445\n\n\nstatistic\n1.513559\n\n\np.value\n0.2185969\n\n\nparameter\n1\n\n\neffect_size\n0.1503011\n\n\n\n\n\n\nIf we consider a t-test on the frequency (per million words) of the UK corpus (including all texts in each sub-corpus, including those without instances), a very small difference is detected, with a higher mean frequency in tabloids than broadsheets.\n\nCodettest_broad_vs_tabl(frequency_table, \"UK\", \"obes_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between broadsheet and tabloid (mean of x = 62.96, mean of y = 96.08) suggests that the effect is negative, statistically significant, and very small (difference = -33.12, 95% CI [-54.84, -11.40], t(13745.07) = -2.99, p = 0.003; Cohen’s d = -0.05, 95% CI [-0.08, -0.02])\n\nAgain, like the case of the Australian corpus, this could be explained by articles in tabloids being overall shorter than in broadsheets in the UK corpus.\nThere is some support for this difference when using a non-parametric FP test (p < 0.01):\n\nCodefp_test_broad_vs_tabl(frequency_table,\n        corpus_label = \"UK\",\n        dataset = \"obes\", \n        formula(obes_freq ~ source_type), \n        dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  obes_freq by source_type (broadsheet, tabloid)\nZ = -2.9707, p-value = 0.0021\nalternative hypothesis: true mu is not equal to 0\n\n\nAustralian vs UK broadsheets\nThere is not a strong difference in use of the adjective lemma OBESE between Australian and UK broadsheets.\n\nCodeget_chisq_aus_uk(obes_4chisq_matrix_sumhits, \n       row_no = 1,\n       type = \"wc\") |>\n  kable()\n\n\n\nTable 29: Results of Chi-square test between the number of articles that use the adjective lemma OBESE in broadsheets in the UK and Australia vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\nbroadsheet\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n72\n\n\nAUS_expected\n85.50766\n\n\nUK_observed\n263\n\n\nUK_expected\n249.4923\n\n\nstatistic\n2.865119\n\n\np.value\n0.09051901\n\n\nparameter\n1\n\n\neffect_size\n0.09248024\n\n\n\n\n\n\nThis lack of a difference also holds true when we consider the number of articles that have at least one instance of the adjective lemma OBESE:\n\nCodeget_chisq_aus_uk(obes_4chisq_matrix_sumhits, \n           row_no = 1,\n           type = \"art\") |>\n  kable()\n\n\n\nTable 30: Results of Chi-square test between the number of articles that use the adjective lemma OBESE in broadsheets in the UK and Australia vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\nbroadsheet\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n72\n\n\nAUS_expected\n75.80893\n\n\nUK_observed\n263\n\n\nUK_expected\n259.1911\n\n\nstatistic\n0.247349\n\n\np.value\n0.6189479\n\n\nparameter\n1\n\n\neffect_size\n0.02717269\n\n\n\n\n\n\nIf we consider the frequency (per million words) of the Australian vs UK broadsheets (including all texts in each sub-corpus, including those without instances), a very small difference is detected, with UK broadsheets having a higher frequency than Australian ones:\n\nCodettest_aus_vs_uk(frequency_table, \"broadsheet\", \"obes_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between aus and uk (mean of x = 42.65, mean of y = 62.96) suggests that the effect is negative, statistically significant, and very small (difference = -20.31, 95% CI [-37.99, -2.62], t(6599.44) = -2.25, p = 0.024; Cohen’s d = -0.06, 95% CI [-0.10, -7.15e-03])\n\nThis is also supported by the FP test:\n\nCodefp_test_aus_vs_uk(freq_table = frequency_table, \n  source_label = \"broadsheet\",\n  dataset = \"obes\",\n  myformula = formula(obes_freq ~ corpus),\n  dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  obes_freq by corpus (AUS, UK)\nZ = -1.6747, p-value = 0.0883\nalternative hypothesis: true mu is not equal to 0\n\n\nThis discrepancy may be explained by the fact that, as discussed above, articles in UK broadsheets are somewhat shorter than in Australian ones.\nAustralian vs UK tabloids\nThere is a somewhat lower number of total instances of use of the adjective lemma OBESE in Australian tabloids, and a higher number in UK tabloids (p < 0.05):\n\nCodeget_chisq_aus_uk(obes_4chisq_matrix_sumhits, \n       row_no = 2,\n       type = \"wc\") |>\n  kable()\n\n\n\nTable 31: Results of Chi-square test between the number of articles that use the adjective lemma OBESE in tabloids in the UK and Australia vs the number that would be expected based on total word count of articles from each of those subcorpora.\n\n\ntabloid\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n78\n\n\nAUS_expected\n98.00055\n\n\nUK_observed\n264\n\n\nUK_expected\n243.9994\n\n\nstatistic\n5.721273\n\n\np.value\n0.01676057\n\n\nparameter\n1\n\n\neffect_size\n0.1293401\n\n\n\n\n\n\nThis also holds true when we consider the number of articles that have at least one instance of the adjective lemma OBESE:\n\nCodeget_chisq_aus_uk(obes_4chisq_matrix_noarticles, \n           row_no  = 2,\n           type = \"art\") |>\n  kable()\n\n\n\nTable 32: Results of Chi-square test between the number of articles that use the adjective lemma OBESE in tabloids in the UK and Australia vs the number that would be expected based on total number of articles from each of those subcorpora.\n\n\ntabloid\n\n\n\nmethod\nChi-squared test for given probabilities\n\n\nAUS_observed\n69\n\n\nAUS_expected\n84.33367\n\n\nUK_observed\n223\n\n\nUK_expected\n207.6663\n\n\nstatistic\n3.920197\n\n\np.value\n0.0477093\n\n\nparameter\n1\n\n\neffect_size\n0.1158677\n\n\n\n\n\n\nIf we consider the frequency (per million words) in the Australian vs UK tabloids (including all texts in each sub-corpus, including those without instances), no significant differences in frequency are observed.\n\nCodettest_aus_vs_uk(frequency_table, \"tabloid\", \"obes_freq\")\nEffect sizes were labelled following Cohen’s (1988) recommendations.\nThe Welch Two Sample t-test testing the difference between aus and uk (mean of x = 70.46, mean of y = 96.08) suggests that the effect is negative, statistically not significant, and very small (difference = -25.62, 95% CI [-54.08, 2.85], t(7380.43) = -1.76, p = 0.078; Cohen’s d = -0.04, 95% CI [-0.09, 4.57e-03])\n\nThis is also supported by the FP test:\n\nCodefp_test_aus_vs_uk(freq_table = frequency_table, \n  source_label = \"tabloid\",\n  dataset = \"obes\",\n  myformula = formula(obes_freq ~ corpus),\n  dist = mydistribution)\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\ndata:  obes_freq by corpus (AUS, UK)\nZ = -1.5974, p-value = 0.1202\nalternative hypothesis: true mu is not equal to 0\n\n\nAs shown above, there is no significant difference in articles lengths between Australian and UK tabloids."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Obesity comparison of UK and Australian corpora",
    "section": "",
    "text": "Please see the rendered analysis results and code here."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "The Sydney Informatics Hub is a Core Research Facility of the University of Sydney.\nThe use of the SIH services including the Artemis HPC and associated support and training warrants acknowledgement in any publications, conference proceedings or posters describing work facilitated by these services.\nThe continued acknowledgement of the use of SIH facilities ensures the sustainability of our services."
  },
  {
    "objectID": "about.html#suggested-wording",
    "href": "about.html#suggested-wording",
    "title": "Sydney Informatics Hub",
    "section": "Suggested wording",
    "text": "Suggested wording\n\nGeneral acknowledgement:\nThe authors acknowledge the technical assistance provided by the Sydney Informatics Hub, a Core Research Facility of the University of Sydney.\n\n\nAcknowledging specific staff:\nThe authors acknowledge the technical assistance of (name of staff) of the Sydney Informatics Hub, a Core Research Facility of the University of Sydney.\nFor further information about acknowledging the Sydney Informatics Hub, please contact us at sih.info@sydney.edu.au."
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/dataEDA.html",
    "href": "100_data_cleaning_scripts_EDA/dataEDA.html",
    "title": "Loading and exploring the data",
    "section": "",
    "text": "In this file, we start by loading and exploring the data for the UK and Australian obesity corpus.\nThe Australian corpus was first explored (for texts that had >= 3 mentions of the word “obes*“, to maintain consistency with the UK corpus) using the same syntax as in our previous manuscript. This resulted in the”May” dataset (note spaces have been added after the pipe to enable line breaks below; spaces were not part of the original query).\n\nMay Condition-first: “obese (adolescent| adolescents| adult| adults| American| Americans| amputee| amputees| asthmatic| asthmatics| aussie| aussies| australian| Australians| banker| bankers| boss| bosses| boy| boys| Brit| Brits| Canberran| Canberrans| child| children| citizen| citizens| client| clients| contestant| contestants| customer| customers| dad| dads| daughter| daughters| diabetic| diabetics| dieter| dieters| driver| drivers| employee| employees| fan| fans| father| fathers| Frenchman| Frenchmen| Frenchwomen| friend| friends| girl| girls| guy| guys| individual| individuals| kid| kids| ladies| lady| man| men| model| models| mother| mothers| motorist| motorists| mum| mums| pal| pals| parent| parents| participant| participants| passenger| passengers| patient| patients| people| person| persons| preschooler| preschoolers| Queenslander| Queenslanders| resident| residents| smoker| smokers| socialite| socialites| soldier| soldiers| son| sons| student| students| subject| subjects| Tasmanian| Tasmanians| teen| teenager| teenagers| teens| traveller| travellers| Victorian| Victorians| volunteer| volunteers| woman| women| worker| workers| youngster| youngsters)”\nMay Person-first: “(adolescent| adolescents| adult| adults| American| Americans| amputee| amputees| asthmatic| asthmatics| aussie| aussies| australian| Australians| banker| bankers| boss| bosses| boy| boys| Brit| Brits| Canberran| Canberrans| child| children| citizen| citizens| client| clients| contestant| contestants| customer| customers| dad| dads| daughter| daughters| diabetic| diabetics| dieter| dieters| driver| drivers| employee| employees| fan| fans| father| fathers| Frenchman| Frenchmen| Frenchwomen| friend| friends| girl| girls| guy| guys| individual| individuals| kid| kids| ladies| lady| man| men| model| models| mother| mothers| motorist| motorists| mum| mums| pal| pals| parent| parents| participant| participants| passenger| passengers| patient| patients| people| person| persons| preschooler| preschoolers| Queenslander| Queenslanders| resident| residents| smoker| smokers| socialite| socialites| soldier| soldiers| son| sons| student| students| subject| subjects| Tasmanian| Tasmanians| teen| teenager| teenagers| teens| traveller| travellers| Victorian| Victorians| volunteer| volunteers| woman| women| worker| workers| youngster| youngsters| those| many) with * obesity”, in subcorpus “Tara Parallel Corpus”\n\nIt was then explored using the same syntax as the UK corpus, resulting in the early July dataset.\n\nUK Condition first: query “[word=”obese”%c] [semtag=”S2.*”]”\nUK Person first: “[semtag=”S2.*”] [word=”with”] []? [word=”obesity”]”\nJuly Condition first: “[word=”obese”%c] [semtag=”S2.*”]”, restricted to texts meeting - criteria “obesity_boolean_ukcorpus\nJuly Person-first: “[semtag=”S2.*”] [word=”with”] []? [word=”obesity”]”, restricted to texts meeting criteria “obesity_boolean_ukcorpus\n\nFinally, a different syntax was used to generate the final UK and Australian corpora. This “late July” dataset was used for subsequent comparison and analysis in the paper:\n\nCondition-first language: “[word=”obese”%c] [fullsemtag=”.S2.”]” (for Australian corpus: meeting criteria “obesity_boolean_ukcorpus: True”)\nPerson-first language: “[fullsemtag=”.S2.”] [word=”with”] []? [word=”obesity”]”\nNominal “the obese”: the_obese_nominal->Yes (2022-Jul-12 17:55): query “the obese” returned 870 matches in 713 different texts, manually categorised as “1” (UK corpus) and the_obese_nominal (1)->Yes (2022-Mar-18 05:03): query “the obese”, in subcorpus “Tara Parallel Corpus”, returned 296 matches in 259 different texts, manually categorised as “1&rdquo (Aus corpus)\nPerson-first language: UK vs Australia: i.e. uk_with_obesity_new.txt vs aus_subset_with_obesity_new.txt.\n“The adjective lemma OBESE”: UK vs Australia i.e uk_the obese text-frequency-data.txt vs aoc_subset_the_obese_cat_nominal.txt\n\n\nCodelibrary(here)\nlibrary(janitor)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(patchwork)\ntheme_set(theme_minimal())\n\n\n\n\nread_cqpweb <- function(filename){\n  read.csv(\n    here(\"100_data_raw\", filename), \n    skip = 3, sep = \"\\t\") |> \n    janitor::clean_names() |> \n  rename(article_id = text)\n}\n\n\n\nCodemay_aus_cond <- read_cqpweb(\"may/aoc_subset_condition_first.txt\")\njul_aus_cond <- read_cqpweb(\"july/aoc_subset_condition_first.txt\")\nuk_cond <- read_cqpweb(\"british_data/uk_condition first lang text-frequency-data.txt\") \n  \nmay_aus_pers <- read_cqpweb(\"may/aoc_subset_person_first.txt\")\njul_aus_pers <- read_cqpweb(\"july/aoc_subset_person_first.txt\")\nuk_pers <- read_cqpweb(\"british_data/uk_person first language text-frequency-data.txt\") \n\nmay_aus_obese <- read_cqpweb(\"may/aoc_subset_the_obese_cat_nominal.txt\")\njul_aus_obese <- read_cqpweb(\"july/aoc_subset_the_obese_cat_nominal.txt\")\nuk_obes <- read_cqpweb(\"british_data/uk_the obese nominal text-frequency-data.txt\") \n\n# grab the late july datasets\nlatejul_uk_cond <- read_cqpweb(\"new_syntax_july22/uk_obese_first_new.txt\")\nlatejul_au_cond <- read_cqpweb(\"new_syntax_july22/aus_subset_obese_first_new.txt\")\nlatejul_uk_pers <- read_cqpweb(\"new_syntax_july22/uk_with_obesity_new.txt\")\nlatejul_au_pers <- read_cqpweb(\"new_syntax_july22/aus_subset_with_obesity_new.txt\")\nlatejul_uk_theobese <- read_cqpweb(\"new_syntax_july22/uk_the obese nominal text-frequency-data.txt\")\nlatejul_au_theobese <- read_cqpweb(\"new_syntax_july22/aoc_subset_the_obese_cat_nominal.txt\")\nlatejul_uk_fullwc <- read_cqpweb(\"new_syntax_july22/UK corpus text-frequency-data.txt\") |> select(article_id, no_words_in_text)\nlatejul_au_fullwc <- read_cqpweb(\"new_syntax_july22/aus_subset_text_frequency_data.txt\") |> select(article_id, no_words_in_text)\n\n# this is the early JULY dataset!\nall_aus <- rbind({jul_aus_cond |> mutate(dataset = \"cond\")}, \n                {jul_aus_pers |> mutate(dataset = \"pers\")}, \n                {jul_aus_obese |> mutate(dataset = \"obes\")})\n\n# late july datasets\nlatejul_all_au <- rbind(\n  {latejul_au_cond |> mutate(dataset = \"cond\")}, \n  {latejul_au_pers |> mutate(dataset = \"pers\")}, \n  {latejul_au_theobese |> mutate(dataset = \"obes\")})\n\nlatejul_all_uk <- rbind(\n  {latejul_uk_cond |> mutate(dataset = \"cond\")}, \n  {latejul_uk_pers |> mutate(dataset = \"pers\")}, \n  {latejul_uk_theobese |> mutate(dataset = \"obes\")})\n\n# joined datasets\njoined_cond <- dplyr::full_join(\n  dplyr::full_join(may_aus_cond, \n                   jul_aus_cond, by = \"article_id\", suffix = c(\"may\", \"jul\")),\n  # using suffix = c(\"may\", \"jul\") doesn't work\n  latejul_au_cond, by = \"article_id\") |> rename(\n        no_words_in_textlatejul = no_words_in_text,\n        no_hits_in_textlatejul = no_hits_in_text,\n        freq_per_million_wordslatejul = freq_per_million_words\n  )\n\njoined_pers <- dplyr::full_join(full_join(may_aus_pers, jul_aus_pers, by = \"article_id\", suffix = c(\"may\", \"jul\")),\n  # using suffix = c(\"may\", \"jul\") doesn't work\n  latejul_au_pers, by = \"article_id\") |> rename(\n        no_words_in_textlatejul = no_words_in_text,\n        no_hits_in_textlatejul = no_hits_in_text,\n        freq_per_million_wordslatejul = freq_per_million_words\n  )\njoined_obese <- dplyr::full_join(full_join(may_aus_obese, jul_aus_obese, by = \"article_id\", suffix = c(\"may\", \"jul\")),\n  # using suffix = c(\"may\", \"jul\") doesn't work\n  latejul_au_theobese, by = \"article_id\") |> rename(\n        no_words_in_textlatejul = no_words_in_text,\n        no_hits_in_textlatejul = no_hits_in_text,\n        freq_per_million_wordslatejul = freq_per_million_words\n  )\n\n# australian metadata\naus_tabloid_anno <- read_csv(here::here(\"100_data_raw/aus_subset_data\",\n                                    \"addition_source_metadata.csv\")) |>\n  select(source, source_type)\n\naus_metadata <- read_csv(here::here(\"100_data_raw/aus_subset_data\",\n                                    \"corpus_3plus_metadata.csv\")) |> filter(obesity_boolean_ukcorpus == TRUE) |>\n  rename(source_code = shortcode) |>\n  select(article_id, source_code, year, month_metadata, source, date) |>\n  rename(month = month_metadata) |>\n  left_join(aus_tabloid_anno, by = \"source\") |>\n  left_join(latejul_au_fullwc, by = \"article_id\")\n\n# this is a file I created from information in a spreadsheet Monika provided manually\nuk_source_anno <- read_csv(here::here(\"100_data_raw\", \"british_corpus_source_annotation.csv\"))"
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/dataEDA.html#normalised-frequency",
    "href": "100_data_cleaning_scripts_EDA/dataEDA.html#normalised-frequency",
    "title": "Loading and exploring the data",
    "section": "Normalised frequency",
    "text": "Normalised frequency\nWhat is the normalised frequency of each of the language types?\n\nCodeget_norm_freq <- function(df, wc_aus_broad, wc_aus_tab, wc_uk_broad, wc_uk_tab){\n  df |> \n  group_by(dataset, corpus, source_type) |>\n  summarise(total_instances = sum(no_hits_in_text)) |>\n  # create tmp column b/c case_when doesn't work with multiple conditions\n  mutate(tmp = paste0(corpus, source_type)) |>\n  ungroup() |>\n  mutate(total_words_in_corpus = case_when(\n    tmp == \"AUSbroadsheet\" ~ wc_aus_broad,\n    tmp == \"AUStabloid\" ~ wc_aus_tab,\n    tmp == \"UKbroadsheet\" ~ wc_uk_broad,\n    tmp == \"UKtabloid\" ~ wc_uk_tab\n  )) |>\n  select(-tmp) |>\n  mutate(normalised_corpus_per_million = 10^6*total_instances/total_words_in_corpus) \n}\n\nfreq_normalised <- get_norm_freq(both_datasets,\n                                 total_words_aus_broad,\n                                 total_words_aus_tab,\n                                 total_words_uk_broad,\n                                 total_words_uk_tab)\n  \nfreq_normalised_bothyears <- both_datasets |> \n  filter(year >= 2008 & year <=2016) |>\n  get_norm_freq(\n    total_words_aus_broad_bothyears,\n    total_words_aus_tab_bothyears,\n    total_words_uk_broad_bothyears,\n    total_words_uk_tab_bothyears)\n\nfreq_normalised |>\n  kable()\n\n\n\nTable 18: Normalised frequency, instances per million words as counted using CQP-web, across the entire UK and Australian corpora.\n\n\n\n\n\n\n\n\n\ndataset\ncorpus\nsource_type\ntotal_instances\ntotal_words_in_corpus\nnormalised_corpus_per_million\n\n\n\ncond\nAUS\nbroadsheet\n1360\n1860166\n731.117545\n\n\ncond\nAUS\ntabloid\n1718\n1758364\n977.044571\n\n\ncond\nUK\nbroadsheet\n2859\n5427551\n526.756911\n\n\ncond\nUK\ntabloid\n2892\n4377933\n660.585715\n\n\nobes\nAUS\nbroadsheet\n77\n1860166\n41.394155\n\n\nobes\nAUS\ntabloid\n89\n1758364\n50.615231\n\n\nobes\nUK\nbroadsheet\n315\n5427551\n58.037225\n\n\nobes\nUK\ntabloid\n317\n4377933\n72.408600\n\n\npers\nAUS\nbroadsheet\n68\n1860166\n36.555877\n\n\npers\nAUS\ntabloid\n29\n1758364\n16.492603\n\n\npers\nUK\nbroadsheet\n45\n5427551\n8.291032\n\n\npers\nUK\ntabloid\n30\n4377933\n6.852549\n\n\n\n\n\n\nWhat about the normalised frequency across the time period where we have data for both corpora?\n\nCodefreq_normalised_bothyears |>\n  kable()\n\n\n\nTable 19: Normalised frequency, instances per million words as counted using CQP-web, across the UK and Australian corpora between 2008 and 2016, when we have data from both countries (i.e. the final corpus used for comparison).\n\n\n\n\n\n\n\n\n\ndataset\ncorpus\nsource_type\ntotal_instances\ntotal_words_in_corpus\nnormalised_corpus_per_million\n\n\n\ncond\nAUS\nbroadsheet\n1143\n1446391\n790.242749\n\n\ncond\nAUS\ntabloid\n1428\n1418041\n1007.023069\n\n\ncond\nUK\nbroadsheet\n2320\n4333157\n535.406402\n\n\ncond\nUK\ntabloid\n2399\n3596164\n667.099721\n\n\nobes\nAUS\nbroadsheet\n72\n1446391\n49.779071\n\n\nobes\nAUS\ntabloid\n78\n1418041\n55.005462\n\n\nobes\nUK\nbroadsheet\n263\n4333157\n60.694778\n\n\nobes\nUK\ntabloid\n264\n3596164\n73.411557\n\n\npers\nAUS\nbroadsheet\n30\n1446391\n20.741280\n\n\npers\nAUS\ntabloid\n15\n1418041\n10.577973\n\n\npers\nUK\nbroadsheet\n41\n4333157\n9.461923\n\n\npers\nUK\ntabloid\n30\n3596164\n8.342222\n\n\n\n\n\n\n\nCodeexport_summary_stats <- function(freq_calc1, freq_norm, outfile){\n  rbind(\n  {freq_calc1 %>%\n  pivot_longer(cols = c(\"total_instances\", \"total_articles_with_one_or_more_instance\"))},\n  {freq_norm %>% \n  select(-total_instances, -total_words_in_corpus) %>%\n  pivot_longer(cols = \"normalised_corpus_per_million\")}) %>%\n  pivot_wider(names_from = corpus, values_from = value) %>%\n  # reorder in the order Monika has in her xlsx file\n  mutate(dataset = factor(dataset, levels = c(\"pers\", \"cond\", \"obes\")),\n         source_type = factor(source_type, levels = c(\"tabloid\", \"broadsheet\")),\n         name = factor(name,\n                       levels = c(\"total_instances\",\n                                  \"normalised_corpus_per_million\",\n                                  \"total_articles_with_one_or_more_instance\"))\n         ) %>%\n  arrange(dataset, source_type, name) %>%\n  write_csv(file = here::here(\"300_data_processed\", paste0(outfile, \".csv\")))\n}\n\n# export so Monika can copy-paste into csv file\nexport_summary_stats(freq_calc1 = freq_calc,\n                     freq_norm = freq_normalised,\n                     outfile = \"all_data\")\n\nexport_summary_stats(freq_calc1 = freq_calc_bothyears,\n                     freq_norm = freq_normalised_bothyears,\n                     outfile = \"overlapping_years\")\n\n# how many words in corpus?\nrbind(\n  {freq_normalised_bothyears |> \n  select(corpus, source_type, total_words_in_corpus) |>\n  distinct() |>\n  mutate(period = \"2008-2016\")},\n  {freq_normalised |>\n  select(corpus, source_type, total_words_in_corpus) |>\n  distinct() |>\n  mutate(period = \"alltime\")}\n) %>%\n  pivot_wider(names_from = period, values_from = total_words_in_corpus) |>\n  write_csv(file = here::here(\"300_data_processed\", \"wordcounts_corpus.csv\"))\n\n\nA small sanity check about the differences:\nAs discussed above:\n\nThere is no data from Australia for 2006-2007\nBoth countries have data from 2008-2016\nThere is no data from the UK for 2017-2019\n\nHow many articles with how many words do we have in Australia in 2017 - 2019?\n\nCodeaus_missing_summary <- aus_metadata |>\n  filter(year >= 2017) |>\n  group_by(year, source_type) |> \n  summarise(count = n(),\n            wc = sum(no_words_in_text)) |>\n  pivot_wider(names_from = source_type, values_from = c(\"count\", \"wc\"))\n\naus_full_summary <- aus_metadata |>\n  group_by(year, source_type) |> \n  summarise(count = n(),\n            wc = sum(no_words_in_text)) |>\n  pivot_wider(names_from = source_type, values_from = c(\"count\", \"wc\"))\n\naus_missing_summary |> kable()\n\n\n\nTable 20: Number of articles and their word count, from Australian tabloids and broadsheets, filtered out from the dataset due to no matching data existing from the UK.\n\nyear\ncount_broadsheet\ncount_tabloid\nwc_broadsheet\nwc_tabloid\n\n\n\n2017\n179\n218\n147728\n122405\n\n\n2018\n182\n246\n170085\n140285\n\n\n2019\n108\n166\n95962\n77633\n\n\n\n\n\n\nTotal filtered out from Australian corpus:\n\nCodesummary_clean <- function(df) {\n  x <- df |>\n    ungroup() |>\n    summarise(across(everything(), sum)) |>\n    select(-year) |>\n    data.table::transpose() \n  x$variable <- colnames(df)[colnames(df) != \"year\"]\n  x |> select(variable, V1)\n}\n\nsummary_clean(aus_full_summary) |>\n  rename(full_dataset = V1) |>\n  left_join(\n  {summary_clean(aus_missing_summary) |>\n  rename(filtered_out = V1)}\n  ) |> mutate(\n    remaining = full_dataset - filtered_out,\n    perc_filt_out = round(100* filtered_out/full_dataset, digits = 2)\n  ) |> kable()\n\n\n\nTable 21: Proportion of data filtered out from the Australian corpus.\n\n\n\n\n\n\n\n\nvariable\nfull_dataset\nfiltered_out\nremaining\nperc_filt_out\n\n\n\ncount_broadsheet\n2210\n469\n1741\n21.22\n\n\ncount_tabloid\n3168\n630\n2538\n19.89\n\n\nwc_broadsheet\n1860166\n413775\n1446391\n22.24\n\n\nwc_tabloid\n1758364\n340323\n1418041\n19.35\n\n\n\n\n\n\nHow many articles with how many words do we have in the UK in 2006 - 2007?\n\nCodeuk_missing_summary <- full_metadata |>\n  filter(corpus == \"UK\") |>\n  filter(year <= 2007) |>\n  group_by(year, source_type) |> \n  summarise(count = n(),\n            wc = sum(no_words_in_text)) |>\n  pivot_wider(names_from = source_type, values_from = c(\"count\", \"wc\"))\n\nuk_missing_summary |> kable()\n\n\n\nTable 22: Number of articles and their word count, from UK tabloids and broadsheets, filtered out from the dataset due to no matching data existing from Australia.\n\nyear\ncount_broadsheet\ncount_tabloid\nwc_broadsheet\nwc_tabloid\n\n\n\n2006\n773\n571\n554728\n338643\n\n\n2007\n754\n775\n539666\n443126\n\n\n\n\n\n\nTotal filtered out from UK corpus:\n\nCodeuk_full_summary <-  full_metadata |>\n  filter(corpus == \"UK\") |>\n  group_by(year, source_type) |> \n  summarise(count = n(),\n            wc = sum(no_words_in_text)) |>\n  pivot_wider(names_from = source_type, values_from = c(\"count\", \"wc\"))\n\nsummary_clean(uk_full_summary) |>\n  rename(full_dataset = V1) |>\n  left_join(\n  {summary_clean(uk_missing_summary) |>\n  rename(filtered_out = V1)}\n  ) |> mutate(\n    remaining = full_dataset - filtered_out,\n    perc_filt_out = round(100* filtered_out/full_dataset, digits = 2)\n  ) |> kable()\n\n\n\nTable 23: Proportion of data filtered out from the UK corpus.\n\n\n\n\n\n\n\n\nvariable\nfull_dataset\nfiltered_out\nremaining\nperc_filt_out\n\n\n\ncount_broadsheet\n7556\n1527\n6029\n20.21\n\n\ncount_tabloid\n7801\n1346\n6455\n17.25\n\n\nwc_broadsheet\n5427551\n1094394\n4333157\n20.16\n\n\nwc_tabloid\n4377933\n781769\n3596164\n17.86"
  },
  {
    "objectID": "000_scoping/scoping.html",
    "href": "000_scoping/scoping.html",
    "title": "PIPE-3034: Obesity stigma analysis with pattern frequency over time and publication",
    "section": "",
    "text": "Scope link, Sharepoint Usyd"
  },
  {
    "objectID": "000_scoping/scoping.html#project-details",
    "href": "000_scoping/scoping.html#project-details",
    "title": "PIPE-3034: Obesity stigma analysis with pattern frequency over time and publication",
    "section": "1. Project details",
    "text": "1. Project details\n\nClients: Monika Bednarek\nFaculty: FASS\nCollaborators and their affiliations: Carly Bray (USyd PhD Student), Tara (Lancaster),"
  },
  {
    "objectID": "000_scoping/scoping.html#project-scoping-details",
    "href": "000_scoping/scoping.html#project-scoping-details",
    "title": "PIPE-3034: Obesity stigma analysis with pattern frequency over time and publication",
    "section": "Project scoping details",
    "text": "Project scoping details\n\nScope prepared by Joel Nothman, Feb 2022\nProject manager - Marius Mather"
  }
]