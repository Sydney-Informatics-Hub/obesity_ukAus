---
title: "Loading and exploring the data"
author: "Darya Vanichkina"
execute: 
  message: false
  warning: false
---

### Executive summary

***

In this file, we start by loading and exploring the data for the UK and Australian obesity corpus.

The Australian corpus was first explored (for texts that had >= 3 mentions of the word "obes*", to maintain consistency with the UK corpus) using the same syntax as in our previous manuscript. This resulted in the "May" dataset (note spaces have been added after the pipe to enable line breaks below; spaces were not part of the original query).

- May Condition-first: “obese (adolescent| adolescents| adult| adults| American| Americans| amputee| amputees| asthmatic| asthmatics| aussie| aussies| australian| Australians| banker| bankers| boss| bosses| boy| boys| Brit| Brits| Canberran| Canberrans| child| children| citizen| citizens| client| clients| contestant| contestants| customer| customers| dad| dads| daughter| daughters| diabetic| diabetics| dieter| dieters| driver| drivers| employee| employees| fan| fans| father| fathers| Frenchman| Frenchmen| Frenchwomen| friend| friends| girl| girls| guy| guys| individual| individuals| kid| kids| ladies| lady| man| men| model| models| mother| mothers| motorist| motorists| mum| mums| pal| pals| parent| parents| participant| participants| passenger| passengers| patient| patients| people| person| persons| preschooler| preschoolers| Queenslander| Queenslanders| resident| residents| smoker| smokers| socialite| socialites| soldier| soldiers| son| sons| student| students| subject| subjects| Tasmanian| Tasmanians| teen| teenager| teenagers| teens| traveller| travellers| Victorian| Victorians| volunteer| volunteers| woman| women| worker| workers| youngster| youngsters)"

- May Person-first: “(adolescent| adolescents| adult| adults| American| Americans| amputee| amputees| asthmatic| asthmatics| aussie| aussies| australian| Australians| banker| bankers| boss| bosses| boy| boys| Brit| Brits| Canberran| Canberrans| child| children| citizen| citizens| client| clients| contestant| contestants| customer| customers| dad| dads| daughter| daughters| diabetic| diabetics| dieter| dieters| driver| drivers| employee| employees| fan| fans| father| fathers| Frenchman| Frenchmen| Frenchwomen| friend| friends| girl| girls| guy| guys| individual| individuals| kid| kids| ladies| lady| man| men| model| models| mother| mothers| motorist| motorists| mum| mums| pal| pals| parent| parents| participant| participants| passenger| passengers| patient| patients| people| person| persons| preschooler| preschoolers| Queenslander| Queenslanders| resident| residents| smoker| smokers| socialite| socialites| soldier| soldiers| son| sons| student| students| subject| subjects| Tasmanian| Tasmanians| teen| teenager| teenagers| teens| traveller| travellers| Victorian| Victorians| volunteer| volunteers| woman| women| worker| workers| youngster| youngsters| those| many) with * obesity", in subcorpus "Tara Parallel Corpus"


It was then explored using the same syntax as the UK corpus, resulting in the early July dataset.

- UK Condition first: query “[word="obese"%c] [semtag="S2.*"]"
- UK Person first: “[semtag="S2.*"] [word="with"] []? [word="obesity"]"

- July Condition first: “[word="obese"%c] [semtag="S2.*"]", restricted to texts meeting - criteria "*obesity_boolean_ukcorpus*
- July Person-first: “[semtag="S2.*"] [word="with"] []? [word="obesity"]", restricted to texts meeting criteria "*obesity_boolean_ukcorpus*

Finally, a different syntax was used to generate the final UK and Australian corpora. This "late July" dataset was used for subsequent comparison and analysis in the paper:

- Condition-first language: "[word="obese"%c] [fullsemtag=".*S2.*"]" (for Australian corpus: meeting criteria "*obesity_boolean_ukcorpus*: *True*")
- Person-first language: "[fullsemtag=".*S2.*"] [word="with"] []? [word="obesity"]"
- Nominal "the obese": the_obese_nominal->Yes (2022-Jul-12 17:55):  query “the obese" returned 870 matches in 713 different texts, manually categorised as &ldquo;1&rdquo; (UK corpus) and the_obese_nominal (1)->Yes (2022-Mar-18 05:03):  query “the obese", in subcorpus "Tara Parallel Corpus",  returned 296 matches in 259 different texts, manually categorised as &ldquo;1&rdquo (Aus corpus)


- Person-first language: UK vs Australia: i.e. `uk_with_obesity_new.txt` vs `aus_subset_with_obesity_new.txt`.

- "The adjective lemma OBESE": UK vs Australia i.e uk_the obese `text-frequency-data.txt` vs `aoc_subset_the_obese_cat_nominal.txt`

```{r getData, warning=FALSE, message=FALSE}
library(here)
library(janitor)
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(knitr)
library(patchwork)
theme_set(theme_minimal())




read_cqpweb <- function(filename){
  read.csv(
    here("100_data_raw", filename), 
    skip = 3, sep = "\t") |> 
    janitor::clean_names() |> 
  rename(article_id = text)
}
```


```{r loaddata, message=FALSE}
may_aus_cond <- read_cqpweb("may/aoc_subset_condition_first.txt")
jul_aus_cond <- read_cqpweb("july/aoc_subset_condition_first.txt")
uk_cond <- read_cqpweb("british_data/uk_condition first lang text-frequency-data.txt") 
  
may_aus_pers <- read_cqpweb("may/aoc_subset_person_first.txt")
jul_aus_pers <- read_cqpweb("july/aoc_subset_person_first.txt")
uk_pers <- read_cqpweb("british_data/uk_person first language text-frequency-data.txt") 

may_aus_obese <- read_cqpweb("may/aoc_subset_the_obese_cat_nominal.txt")
jul_aus_obese <- read_cqpweb("july/aoc_subset_the_obese_cat_nominal.txt")
uk_obes <- read_cqpweb("british_data/uk_the obese nominal text-frequency-data.txt") 

# grab the late july datasets
latejul_uk_cond <- read_cqpweb("new_syntax_july22/uk_obese_first_new.txt")
latejul_au_cond <- read_cqpweb("new_syntax_july22/aus_subset_obese_first_new.txt")
latejul_uk_pers <- read_cqpweb("new_syntax_july22/uk_with_obesity_new.txt")
latejul_au_pers <- read_cqpweb("new_syntax_july22/aus_subset_with_obesity_new.txt")
latejul_uk_theobese <- read_cqpweb("new_syntax_july22/uk_the obese nominal text-frequency-data.txt")
latejul_au_theobese <- read_cqpweb("new_syntax_july22/aoc_subset_the_obese_cat_nominal.txt")
latejul_uk_fullwc <- read_cqpweb("new_syntax_july22/UK corpus text-frequency-data.txt") |> select(article_id, no_words_in_text)
latejul_au_fullwc <- read_cqpweb("new_syntax_july22/aus_subset_text_frequency_data.txt") |> select(article_id, no_words_in_text)

# this is the early JULY dataset!
all_aus <- rbind({jul_aus_cond |> mutate(dataset = "cond")}, 
                {jul_aus_pers |> mutate(dataset = "pers")}, 
                {jul_aus_obese |> mutate(dataset = "obes")})

# late july datasets
latejul_all_au <- rbind(
  {latejul_au_cond |> mutate(dataset = "cond")}, 
  {latejul_au_pers |> mutate(dataset = "pers")}, 
  {latejul_au_theobese |> mutate(dataset = "obes")})

latejul_all_uk <- rbind(
  {latejul_uk_cond |> mutate(dataset = "cond")}, 
  {latejul_uk_pers |> mutate(dataset = "pers")}, 
  {latejul_uk_theobese |> mutate(dataset = "obes")})

# joined datasets
joined_cond <- dplyr::full_join(
  dplyr::full_join(may_aus_cond, 
                   jul_aus_cond, by = "article_id", suffix = c("may", "jul")),
  # using suffix = c("may", "jul") doesn't work
  latejul_au_cond, by = "article_id") |> rename(
        no_words_in_textlatejul = no_words_in_text,
        no_hits_in_textlatejul = no_hits_in_text,
        freq_per_million_wordslatejul = freq_per_million_words
  )

joined_pers <- dplyr::full_join(full_join(may_aus_pers, jul_aus_pers, by = "article_id", suffix = c("may", "jul")),
  # using suffix = c("may", "jul") doesn't work
  latejul_au_pers, by = "article_id") |> rename(
        no_words_in_textlatejul = no_words_in_text,
        no_hits_in_textlatejul = no_hits_in_text,
        freq_per_million_wordslatejul = freq_per_million_words
  )
joined_obese <- dplyr::full_join(full_join(may_aus_obese, jul_aus_obese, by = "article_id", suffix = c("may", "jul")),
  # using suffix = c("may", "jul") doesn't work
  latejul_au_theobese, by = "article_id") |> rename(
        no_words_in_textlatejul = no_words_in_text,
        no_hits_in_textlatejul = no_hits_in_text,
        freq_per_million_wordslatejul = freq_per_million_words
  )

# australian metadata
aus_tabloid_anno <- read_csv(here::here("100_data_raw/aus_subset_data",
                                    "addition_source_metadata.csv")) |>
  select(source, source_type)

aus_metadata <- read_csv(here::here("100_data_raw/aus_subset_data",
                                    "corpus_3plus_metadata.csv")) |> filter(obesity_boolean_ukcorpus == TRUE) |>
  rename(source_code = shortcode) |>
  select(article_id, source_code, year, month_metadata, source, date) |>
  rename(month = month_metadata) |>
  left_join(aus_tabloid_anno, by = "source") |>
  left_join(latejul_au_fullwc, by = "article_id")

# this is a file I created from information in a spreadsheet Monika provided manually
uk_source_anno <- read_csv(here::here("100_data_raw", "british_corpus_source_annotation.csv"))

 
```


### Australian Data

The Australian May and July data don't seem to match. 

The article word counts always do:

```{r warning=FALSE}
#| label: fig-l20220930181935
#| fig-cap: "Scatter plot of CQP-Web generated article word counts between the May and July Australian corpus."
p1 <- joined_cond |>
  ggplot(aes(x = no_words_in_textmay,
             y = no_words_in_textjul)) +
  geom_point() + labs(title="Condition first") + 
  geom_abline(slope = 1, intercept = 0, lty = 2, col = "red")

p2 <- joined_pers |>
  ggplot(aes(x = no_words_in_textmay,
             y = no_words_in_textjul)) +
  geom_point() + labs(title="Person first") + 
  geom_abline(slope = 1, intercept = 0, lty = 2, col = "red")

p3 <- joined_obese |>
  ggplot(aes(x = no_words_in_textmay,
             y = no_words_in_textjul)) +
  geom_point() + labs(title="Obese") + 
  geom_abline(slope = 1, intercept = 0, lty = 2, col = "red")

p1 | p2 | p3
```

The same is true for the two July datasets.

But then the raw counts do not:

```{r warning=FALSE}
#| label: fig-l20220930182052
#| fig-cap: "Comparison of the number of hits in text parameter from CQP-Web between the May and July corpus indicate that a different number of hits are retrieved using the different query syntax."
p1 <- joined_cond |>
  ggplot(aes(x = no_hits_in_textmay,
             y = no_hits_in_textjul)) +
  geom_point() + labs(title="Condition first") + 
  geom_abline(slope = 1, intercept = 0, lty = 2, col = "red")

p2 <- joined_pers |>
  ggplot(aes(x = no_hits_in_textmay,
             y = no_hits_in_textjul)) +
  geom_point() + labs(title="Person first") + 
  geom_abline(slope = 1, intercept = 0, lty = 2, col = "red")

p3 <- joined_obese |>
  ggplot(aes(x = no_hits_in_textmay,
             y = no_hits_in_textjul)) +
  geom_point() + labs(title="Obese") + 
  geom_abline(slope = 1, intercept = 0, lty = 2, col = "red")

p1 | p2 | p3
```

The same is true when comparing the two July datasets:

```{r warning=FALSE}
#| label: fig-l20220930182254
#| fig-cap: "Comparison of the number of hits in text between the two datasets generated in July."
p1 <- joined_cond |>
  ggplot(aes(x = no_hits_in_textlatejul,
             y = no_hits_in_textjul)) +
  geom_point() + labs(title="Condition first") + 
  geom_abline(slope = 1, intercept = 0, lty = 2, col = "red")

p2 <- joined_pers |>
  ggplot(aes(x = no_hits_in_textlatejul,
             y = no_hits_in_textjul)) +
  geom_point() + labs(title="Person first") + 
  geom_abline(slope = 1, intercept = 0, lty = 2, col = "red")

p3 <- joined_obese |>
  ggplot(aes(x = no_hits_in_textlatejul,
             y = no_hits_in_textjul)) +
  geom_point() + labs(title="Obese") + 
  geom_abline(slope = 1, intercept = 0, lty = 2, col = "red")

p1 | p2 | p3
```

And so the frequencies don't either, for "Condition-first" and "Person-first" language.


How many articles show a mismatch, for condition-first language?

- Matches: article id is in both datasets, number of instances match
- Counts don't match: article id is in both datasets, number of instances do not match
- Only July: article id is only in the early July dataset
- Only May: article id is only in the May dataset

```{r}
#| label: tbl-l20220930182321
#| tbl-cap: "Comparison of number of articles retrieved using the condition-first language search in the May and early July scrapes."
joined_cond |> 
  mutate(matching_count = as.numeric(no_hits_in_textjul == no_hits_in_textmay),
         unmatching_count = as.numeric(no_hits_in_textjul != no_hits_in_textmay),
         only_july = as.numeric(is.na(no_hits_in_textmay)),
         only_may = as.numeric(is.na(no_hits_in_textjul))) |>
  summarize(matches = sum(matching_count, na.rm = T),
            counts_dont_match = sum(unmatching_count, na.rm = T),
            only_july = sum(only_july, na.rm = T),
            only_may = sum(only_may, na.rm = T)) |>
  kable()

```

What about the two July datasets?:

```{r}
#| label: tbl-l20220930182321b
#| tbl-cap: "Comparison of number of articles retrieved using the condition-first language search in the early and late July scrapes."
joined_cond |> 
  mutate(matching_count = as.numeric(no_hits_in_textjul == no_hits_in_textlatejul),
         unmatching_count = as.numeric(no_hits_in_textjul != no_hits_in_textlatejul),
         only_july = as.numeric(is.na(no_hits_in_textlatejul)),
         only_latejul = as.numeric(is.na(no_hits_in_textjul))) |>
  summarize(matches = sum(matching_count, na.rm = T),
            counts_dont_match = sum(unmatching_count, na.rm = T),
            only_july = sum(only_july, na.rm = T),
            only_latejul = sum(only_latejul, na.rm = T)) |>
  kable()
```

Late July syntax clearly picks up more data.


And for person-first language?

```{r}
#| label: tbl-l20220930182321c
#| tbl-cap: "Comparison of number of articles retrieved using the person-first language search in the May and early July scrapes."
joined_pers |> 
  mutate(matching_count = as.numeric(no_hits_in_textjul == no_hits_in_textmay),
         unmatching_count = as.numeric(no_hits_in_textjul != no_hits_in_textmay),
         only_july = as.numeric(is.na(no_hits_in_textmay)),
         only_may = as.numeric(is.na(no_hits_in_textjul))) |>
  summarize(matches = sum(matching_count, na.rm = T),
            counts_dont_match = sum(unmatching_count, na.rm = T),
            only_july = sum(only_july, na.rm = T),
            only_may = sum(only_may, na.rm = T)) |> kable()
```


```{r}
#| label: tbl-l20220930182321d
#| tbl-cap: "Comparison of number of articles retrieved using the person-first language search in the early and late July scrapes."
joined_pers |> 
  mutate(matching_count = as.numeric(no_hits_in_textjul == no_hits_in_textlatejul),
         unmatching_count = as.numeric(no_hits_in_textjul != no_hits_in_textlatejul),
         only_july = as.numeric(is.na(no_hits_in_textlatejul)),
         only_latejul = as.numeric(is.na(no_hits_in_textjul))) |>
  summarize(matches = sum(matching_count, na.rm = T),
            counts_dont_match = sum(unmatching_count, na.rm = T),
            only_july = sum(only_july, na.rm = T),
            only_latejul = sum(only_latejul, na.rm = T)) |> kable()
```

A similar trend is observed between the late July and early July exports.

### UK Data

The UK data was not provided with any metadata, so we will need to break apart the text id to infer this information.

```{r generateUKmetadata}

clean_uk <- function(df){
  df |>
  mutate(
    dateid = stringr::str_extract(article_id, "_.*"),
    sourceyrmo = stringr::str_extract(article_id, ".*_")) |>
  mutate(
    dateid = stringr::str_replace(dateid, "_", ""),
    sourceyrmo = stringr::str_replace(sourceyrmo, "_", "")
  ) |>
  mutate(
    source_code = substr(sourceyrmo, 1, 2),
    year = as.numeric(paste0("20", substr(sourceyrmo, 3,4))),
    month = substr(sourceyrmo, 5, nchar(sourceyrmo)),
    day = stringr::str_extract(dateid, "\\d+"),
    index = stringr::str_extract(dateid, "[a-z]+")) |>
  select(-sourceyrmo, -dateid) |>
    mutate(tmpdate = paste(day, month, year, sep = "/")) |>
    mutate(date = lubridate::dmy(tmpdate)) |>
    select(-tmpdate) |>
    left_join(uk_source_anno, by = "source_code")
}


uk_cond_clean <-clean_uk(uk_cond)
uk_pers_clean <-clean_uk(uk_pers)
uk_obes_clean <-clean_uk(uk_obes)
latejul_uk_fullwc_clean <- clean_uk(latejul_uk_fullwc)
uk_all_of_interest <- rbind(
  {uk_cond_clean |> mutate(dataset = "cond")}, 
  {uk_pers_clean |> mutate(dataset = "pers")}, 
  {uk_obes_clean |> mutate(dataset = "obes")})
```


What sources are there?

```{r uksources}
#| label: tbl-l20220930183328
#| tbl-cap: "UK source publication, and number of articles from each of them that feature each of the language types of interest."
uk_all_of_interest |>
  group_by(dataset) |>
  count(source) |>
  pivot_wider(names_from=dataset, values_from = n, values_fill = 0, id_cols=source) |>
  full_join({latejul_uk_fullwc_clean |> group_by(source) |> count() |> rename(total_articles_in_corpus=n)}) |>
  kable()
```

What years do we have for each of the sources?

For condition-first language:

```{r}
#| label: tbl-l20220930183416
#| tbl-cap: "Number of articles featuring condition-first language by year and source in the UK corpus."
uk_all_of_interest |>
  filter(dataset == "cond") |>
  group_by(year) |>
  count(source) |>
  pivot_wider(names_from=year, values_from = n, values_fill = 0) |>
  t() |>
  kable()
```

For person-first language:

```{r}
#| label: tbl-l20220930183416b
#| tbl-cap: "Number of articles featuring person-first language by year and source in the UK corpus."
uk_all_of_interest |>
  filter(dataset == "pers") |>
  group_by(year) |>
  count(source) |>
  pivot_wider(names_from=year, values_from = n, values_fill = 0) |>
  t() |>
  kable()
```

For the adjective lemma OBESE:

```{r}
#| label: tbl-l20220930183416c
#| tbl-cap: "Number of articles featuring the adjective lemma OBESE by year and source in the UK corpus."
uk_all_of_interest |>
  filter(dataset == "obes") |>
  group_by(year) |>
  count(source) |>
  pivot_wider(names_from=year, values_from = n, values_fill = 0) |>
  t() |>
  kable()
```

For the entire corpus:

```{r}
#| label: tbl-l20220930183531z
#| tbl-cap: "Number of articles in the entire UK corpus, by year and source."
latejul_uk_fullwc_clean |>
  group_by(year) |>
  count(source) |>
  pivot_wider(names_from=year, values_from = n, values_fill = 0) |>
  t() |>
  kable()
```


What years are they coming from?

```{r ukyears}
#| label: tbl-l20220930183531
#| tbl-cap: "Number of articles in the entire UK corpus, by year and language feature."
uk_all_of_interest |>
  group_by(dataset) |>
  count(year) |>
  pivot_wider(names_from=dataset, values_from = n, values_fill = 0, id_cols=year) |>
  full_join({latejul_uk_fullwc_clean |> group_by(year) |> count(year) |> rename(total_articles_in_corpus=n)}) |>
  kable()
```

Note that this is different to the Australian corpus:

```{r}
#| label: tbl-l20220930183531f
#| tbl-cap: "Number of articles in all of the Australian data, by year and language feature."
all_aus |>
  group_by(dataset) |>
  mutate(year = paste0("20", substr(article_id,3, 4))) |>
  count(year) |>
  pivot_wider(names_from=dataset, values_from = n, values_fill = 0, id_cols=year) |>
  kable()
```

So:

- There is no data from Australia for 2006-2007
- Both countries have data from 2008-2016
- There is no data from the UK for 2017-2019



Also sanity-checking that the months in the UK corpus make sense (they do):


```{r ukmonths}
#| label: tbl-l20220930183905
#| tbl-cap: "Number of articles per month in the UK corpus."
uk_all_of_interest |>
  group_by(dataset) |>
  count(month) |>
  pivot_wider(names_from=dataset, values_from = n, values_fill = 0, id_cols=month) |>
  kable()

```

```{r}
#| label: tbl-l20220930183905b
#| tbl-cap: "Number of articles per month in the late July Australian corpus."
latejul_uk_fullwc_clean |>
  count(month) |>
  kable()
```


And the iterative text IDs:

```{r uktextIDs}
#| label: tbl-l20220930183954
#| tbl-cap: "Number of articles with each of the 'iterative' text ids in the UK corpus. As expected, most articles don't have a text id, as this is used to showcase more than one article per day per source being harvested."
uk_all_of_interest |>
  group_by(dataset) |>
  count(index) |>
  pivot_wider(names_from=dataset, values_from = n, values_fill = 0, id_cols=index) |>
  full_join({latejul_uk_fullwc_clean |> group_by(index) |> count(index) |> rename(total_articles_in_corpus=n)}) |>
  arrange(index) |>
  kable()
```

For some reason, there are no "y"s in the indexing, but there are zs... Somewhat suspicious, but would probably affect 1 article tops.


Let's create some clean files we can later use for analysis.

We can join the two metadata files as there are no overlaps between Aus and UK shortcodes:

```{r check}
#| echo: true
# we can join the two metadata files as there are no overlaps between Aus and UK shortcodes etc
table(latejul_uk_fullwc_clean$article_id %in% aus_metadata$article_id)
table(aus_metadata$article_id %in% latejul_uk_fullwc_clean$article_id)
table(latejul_uk_fullwc_clean$source_code %in% aus_metadata$source_code)
table(aus_metadata$source_code %in% latejul_uk_fullwc_clean$source_code)
```

```{r exportClean}
full_metadata <-
  dplyr::bind_rows({
  latejul_uk_fullwc_clean |> mutate(corpus = "UK")
}, 
{
  aus_metadata |> mutate(corpus = "AUS")
})

write_csv(latejul_all_au, 
          here::here("200_data_clean/", "australian_corpus_of_interest.csv"))
write_csv({latejul_all_uk |> select(names(all_aus))}, 
          here::here("200_data_clean/", "uk_corpus_of_interest.csv"))
write_csv(full_metadata, 
          here::here("200_data_clean/", "full_metadata.csv"))

australian <- left_join({latejul_all_au |> select(-no_words_in_text)}, 
                        full_metadata, by = "article_id")
uk <- left_join({latejul_all_uk |> select(-no_words_in_text)}, 
                full_metadata, by = "article_id")
both_datasets <- rbind(australian, uk)

write_csv(both_datasets, 
          here::here("200_data_clean/", "both_datasets.csv"))

```



***

Monika wanted some summary statistics of the datasets, separated by broadsheets and tabloids:

- Raw frequency
- Normalised frequency (to total number of words in corpus)
- Range (document frequency)

This is provided for the:

- UK corpus
- Australian corpus, late July scrape


1. What is the raw frequency and document frequency of use of the language types in the two corpora?

```{r rawFreq}
#| label: tbl-l20220930184147
#| tbl-cap: "Feature identified (dataset), total number of instances and articles with at least one instance in the Australian and UK corpus, categorised by source type."
freq_calc <- both_datasets |> 
  group_by(dataset, corpus, source_type) |>
  summarise(total_instances = sum(no_hits_in_text),
            total_articles_with_one_or_more_instance = n()) 
freq_calc |>
  kable()
```

Note that sources were classified as follows across the two corpora:

```{r}
#| label: tbl-l20220930184341
#| tbl-cap: "Classification of sources in the UK and Australian corpora."
both_datasets |> 
  select(source, corpus, source_type) |> 
  distinct() |>
  kable()
```


We should also provide the export information only for the years where both corpora have data, i.e. 2008-2016.

```{r rawFreqbothyears}
#| label: tbl-l20220930184147b
#| tbl-cap: "Feature identified (dataset), total number of instances and articles with at least one instance in the Australian and UK corpus, categorised by source type, for articles from 2008-2016, when both corpora have data. This is the dataset that was used for comparison in the analysis section."
freq_calc_bothyears <- both_datasets |> 
  filter(year >= 2008 & year <=2016) |>
  group_by(dataset, corpus, source_type) |>
  summarise(total_instances = sum(no_hits_in_text),
            total_articles_with_one_or_more_instance = n()) 

freq_calc_bothyears |>
  kable()
```



2. What is the total word count of the UK and Australian corpus, grouped by broadsheets and tabloids?

```{r}

get_total_wc <- function(metadata_df, crp, st){
  metadata_df |> 
  filter(corpus == crp) |> 
  filter(source_type == st) |> 
  mutate(x = sum(no_words_in_text)) |>
  select(x) |>
  distinct() |>
  pull(x)
}

get_total_wc_bothyears <- function(metadata_df, crp, st){
  metadata_df |> 
  filter(year >= 2008 & year <=2016) |>
  filter(corpus == crp) |> 
  filter(source_type == st) |> 
  mutate(x = sum(no_words_in_text)) |>
  select(x) |>
  distinct() |>
  pull(x)
}
  
  

total_words_aus_broad <- get_total_wc(full_metadata,
                                      crp = "AUS",
                                      st = "broadsheet")
total_words_aus_tab <- get_total_wc(full_metadata,
                                    crp = "AUS",
                                    st = "tabloid")
total_words_uk_broad <- get_total_wc(full_metadata,
                                      crp = "UK",
                                      st = "broadsheet")
total_words_uk_tab <- get_total_wc(full_metadata,
                                    crp = "UK",
                                    st = "tabloid")
total_words_aus_broad_bothyears <- get_total_wc_bothyears(full_metadata,
                                      crp = "AUS",
                                      st = "broadsheet")
total_words_aus_tab_bothyears <- get_total_wc_bothyears(full_metadata,
                                    crp = "AUS",
                                    st = "tabloid")
total_words_uk_broad_bothyears <- get_total_wc_bothyears(full_metadata,
                                      crp = "UK",
                                      st = "broadsheet")
total_words_uk_tab_bothyears <- get_total_wc_bothyears(full_metadata,
                                    crp = "UK",
                                    st = "tabloid")


```

The total word counts are:

- The British tabloids corpus contains `r total_words_uk_tab` words in total
- The British broadsheets corpus contains `r total_words_uk_broad` words in total
- The Australian AOC subset tabloids corpus contains `r total_words_aus_tab` words
- The Australian AOC subset broadsheet corpus contains `r total_words_aus_broad` words

This should match the information provided via the manual CQP web export below.

- The British tabloids corpus contains 4,377,933 words in total (including all texts, not just those that have an instance of the relevant language)
- The British broadsheets corpus contains 5,427,551 words in total (including all texts, not just those that have an instance of the relevant language)
- The Australian AOC subset tabloids corpus contains 1,758,364 words (all included)
- The Australian AOC subset broadsheet corpus contains 1,860,166 words (all included)


The word counts for the years 2008 - 2016, when we have data for both corpora, are:

- The British tabloids corpus contains `r total_words_uk_tab_bothyears` words in total
- The British broadsheets corpus contains `r total_words_uk_broad_bothyears` words in total
- The Australian AOC subset tabloids corpus contains `r total_words_aus_tab_bothyears` words
- The Australian AOC subset broadsheet corpus contains `r total_words_aus_broad_bothyears` words


## Normalised frequency

What is the normalised frequency of each of the language types?


```{r}
#| label: tbl-l20220930185332
#| tbl-cap: "Normalised frequency, instances per million words as counted using CQP-web, across the entire UK and Australian corpora."

get_norm_freq <- function(df, wc_aus_broad, wc_aus_tab, wc_uk_broad, wc_uk_tab){
  df |> 
  group_by(dataset, corpus, source_type) |>
  summarise(total_instances = sum(no_hits_in_text)) |>
  # create tmp column b/c case_when doesn't work with multiple conditions
  mutate(tmp = paste0(corpus, source_type)) |>
  ungroup() |>
  mutate(total_words_in_corpus = case_when(
    tmp == "AUSbroadsheet" ~ wc_aus_broad,
    tmp == "AUStabloid" ~ wc_aus_tab,
    tmp == "UKbroadsheet" ~ wc_uk_broad,
    tmp == "UKtabloid" ~ wc_uk_tab
  )) |>
  select(-tmp) |>
  mutate(normalised_corpus_per_million = 10^6*total_instances/total_words_in_corpus) 
}

freq_normalised <- get_norm_freq(both_datasets,
                                 total_words_aus_broad,
                                 total_words_aus_tab,
                                 total_words_uk_broad,
                                 total_words_uk_tab)
  
freq_normalised_bothyears <- both_datasets |> 
  filter(year >= 2008 & year <=2016) |>
  get_norm_freq(
    total_words_aus_broad_bothyears,
    total_words_aus_tab_bothyears,
    total_words_uk_broad_bothyears,
    total_words_uk_tab_bothyears)

freq_normalised |>
  kable()
```

What about the normalised frequency across the time period where we have data for both corpora?

```{r}
#| label: tbl-l20220930185332b
#| tbl-cap: "Normalised frequency, instances per million words as counted using CQP-web, across the  UK and Australian corpora between 2008 and 2016, when we have data from both countries (i.e. the final corpus used for comparison)."
freq_normalised_bothyears |>
  kable()
```



```{r}
export_summary_stats <- function(freq_calc1, freq_norm, outfile){
  rbind(
  {freq_calc1 %>%
  pivot_longer(cols = c("total_instances", "total_articles_with_one_or_more_instance"))},
  {freq_norm %>% 
  select(-total_instances, -total_words_in_corpus) %>%
  pivot_longer(cols = "normalised_corpus_per_million")}) %>%
  pivot_wider(names_from = corpus, values_from = value) %>%
  # reorder in the order Monika has in her xlsx file
  mutate(dataset = factor(dataset, levels = c("pers", "cond", "obes")),
         source_type = factor(source_type, levels = c("tabloid", "broadsheet")),
         name = factor(name,
                       levels = c("total_instances",
                                  "normalised_corpus_per_million",
                                  "total_articles_with_one_or_more_instance"))
         ) %>%
  arrange(dataset, source_type, name) %>%
  write_csv(file = here::here("300_data_processed", paste0(outfile, ".csv")))
}

# export so Monika can copy-paste into csv file
export_summary_stats(freq_calc1 = freq_calc,
                     freq_norm = freq_normalised,
                     outfile = "all_data")

export_summary_stats(freq_calc1 = freq_calc_bothyears,
                     freq_norm = freq_normalised_bothyears,
                     outfile = "overlapping_years")

# how many words in corpus?
rbind(
  {freq_normalised_bothyears |> 
  select(corpus, source_type, total_words_in_corpus) |>
  distinct() |>
  mutate(period = "2008-2016")},
  {freq_normalised |>
  select(corpus, source_type, total_words_in_corpus) |>
  distinct() |>
  mutate(period = "alltime")}
) %>%
  pivot_wider(names_from = period, values_from = total_words_in_corpus) |>
  write_csv(file = here::here("300_data_processed", "wordcounts_corpus.csv"))
```


A small sanity check about the differences:

As discussed above:

- There is no data from Australia for 2006-2007
- Both countries have data from 2008-2016
- There is no data from the UK for 2017-2019

How many articles with how many words do we have in Australia in 2017 - 2019?

```{r}
#| label: tbl-l20220930185538
#| tbl-cap: "Number of articles and their word count, from Australian tabloids and broadsheets, filtered out from the dataset due to no matching data existing from the UK."
aus_missing_summary <- aus_metadata |>
  filter(year >= 2017) |>
  group_by(year, source_type) |> 
  summarise(count = n(),
            wc = sum(no_words_in_text)) |>
  pivot_wider(names_from = source_type, values_from = c("count", "wc"))

aus_full_summary <- aus_metadata |>
  group_by(year, source_type) |> 
  summarise(count = n(),
            wc = sum(no_words_in_text)) |>
  pivot_wider(names_from = source_type, values_from = c("count", "wc"))

aus_missing_summary |> kable()
```

Total filtered out from Australian corpus:

```{r}
#| label: tbl-l20220930185538c
#| tbl-cap: "Proportion of data filtered out from the Australian corpus."
summary_clean <- function(df) {
  x <- df |>
    ungroup() |>
    summarise(across(everything(), sum)) |>
    select(-year) |>
    data.table::transpose() 
  x$variable <- colnames(df)[colnames(df) != "year"]
  x |> select(variable, V1)
}

summary_clean(aus_full_summary) |>
  rename(full_dataset = V1) |>
  left_join(
  {summary_clean(aus_missing_summary) |>
  rename(filtered_out = V1)}
  ) |> mutate(
    remaining = full_dataset - filtered_out,
    perc_filt_out = round(100* filtered_out/full_dataset, digits = 2)
  ) |> kable()


```

How many articles with how many words do we have in the UK in 2006 - 2007?

```{r}
#| label: tbl-l20220930185538d
#| tbl-cap: "Number of articles and their word count, from UK tabloids and broadsheets, filtered out from the dataset due to no matching data existing from Australia."
uk_missing_summary <- full_metadata |>
  filter(corpus == "UK") |>
  filter(year <= 2007) |>
  group_by(year, source_type) |> 
  summarise(count = n(),
            wc = sum(no_words_in_text)) |>
  pivot_wider(names_from = source_type, values_from = c("count", "wc"))

uk_missing_summary |> kable()
```

Total filtered out from UK corpus:

```{r}
#| label: tbl-l20220930185538z
#| tbl-cap: "Proportion of data filtered out from the UK corpus."
uk_full_summary <-  full_metadata |>
  filter(corpus == "UK") |>
  group_by(year, source_type) |> 
  summarise(count = n(),
            wc = sum(no_words_in_text)) |>
  pivot_wider(names_from = source_type, values_from = c("count", "wc"))

summary_clean(uk_full_summary) |>
  rename(full_dataset = V1) |>
  left_join(
  {summary_clean(uk_missing_summary) |>
  rename(filtered_out = V1)}
  ) |> mutate(
    remaining = full_dataset - filtered_out,
    perc_filt_out = round(100* filtered_out/full_dataset, digits = 2)
  ) |> kable()
```

